<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ai-data-integrity-maintaining"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Maintaining data integrity of AI applications</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        This topic describes how to avoid compromising AI applications and keep
        sensitive data secure.
      </para>
    </abstract>
  </info>
  <section xml:id="why-care-about-ai-security">
    <title>Why care about the security of AI applications?</title>
    <para>
      AI applications use AI-driven chatbots to interact with users. These
      chatbots are powered by large language models (LLMs) and can process
      external data sources (RAGs). Such applications are prone to cyber attacks
      as any other software solutions. Attackers may impersonate users and apply
      a series of techniques to steal data and to corrupt the responses provided
      by AI models.
    </para>
  </section>
  <section xml:id="ai-components-prone-to-attacks">
    <title>Which &productname; components are prone to attacks</title>
    <para>
      Users interact with &productname; via the &owui; user interface. With
      &owui;, you can manage users, permissions, AI models, knowledge bases, and
      chat interactions. The following &productname; components are the most
      susceptible to security attacks:
    </para>
    <variablelist>
      <varlistentry>
        <term>&owui;</term>
        <listitem>
          <para>
            &owui; enables you to specify external data sources to improve
            responses. On a user level, you can append documents directly to the
            chat input field. With administrator privileges, you can upload
            documents to create a knowledge base that enhances the AI model. The
            knowledge base acts as a domain-specific augmentation tool for the
            LLM. It prevents chatbot hallucination and improves the model's
            responses with accurate and up-to-date information.
          </para>
          <tip>
            <para>
              Actions performed by users&mdash;both the administrators and
              guests&mdash;are recorded in an audit log. With the audit log, it
              is possible to map all actions that took the system to its current
              state.
            </para>
          </tip>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&milvus;</term>
        <listitem>
          <para>
            It is possible to input documents directly into &milvus;&mdash;the
            vector database responsible for the low-level implementation of the
            knowledge base concept. Although the user interaction normally takes
            place via &owui;, attackers may bypass &owui; to interact with
            &milvus;. This can happen if no identity access management (IAM)
            policy is controlling database access.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&ollama;</term>
        <listitem>
          <para>
            &ollama; manages the interactions with several LLMs. It can search,
            download, start and manage models within a unified interface.
            &ollama; does not offer authentication and authorization by default.
            Therefore, &ollama;'s API should not be exposed without a an element
            providing IAM capabilities. In &productname;, user interacts with
            &ollama; via &owui;, which is able to configure and secure &ollama;.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="ai-attacks-and-risks">
    <title>What are common attacks and security risks?</title>
    <para>
      This section lists several attacks and security risks related to AI
      applications.
    </para>
    <variablelist>
      <varlistentry>
        <term>RAG poisoning</term>
        <listitem>
          <para>
            A common exploit when a knowledge base&mdash;often a vector
            database&mdash;that provides a context for AI model responses is
            corrupted by the addition of misleading, false or even harmful
            content. The malicious documents tend to be crafted specifically to
            provide wrong answers for a set of user prompts. This kind of attack
            usually requires access to a user with privileges to configure the
            whole platform or the vector databases that support the platform.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Facilitated data exploits</term>
        <listitem>
          <para>
            RAG-powered models can search knowledge bases, summarize content and
            provide references. Attackers may use these characteristics to
            discover and retrieve organizational data with simple prompts
            instead of relying on more refined data-exploitation techniques.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Prompt leaks</term>
        <listitem>
          <para>
            User prompts may contain sensitive data, so chat caches and system
            logs need to be protected against attackers.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="ai-safety-measures">
    <title>What safety measures should my organization follow?</title>
    <para>
      To avoid having your system corrupted, there are a few security measures
      that need to be properly implemented. &owui; and &milvus; allow high level
      user access configurations. Besides these high level configurations,
      provide access management with low level network configurations. To verify
      that your whole AI stack is secure, consider the following points:
    </para>
    <variablelist>
      <varlistentry>
        <term>Adopt strong IAM policies.</term>
        <listitem>
          <itemizedlist>
            <listitem>
              <para>
                At the authentication level, limit the creation of guest users.
              </para>
            </listitem>
            <listitem>
              <para>
                At the authorization level, never allow new users to be
                automatically set as system administrators.
              </para>
            </listitem>
            <listitem>
              <para>
                Limit the number of users with privileges for adding documents
                to your knowledge base.
              </para>
            </listitem>
            <listitem>
              <para>
                Keep in mind that the same policies set for &owui; need to be
                propagated in all systems composing the AI stack.
              </para>
            </listitem>
            <listitem>
              <para>
                Limit the exposure of internal services (such as &milvus; and
                &ollama;) to the Internet.
              </para>
            </listitem>
            <listitem>
              <para>
                Configure authentication and authorization for all components of
                the AI stack.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Adopt an audit log review policy.</term>
        <listitem>
          <para>
            Periodically check the audit logs provided in the Web interface,
            from both the chatbot and the vector databases. Look for abnormal
            behavior from one or more users.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Set up retention policies.</term>
        <listitem>
          <para>
            Avoid saving chat prompts and system logs.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Train users to avoid LLM overreliance.</term>
        <listitem>
          <para>
            Encourage users to approach the answers from the RAG-based models
            with a critical mindset. Make sure they are able to verify the
            references provided by the AI chatbot. Files used in the context of
            a user prompt are appended to the AI model's answer.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Facilitate incident reports.</term>
        <listitem>
          <para>
            Educate your users about how to report problems with the AI model's
            answers. Assign responsibilities for the system support.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Ensure fast action against attacks.</term>
        <listitem>
          <para>
            Remember that when a security breach happens, the sooner the system
            is restored to a trusted state, the less damage your organization
            takes.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Set up a secure environment for your applications.</term>
        <listitem>
          <para>
            Make sure that there are components enforcing authentication and
            authorization rules over the whole installation of the AI Stack. We
            do not recommend exposing &milvus; and &ollama; without proper
            network configurations.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="ai-data-integrity-formoreinfo">
    <title>For more information</title>
    <itemizedlist>
      <listitem>
        <para>
          The article at
          <link
          xlink:href="https://cloudsecurityalliance.org/blog/2023/11/22/mitigating-security-risks-in-retrieval-augmented-generation-rag-llm-applications"/>
          includes a comprehensive overview of security controls.
        </para>
      </listitem>
      <listitem>
        <para>
          Specifying external data sources to the AI model knowledge base is
          described in
          <link xlink:href="&dsc;/suse-ai/1.0/html/openwebui-using/index.html#openwebui-chat-input-field-usage"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</topic>
