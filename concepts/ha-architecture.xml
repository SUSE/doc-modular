<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_concepts.xml -->

<topic xml:id="ha-architecture"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>&sleha; architecture overview</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
         This section provides an overview of &sleha;'s architecture and how the different
         components interoperate.
      </para>
    </abstract>
  </info>
  <section xml:id="ha-architecture-layers">
    <title>Architecture layers</title>
    <para>
      &sleha; has a layered architecture. The following figure shows the different layers and their
      associated components.
    </para>
    <figure xml:id="fig-ha-architecture-layers">
      <title>Architecture of a &sleha; cluster</title>
      <mediaobject>
      <imageobject role="fo">
        <imagedata fileref="ha-cluster-stack-arch.svg" width="100%"/>
      </imageobject>
      <imageobject role="html">
        <imagedata fileref="ha-cluster-stack-arch.svg" width="100%"/>
      </imageobject>
      <textobject role="description">
        <phrase>
          This diagram shows the layers of components on two cluster nodes. Some components
          are local to the node, and some communicate across nodes, such as &corosync; and the CIB.
        </phrase>
      </textobject>
      </mediaobject>
    </figure>
    <variablelist>
      <varlistentry>
        <term>Membership and messaging (&corosync;)</term>
        <listitem>
          <para>
            The &corosync; cluster engine is a group communication system that provides messaging,
            membership and quorum information about the cluster.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Cluster resource manager (&pace;)</term>
        <listitem>
          <para>
            &pace; is the cluster resource manager, or the <quote>brain</quote> that reacts to
            events occurring in the cluster. Events might be nodes that join or leave the cluster,
            failure of resources, or scheduled activities such as maintenance, for example. The
            <systemitem>pacemakerd</systemitem> daemon launches and monitors all other related daemons.
          </para>
          <para>
            The following components are also part of the &pace; layer:
          </para>
          <variablelist>
            <varlistentry>
              <term>Cluster Information Database (CIB)</term>
              <listitem>
                <para>
                  On every node, &pace; maintains the cluster information database (CIB).
                  This is an XML representation of the cluster configuration (including cluster
                  options, nodes, resources, constraints and the relationship to each other).
                  The CIB also reflects the current cluster status. A primary CIB is kept and
                  maintained on the DC and is replicated to the other cluster nodes. The
                  <systemitem>pacemaker-based</systemitem> daemon handles reading and writing
                  cluster configuration and status.
                </para>
              </listitem>
              </varlistentry>
            <varlistentry>
              <term>Designated Coordinator (DC)</term>
              <listitem>
              <para>
                The <systemitem>pacemaker-controld</systemitem> daemon is the cluster controller,
             which coordinates all actions. This daemon has an instance on each cluster node.
             &pace; centralizes all cluster decision-making by electing one of those instances as
             a primary. Should the elected <systemitem >pacemaker-controld</systemitem> daemon fail,
             a new primary is established.
                The DC is elected from the cluster nodes when the cluster services start, or if the
                current DC leaves the cluster for any reason. The DC is the only entity in the
                cluster that can decide that a cluster-wide change must be performed, such as
                fencing a node or moving resources. All other nodes get their configuration and
                resource allocation information from the current DC.
              </para>
              </listitem>
            </varlistentry>
            <varlistentry>
              <term>Policy Engine (PE)</term>
              <listitem>
              <para>
                The policy engine runs on every node as the <systemitem>pacemaker-schedulerd</systemitem>
                daemon, but is only active on the DC. When a cluster transition is needed, the
                policy engine calculates the expected next state of the cluster and determines what
                actions need to be scheduled to achieve that state.
              </para>
              </listitem>
            </varlistentry>
          </variablelist>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Local resource manager</term>
        <listitem>
          <para>
            The local resource manager is located between the &pace; layer and the resources
            layer on each node. The <systemitem>pacemaker-execd</systemitem> daemon allows &pace;
            to start, stop and monitor resources.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Resources and resource agents</term>
        <listitem>
          <para>
            In a &ha; cluster, the services that need to be highly available are called resources.
            Resource agents (RAs) are scripts that start, stop and monitor cluster resources.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section xml:id="ha-architecture-process">
    <title>Process flow</title>
    <para>
      Many actions performed in the cluster cause a cluster-wide change. These actions can include things like adding or removing a cluster resource or changing resource constraints. It is important to understand what happens in the cluster when you perform such an action.
    </para>
    <para>
      For example, suppose you want to add a cluster IP address resource.
    </para>
    <procedure>
      <step>
        <para>
          To do this, you can use the &crmshell; or the Web interface to modify the CIB. It is not required to perform the actions on the DC.
        </para>
      </step>
      <step>
        <para>
          You can use either tool on any node in the cluster and it is relayed to the DC.
        </para>
      </step>
      <step>
        <para>
          The DC replicates the CIB change to all cluster nodes.
        </para>
      </step>
      <step>
        <para>
          Based on the information in the CIB, the <systemitem >pacemaker-schedulerd</systemitem> then computes the ideal state of the cluster and how it should be achieved. It feeds a list of instructions to the DC.
        </para>
      </step>
      <step>
        <para>
          The DC sends commands via &corosync;, which are received by the <systemitem>pacemaker-controld</systemitem> peers on other nodes.
        </para>
      </step>
      <step>
        <para>
          Each of them uses its local resource manager (implemented as <systemitem>pacemaker-execd</systemitem>) to perform resource modifications. The <systemitem>pacemaker-execd</systemitem> is not cluster-aware and interacts directly with resource agents.
        </para>
      </step>
      <step>
        <para>
          All peer nodes report the results of their operations back to the DC.
        </para>
      </step>
      <step>
        <para>
          After the DC concludes that all necessary operations are successfully performed in the cluster, the cluster returns to the idle state and waits for further events.
        </para>
      </step>
      <step>
        <para>
          If any operation was not carried out as planned, the <systemitem>pacemaker-schedulerd</systemitem> is invoked again with the new information recorded in the CIB.
        </para>
      </step>
    </procedure>
    <para>
      In some cases, it might be necessary to power off nodes to protect shared data or complete resource recovery. In a &pace; cluster, the implementation of node level fencing is &stonith;. For this, &pace; comes with a fencing subsystem, <systemitem>pacemaker-fenced</systemitem>. When clients detect a failure, they send a request to <systemitem >pacemaker-fenced</systemitem>, which then executes the fencing agent to bring down the node.
    </para>
  </section>
</topic>
