<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_concepts.xml -->

<topic xml:id="ha-what-is"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>What is &sleha;?</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
       SUSEÂ® Linux Enterprise High Availability is an integrated suite of open source clustering technologies. It enables you to implement highly available physical and virtual Linux clusters, and to eliminate single points of failure. It ensures the high availability and manageability of critical network resources including data, applications, and services. Thus, it helps you maintain business continuity, protect data integrity, and reduce unplanned downtime for your mission-critical Linux workloads.
      </para>
      <para>
        It ships with essential monitoring, messaging, and cluster resource management functionality (supporting failover, failback, and migration (load balancing) of individually managed cluster resources).
      </para>
    </abstract>
  </info>

  <section xml:id="ha-what-is-availability">
    <title>Product availability</title>
    <para>
      &sleha; is available with the following products:
    </para>
    <variablelist>
      <varlistentry>
        <term>&sles;</term>
        <listitem>
          <para>
            &ha; is available as an extension. This extension requires an additional registration code.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&s4s;</term>
        <listitem>
          <para>
            &ha; is included as a module. No additional registration code is required.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section xml:id="ha-what-is-capabilities-benefits">
    <title>Capabilities and benefits</title>
    <para>
   &productnamereg; helps you ensure and manage the availability of your
   network resources. The following sections highlight some key features:
  </para>

  <section xml:id="sec-ha-features-scenarios">
   <title>Wide range of clustering scenarios</title>
   <para>
    &productname; supports the following scenarios:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Active/active configurations
     </para>
    </listitem>
    <listitem>
     <para>
      <remark>toms 2018-09-20: Explain the meaning of these abbreviations
      (from Lars):</remark>
      Active/passive configurations: N+1, N+M, N to 1, N to M
     </para>
    </listitem>
    <listitem>
     <para>
      Hybrid physical and virtual clusters, allowing virtual servers to be
      clustered with physical servers. This improves service availability
      and resource usage.
     </para>
    </listitem>
    <listitem>
     <para>
      Local clusters
     </para>
    </listitem>
    <listitem>
     <para>
      Metro clusters (<quote>stretched</quote> local clusters)
     </para>
    </listitem>
    <listitem>
     <para>
      &geo; clusters (&geo.dispersed; clusters)
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <title>No support for mixed architectures</title>
    <para>
     All nodes belonging to a cluster should have the same processor platform:
     &x86;, &zseries;, or &power;. Clusters of mixed architectures are
     <emphasis>not</emphasis> supported.
    </para>
   </important>
   <para>
    Your cluster can contain up to 32 Linux servers. Using
    &pmrm;, the cluster can be extended to include
    additional Linux servers beyond this limit.
    Any server in the cluster can restart resources (applications, services, IP
    addresses, and file systems) from a failed server in the cluster.
   </para>
  </section>

  <section xml:id="sec-ha-features-flexibility">
   <title>Flexibility</title>
   <para>
    &productname; ships with &corosync; messaging and membership layer
    and Pacemaker Cluster Resource Manager. Using Pacemaker, administrators
    can continually monitor the health and status of their resources, and manage
    dependencies. They can automatically stop and start services based on highly
    configurable rules and policies. &productname; allows you to tailor a
    cluster to the specific applications and hardware infrastructure that
    fit your organization. Time-dependent configuration enables services to
    automatically migrate back to repaired nodes at specified times.
   </para>
  </section>

  <section xml:id="sec-ha-features-storage">
   <title>Storage and data replication</title>
   <para>
    With &productname; you can dynamically assign and reassign server
    storage as needed. It supports Fibre Channel or iSCSI storage area
    networks (SANs). Shared disk systems are also supported, but they are
    not a requirement. &productname; also comes with a cluster-aware file
    system (OCFS2) and the cluster Logical Volume Manager (&clvm;).
    For replication of your data, use DRBD* to mirror the data of
    a &ha; service from the active node of a cluster to its standby node.
    Furthermore, &productname; also supports CTDB (Cluster Trivial Database),
    a technology for Samba clustering.
   </para>
  </section>

  <section xml:id="sec-ha-features-virtualized">
   <title>Support for virtualized environments</title>
   <para>
    &productname; supports the mixed clustering of both physical and
    virtual Linux servers. &sls; &productnumber; ships with &xen;,
    an open source virtualization hypervisor, and with &kvm; (Kernel-based
    Virtual Machine). &kvm; is a virtualization software for Linux which is based on
    hardware virtualization extensions. The cluster resource manager in &productname;
    can recognize, monitor, and manage services running within
    virtual servers and services running in physical servers. Guest
    systems can be managed as services by the cluster.
   </para>
     </section>

  <section xml:id="sec-ha-features-geo">
   <title>Support of local, metro, and &geo; clusters</title>
   <para>
    &productname; supports different geographical scenarios,
    including &geo.dispersed; clusters (&geo; clusters).
   </para>
   <variablelist>
    <varlistentry>
     <term>Local clusters</term>
     <listitem>
      <para>
       A single cluster in one location (for example, all nodes are located
       in one data center). The cluster uses multicast or unicast for
       communication between the nodes and manages failover internally.
       Network latency can be neglected. Storage is typically accessed
       synchronously by all nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Metro clusters</term>
     <listitem>
      <para>
       A single cluster that can stretch over multiple buildings or data
       centers, with all sites connected by Fibre Channel. The cluster uses
       multicast or unicast for communication between the nodes and manages
       failover internally. Network latency is usually low (&lt;5&nbsp;ms for
       distances of approximately 20 miles). Storage is frequently
       replicated (mirroring or synchronous replication).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&geo; clusters (multi-site clusters)</term>
     <listitem>
      <para>
       Multiple, &geo.dispersed; sites with a local cluster each. The
       sites communicate via IP. Failover across the sites is coordinated by
       a higher-level entity. &geo; clusters need to cope with limited
       network bandwidth and high latency. Storage is replicated
       asynchronously.
      </para>
      <note>
       <title>Geo clustering and SAP workloads</title>
       <!--taroth 2022-01-28: might change for some scenarios in the future-->
       <para>
        Currently &geo; clusters support neither &hana; system replication
        nor &s4h; and &nw; enqueue replication setups.
       </para>
      </note>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The greater the geographical distance between individual cluster nodes,
    the more factors may potentially disturb the high availability of
    services the cluster provides. Network latency, limited bandwidth and
    access to storage are the main challenges for long-distance clusters.
   </para>

  </section>

  <section xml:id="sec-ha-features-ra">
   <title>Resource agents</title>
   <para>
    &productname; includes many resource agents to manage
    resources such as Apache, IPv4, IPv6 and many more. It also ships with
    resource agents for popular third party applications such as IBM
    WebSphere Application Server. For an overview of Open Cluster Framework
    (OCF) resource agents included with your product, use the <command>crm
    ra</command> command.
   </para>
  </section>

  <section xml:id="sec-ha-features-tools">
   <title>User-friendly administration tools</title>
   <para>
    &productname; ships with a set of powerful tools. Use them for basic installation
    and setup of your cluster and for effective configuration and
    administration:
   </para>
   <variablelist>
        <varlistentry>
     <term>&hawk;</term>
     <listitem>
      <para>
       A user-friendly Web-based interface with which you can monitor and
       administer your &ha; clusters from Linux or non-Linux machines alike.
       &hawk; can be accessed from any machine inside or outside of the cluster
       by using a (graphical) Web browser. Therefore it is the ideal solution
       even if the system on which you are working only provides a minimal graphical
       user interface.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>crm</command> Shell
     </term>
     <listitem>
      <para>
       A powerful unified command line interface to configure resources and
       execute all monitoring or administration tasks.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
 <section xml:id="sec-ha-benefits">
  <title>Benefits</title>

  <para>
   &productname; allows you to configure up to 32 Linux servers into a
   high-availability cluster (HA cluster). Resources can be
   dynamically switched or moved to any node in the cluster. Resources can
   be configured to automatically migrate if a node fails, or they can be
   moved manually to troubleshoot hardware or balance the workload.
  </para>

  <para>
   &productname; provides high availability from commodity components. Lower
   costs are obtained through the consolidation of applications and
   operations onto a cluster. &productname; also allows you to centrally
   manage the complete cluster. You can adjust resources to meet changing
   workload requirements (thus, manually <quote>load balance</quote> the
   cluster). Allowing clusters of more than two nodes also provides savings
   by allowing several nodes to share a <quote>hot spare</quote>.
  </para>

  <para>
   An equally important benefit is the potential reduction of unplanned
   service outages and planned outages for software and hardware
   maintenance and upgrades.
  </para>

  <para>
   Reasons that you would want to implement a cluster include:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Increased availability
    </para>
   </listitem>
   <listitem>
    <para>
     Improved performance
    </para>
   </listitem>
   <listitem>
    <para>
     Low cost of operation
    </para>
   </listitem>
   <listitem>
    <para>
     Scalability
    </para>
   </listitem>
   <listitem>
    <para>
     Disaster recovery
    </para>
   </listitem>
   <listitem>
    <para>
     Data protection
    </para>
   </listitem>
   <listitem>
    <para>
     Server consolidation
    </para>
   </listitem>
   <listitem>
    <para>
     Storage consolidation
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Shared disk fault tolerance can be obtained by implementing RAID on the shared disk subsystem.
  </para>
  </section>
  </section>

  <section xml:id="ha-what-is-example-cluster-scenario">
    <title>Example cluster scenario</title>
    <para>
      Suppose you have configured a three-node cluster, with a Web server installed on each of the three nodes in the cluster. Each of the nodes in the cluster hosts two Web sites. All the data, graphics, and Web page content for each Web site are stored on a shared disk subsystem connected to each of the nodes in the cluster. The following figure depicts how this setup might look.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster">
      <title>Three-node cluster</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Each Web server has two Web sites on it. Each Web server is also connected to a
            Fibre Channel switch, which is then connected to shared storage.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      During normal cluster operation, each node is in constant communication with the other nodes in the cluster and performs periodic polling of all registered resources to detect failure.
    </para>
    <para>
      Suppose Web Server 1 experiences hardware or software problems and the users depending on Web Server 1 for Internet access, e-mail, and information lose their connections. The following figure shows how resources are moved when Web Server 1 fails.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster-node1-fail">
      <title>Three-node cluster after one node fails</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Web server 1 is crossed out. Web server 2 and Web server 3 each have three Web sites.
            Web site A and Web site B are highlighted in orange to show that they migrated from
            the crossed-out Web server 1.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      Web Site A moves to Web Server 2 and Web Site B moves to Web Server 3. IP addresses and certificates also move to Web Server 2 and Web Server 3.
    </para>
    <para>
      When you configured the cluster, you decided where the Web sites hosted on each Web server would go should a failure occur. In the previous example, you configured Web Site A to move to Web Server 2 and Web Site B to move to Web Server 3. This way, the workload formerly handled by Web Server 1 continues to be available and is evenly distributed between any surviving cluster members.
    </para>
    <para>
      When Web Server 1 failed, the &ha; software did the following:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Detected a failure and verified with &stonith; that Web Server 1 was really dead. &stonith; is an acronym for <quote>Shoot The Other Node In The Head</quote>. It is a means of bringing down misbehaving nodes to prevent them from causing trouble in the cluster.
          </para>
      </listitem>
      <listitem>
        <para>
          Remounted the shared data directories that were formerly mounted on Web server 1 on Web Server 2 and Web Server 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Restarted applications that were running on Web Server 1 on Web Server 2 and Web Server 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Transferred IP addresses to Web Server 2 and Web Server 3.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      In this example, the failover process happened quickly and users regained access to Web site information within seconds, usually without needing to log in again.
    </para>
    <para>
      Now suppose the problems with Web Server 1 are resolved, and Web Server 1 is returned to a normal operating state. Web Site A and Web Site B can either automatically fail back (move back) to Web Server 1, or they can stay where they are. This depends on how you configured the resources for them. Migrating the services back to Web Server 1 will incur some down-time. Therefore &productname; also allows you to defer the migration until a period when it will cause little or no service interruption. There are advantages and disadvantages to both alternatives.
    </para>
    <para>
      &productname; also provides resource migration capabilities. You can move applications, Web sites, etc. to other servers in your cluster as required for system management.
    </para>
    <para>
      For example, you could have manually moved Web Site A or Web Site B from Web Server 1 to either of the other servers in the cluster. Use cases for this are upgrading or performing scheduled maintenance on Web Server 1, or increasing performance or accessibility of the Web sites.
    </para>
  </section>
</topic>
