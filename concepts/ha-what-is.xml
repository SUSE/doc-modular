<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_concepts.xml -->

<topic xml:id="ha-what-is"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>What is &sleha;?</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
       &slereg; &ha; is an integrated suite of open source clustering technologies. It enables you to implement highly available physical and virtual Linux clusters, and to eliminate single points of failure. It ensures the high availability and manageability of critical network resources including data, applications, and services. Thus, it helps you maintain business continuity, protect data integrity, and reduce unplanned downtime for your mission-critical Linux workloads. It ships with essential monitoring, messaging, and cluster resource management functionality (supporting failover, failback, and migration (load balancing) of individually managed cluster resources).
      </para>
      <para>
        The main purpose of an HA cluster is to manage user services. As a cluster administrator, you need to create cluster resources for every service or application you run on servers in your cluster. Cluster resources can include Web sites, e-mail servers, databases, file systems, virtual machines and any other server-based applications or services you want to make available to users at all times.
      </para>
      <para>
      &sleha; allows you to configure up to 32 Linux servers into a high-availability cluster (<quote>HA cluster</quote>). Resources can be dynamically switched or moved to any node in the cluster. Resources can be configured to automatically migrate if a node fails, or they can be moved manually to troubleshoot hardware or balance the workload.
    </para>
    </abstract>
  </info>

  <section xml:id="ha-what-is-availability">
    <title>Product availability</title>
    <para>
      &sleha; is available with the following products:
    </para>
    <variablelist>
      <varlistentry>
        <term>&sles;</term>
        <listitem>
          <para>
            &ha; is available as an extension. This extension requires an additional registration code.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&s4s;</term>
        <listitem>
          <para>
            &ha; is included as a module. No additional registration code is required.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section xml:id="ha-what-is-capabilities-benefits">
    <title>Capabilities and benefits</title>
    <para>
      &sleha; allows you to configure up to 32 Linux servers into a high-availability cluster (<quote>HA cluster</quote>). Resources can be dynamically switched or moved to any node in the cluster. Resources can be configured to automatically migrate if a node fails, or they can be moved manually to troubleshoot hardware or balance the workload.
    </para>
    <para>
      &sleha; provides high availability from commodity components. Lower costs are obtained through the consolidation of applications and operations onto a cluster. &sleha; also allows you to centrally manage the complete cluster. You can adjust resources to meet changing workload requirements (thus, manually <quote>load balance</quote> the cluster). Allowing clusters of more than two nodes also provides savings by allowing several nodes to share a <quote>hot spare</quote>.
    </para>
    <para>
      An equally important benefit is the potential reduction of unplanned service outages and planned outages for software and hardware maintenance and upgrades.
    </para>
    <para>
      Reasons that you would want to implement a cluster include:
    </para>
    <itemizedlist>
      <listitem>
        <para>
        Increased availability
        </para>
      </listitem>
      <listitem>
        <para>
        Improved performance
        </para>
      </listitem>
      <listitem>
        <para>
        Low cost of operation
        </para>
      </listitem>
      <listitem>
        <para>
        Scalability
        </para>
      </listitem>
      <listitem>
        <para>
        Disaster recovery
        </para>
      </listitem>
      <listitem>
        <para>
        Data protection
        </para>
      </listitem>
      <listitem>
        <para>
        server consolidation
        </para>
      </listitem>
      <listitem>
        <para>
        Storage consolidation
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Shared disk fault tolerance can be obtained by implementing RAID on the shared disk subsystem.
    </para>
    <para>
      &sleha; also provides resource migration capabilities. You can move applications, Web sites, etc. to other servers in your cluster as required for system management.
    </para>
    <para>
      For example, you could have manually moved Web site A or Web site B from Web server 1 to either of the other servers in the cluster. Use cases for this are upgrading or performing scheduled maintenance on Web server 1, or increasing performance or accessibility of the Web sites.
    </para>
    <variablelist>
      <varlistentry>
          <term>Wide range of clustering scenarios</term>
          <listitem>
            <para>
              &sleha; supports the following scenarios:
            </para>
            <itemizedlist>
              <listitem>
              <para>
                Active/active configurations
              </para>
              </listitem>
              <listitem>
              <para>
                <remark>toms 2018-09-20: Explain the meaning of these abbreviations
                (from Lars):</remark>
                Active/passive configurations: N+1, N+M, N to 1, N to M
              </para>
              </listitem>
              <listitem>
              <para>
                Hybrid physical and virtual clusters, allowing virtual servers to be clustered with physical servers. This improves service availability and resource usage.
              </para>
              </listitem>
              <listitem>
              <para>
                Local clusters
              </para>
              </listitem>
              <listitem>
              <para>
                Metro clusters (<quote>stretched</quote> local clusters)
              </para>
              </listitem>
              <!--listitem>
              <para>
                &geo; clusters (&geo.dispersed; clusters)
              </para>
              </listitem-->
            </itemizedlist>
            <important>
              <title>No support for mixed architectures</title>
              <para>
              All nodes belonging to a cluster must have the same processor platform: &x86;, &zseries;, or &power;. Clusters of mixed architectures are not supported.
              </para>
            </important>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Flexibility</term>
          <listitem>
            <para>
              &sleha; ships with &corosync; messaging and membership layer and Pacemaker Cluster Resource Manager. Using Pacemaker, administrators can continually monitor the health and status of their resources, and manage dependencies. They can automatically stop and start services based on highly configurable rules and policies. &sleha; allows you to tailor a cluster to the specific applications and hardware infrastructure that fit your organization. Time-dependent configuration enables services to automatically migrate back to repaired nodes at specified times.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Storage and data replication</term>
          <listitem>
            <para>
                With &sleha; you can dynamically assign and reassign server storage as needed. It supports Fibre Channel or iSCSI storage area networks (SANs). Shared disk systems are also supported, but they are not a requirement. &sleha; also comes with &gfs; and the cluster Logical Volume Manager (&clvm;). For replication of your data, use DRBD* to mirror the data of a &ha; service from the active node of a cluster to its standby node. Furthermore, &sleha; also supports CTDB (Cluster Trivial Database), a technology for Samba clustering.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Support for virtualized environments</term>
          <listitem>
            <para>
              &sleha; supports the mixed clustering of both physical and virtual Linux servers. The cluster resource manager can recognize, monitor and manage services running in virtual servers and in physical servers. Guest systems can be managed as services by the cluster. &sles; 16 ships with &kvm; (Kernel-based Virtual Machine), a virtualization software for Linux which is based on hardware virtualization extensions.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Resource agents</term>
          <listitem>
            <para>
              &sleha; includes many resource agents to manage resources such as Apache, IPv4, IPv6 and many more. It also ships with resource agents for popular third party applications such as IBM WebSphere Application server. For an overview of Open Cluster Framework (OCF) resource agents included with your product, use the <command>crm ra</command> command.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>User-friendly administration tools</term>
          <listitem>
            <para>
              &sleha; ships with the following tools for cluster configuration and administration:
            </para>
            <itemizedlist>
              <listitem>
                <para>
                  The <emphasis>&crmshell;</emphasis> (&crmsh;) is a command line interface for installing and setting up &ha; clusters, configuring resources, and performing monitoring and administration tasks.
                </para>
              </listitem>
              <listitem>
                <para>
                  <emphasis>&hawk;</emphasis> is a Web-based graphical interface for monitoring and administering &ha; clusters. It can be accessed using a Web browser from either Linux or non-Linux machines inside or outside the cluster.
                </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
      </variablelist>
  </section>

  <section xml:id="ha-what-is-example-cluster-scenario">
    <title>Example cluster scenario</title>
    <para>
      This example explains how a &ha; cluster migrates resources when a cluster node fails.
    </para>
    <para>
      The following figure shows a three-node cluster. Each of the nodes has a Web server installed,
      and hosts two Web sites. All the data, graphics and Web page content for each Web site are
      stored on a shared disk subsystem connected to each of the nodes.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster">
      <title>Three-node cluster</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Each Web server has two Web sites on it. Each Web server is also connected to a
            Fibre Channel switch, which is then connected to shared storage.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      During normal cluster operation, each node is in constant communication with the other nodes
      in the cluster and periodically checks resources to detect failure.
    </para>
    <para>
      The following figure shows how the cluster moves resources when Web server 1 fails due to
      hardware or software problems.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster-node1-fail">
      <title>Three-node cluster after one node fails</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Web server 1 is crossed out. Web server 2 and Web server 3 each have three Web sites.
            Web site A and Web site B are highlighted in orange to show that they migrated from
            the crossed-out Web server 1.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      Web site A moves to Web server 2 and Web site B moves to Web server 3. Certificates and
      IP addresses also move to Web server 2 and Web server 3. The workload formerly handled by
      Web server 1 continues to be available and is evenly distributed between the remaining
      cluster members.
    </para>
    <para>
      When Web server 1 failed, the &ha; software performed the following tasks:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Detected a failure and verified that Web server 1 was really down.
          </para>
      </listitem>
      <listitem>
        <para>
          Remounted the shared data directories that were mounted on Web server 1 on Web servers 2 and 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Restarted applications that were running on Web server 1 on Web servers 2 and 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Transferred application IP addresses to Web servers 2 and 3.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      In this example, the failover process happened quickly and users regained access to Web site
      information within seconds, usually without needing to log in again.
    </para>
    <para>
      When the problems with Web server 1 are resolved and it returns to a normal operating state,
      Web site A and Web site B can either move back to Web server 1 automatically, or stay where
      they are. This depends on how you configure the resources. Migrating the services back to
      Web server 1 incurs down-time, so alternatively you can defer the migration until a time when
      it will cause little or no service interruption.
    </para>
  </section>
</topic>
