<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_concepts.xml -->

<topic xml:id="ha-what-is"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>What is &sleha;?</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
        &slereg; &ha; is an integrated suite of open source clustering technologies. A
        <emphasis>high-availability</emphasis> cluster is a group of servers (<emphasis>nodes</emphasis>)
        that work together to ensure the highest possible availability of data, applications and
        services. If one node fails, the <emphasis>resources</emphasis> that were running on it move
        to another node with little or no downtime. You can also manually move resources between
        nodes for load balancing, or to perform maintenance tasks with minimal downtime. Cluster
        resources can include Web sites, e-mail servers, databases, file systems, virtual machines
        and any other server-based applications or services that must be available to users at all times.
      </para>
    </abstract>
  </info>

  <section xml:id="ha-what-is-availability">
    <title>Product availability</title>
    <para>
      &sleha; is available with the following products:
    </para>
    <variablelist>
      <varlistentry>
        <term>&sles;</term>
        <listitem>
          <para>
            &ha; is available as an extension. This requires an additional registration code.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&s4s;</term>
        <listitem>
          <para>
            &ha; is included as a module. No additional registration code is required.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section xml:id="ha-what-is-capabilities-benefits">
    <title>Capabilities and benefits</title>
    <para>
      &sleha; eliminates single points of failure to improve the availability and manageability of
      critical resources. This helps you maintain business continuity, protect data integrity, and
      reduce unplanned downtime for critical workloads.
    </para>
    <variablelist>
      <varlistentry>
        <term>Multiple clustering options</term>
        <listitem>
          <para>
            &sleha; clusters can be configured in different ways:
          </para>
          <itemizedlist>
            <listitem>
              <para>
                Active/active node configurations: all nodes are active at once. Resources can run on any node in the cluster, and can move to any other node if the current node fails.
              </para>
            </listitem>
            <listitem>
              <para>
                Active/passive node configurations: only one node (or group of nodes) is active, while the rest of the nodes are idle. Resources only run on the active node. If the active node fails, a passive node becomes active and the resources from the failed node move to the new active node.
              </para>
            </listitem>
            <listitem>
              <para>
                Hybrid clusters: virtual servers can be clustered together with physical servers.
                This improves service availability and resource usage.
              </para>
            </listitem>
            <listitem>
              <para>
                Local clusters: a single cluster in one location (for example, all nodes are located
                in one data center). Network latency is minimal. Storage is typically accessed
                synchronously by all nodes.
              </para>
            </listitem>
            <listitem>
              <para>
                Metro clusters (<quote>stretched</quote> clusters): a single cluster that can
                stretch over multiple buildings or data centers, with all sites connected by
                Fibre Channel. Network latency is usually low. Storage is frequently replicated
                using mirroring or synchronous replication.
              </para>
            </listitem>
          </itemizedlist>
          <important>
            <title>No support for mixed architectures</title>
            <para>
              Clusters with mixed architectures are not supported. All nodes in the same cluster
              must have the same processor platform: &x86;, &zseries;, or &power;.
            </para>
          </important>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Flexible and scalable resource management</term>
        <listitem>
          <para>
            &sleha; supports clusters of up to 32 nodes. Resources can automatically move to another
            node if the current node fails, or they can be moved manually to troubleshoot hardware
            or balance the workload. Resources can also be configured to automatically move back to
            repaired nodes at specified times. The cluster can stop and start resources based on
            configurable rules and policies.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Wide range of resource agents</term>
        <listitem>
          <para>
            The cluster manages resources via <emphasis>resource agents</emphasis> (RAs). &sleha;
            supports many different resource agents that are designed to manage specific types of
            applications or services, including Apache, IPv4, IPv6, NFS and many more. It also ships
            with resource agents for third-party applications such as IBM WebSphere Application server.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Storage and data replication</term>
        <listitem>
          <para>
            &sleha; supports Fibre Channel or iSCSI storage area networks (SANs), allowing you to
            dynamically assign and reassign server storage as needed. It also comes with &gfs; and
            the cluster Logical Volume Manager (&clvm;). For data replication, use DRBD* to mirror
            a resource's data from the active node to a standby node.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Support for virtualized environments</term>
        <listitem>
          <para>
            &sleha; supports the mixed clustering of both physical and virtual Linux servers.
            The cluster can recognize and manage resources running in virtual servers and in
            physical servers, and can also manage &kvm; virtual machines as resources.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Disaster recovery</term>
        <listitem>
          <para>
            &sleha; ships with &rear.long; (&rear;), a disaster recovery framework that helps to
            back up and restore systems.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>User-friendly administration tools</term>
        <listitem>
          <para>
            &sleha; includes tools for configuration and administration:
          </para>
          <itemizedlist>
            <listitem>
              <para>
                The <emphasis>&crmshell;</emphasis> (&crmsh;) is a command line interface for
                installing and setting up &ha; clusters, configuring resources, and performing
                monitoring and administration tasks.
              </para>
            </listitem>
            <listitem>
              <para>
                <emphasis>&hawk;</emphasis> is a Web-based graphical interface for monitoring and
                administration of &ha; clusters. It can be accessed using a Web browser from any
                Linux or non-Linux machine that can connect to the cluster nodes.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>

  <section xml:id="ha-what-is-example-cluster-scenario">
    <title>Example cluster scenario</title>
    <para>
      This example explains how a &ha; cluster moves resources when a cluster node fails.
    </para>
    <para>
      The following figure shows a three-node cluster. Each of the nodes has a Web server installed,
      and hosts two Web sites. All the data, graphics and Web page content for each Web site are
      stored on a shared disk subsystem connected to each of the nodes.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster">
      <title>Three-node cluster</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Each Web server has two Web sites on it. Each Web server is also connected to a
            Fibre Channel switch, which is then connected to shared storage.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      During normal cluster operation, each node is in constant communication with the other nodes
      in the cluster and periodically checks resources to detect failure.
    </para>
    <para>
      The following figure shows how the cluster moves resources when Web server 1 fails due to
      hardware or software problems.
    </para>
    <figure xml:id="fig-ha-example-three-node-cluster-node1-fail">
      <title>Three-node cluster after one node fails</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-example-three-node-cluster-node1-fail.svg" width="80%"/>
        </imageobject>
        <textobject role="description">
          <phrase>
            This diagram shows three cluster nodes: Web server 1, Web server 2, and Web server 3.
            Web server 1 is crossed out. Web server 2 and Web server 3 each have three Web sites.
            Web site A and Web site B are highlighted in orange to show that they moved away from
            the crossed-out Web server 1.
          </phrase>
        </textobject>
      </mediaobject>
    </figure>
    <para>
      Web site A moves to Web server 2 and Web site B moves to Web server 3. The Web sites continue
      to be available and are evenly distributed between the remaining cluster nodes.
    </para>
    <para>
      When Web server 1 failed, the &ha; software performed the following tasks:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Detected a failure and verified that Web server 1 was really down.
          </para>
      </listitem>
      <listitem>
        <para>
          Remounted the shared data directories that were mounted on Web server 1 on Web servers 2 and 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Restarted applications that were running on Web server 1 on Web servers 2 and 3.
        </para>
      </listitem>
      <listitem>
        <para>
          Transferred certificates and IP addresses to Web servers 2 and 3.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      In this example, failover happened quickly and users regained access to the Web sites
      within seconds, usually without needing to log in again.
    </para>
    <para>
      When Web server 1 returns to a normal operating state, Web site A and Web site B can move
      back (<quote>fail back</quote>) to Web server 1 automatically. This incurs some downtime, so
      alternatively you can configure resources to only fail back at a specified time that causes
      minimal service interruption.
    </para>
  </section>
</topic>
