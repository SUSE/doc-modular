<?xml version='1.0' encoding='UTF-8'?><article xmlns="http://docbook.org/ns/docbook" version="5.2" xml:id="two-node-cluster-diskless-sbd-qdevice" xml:lang="pt-br">
  <info>
    <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">Instalando um cluster básico de dois nós de alta disponibilidade</title>
    <revhistory xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="rh-HA-two-node-cluster-diskless-sbd-qdevice">
      <revision>
        <date>04-11-2025</date>
        <revdescription>
          <para>
              Versão inicial
            </para>
        </revdescription>
      </revision>
    </revhistory>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/> <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="architecture">
      <phrase>AMD64/Intel 64</phrase>
      <phrase>POWER</phrase>
      <phrase>IBM Z</phrase>
    </meta> <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="productname" its:translate="no">
      <productname version="16.0">SUSE Linux Enterprise High Availability</productname>
    </meta>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="title" its:translate="yes">Instalando um cluster básico de dois nós de alta disponibilidade</meta>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="description" its:translate="yes">Como configurar um cluster básico de dois nós de alta disponibilidade para fins de teste</meta>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="social-descr" its:translate="yes">Configurar um cluster básico de dois nós HA</meta>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="category" its:translate="no">
      <phrase>Deployment</phrase>
    </meta> <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="task" its:translate="no">
      <phrase>Clustering</phrase>
      <phrase>Configuration</phrase>
      <phrase>Deployment</phrase>
      <phrase>High Availability</phrase>
      <phrase>Installation</phrase>
    </meta>
    <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="series">Produtos e Soluções</meta>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">
      <dm:bugtracker>
        <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
        <dm:component>Documentation</dm:component>
        <dm:product>
          <dm:product condition="16.0">SUSE Linux Enterprise High Availability 16.0</dm:product>
        </dm:product>
        <dm:assignee>tahlia.richardson@suse.com</dm:assignee>
      </dm:bugtracker>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
    <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">
      <variablelist>
        <varlistentry>
          <term>O QUE É?</term>
          <listitem>
            <para>
              Como configurar um cluster básico de dois nós de alta disponibilidade com QDevice, SBD sem disco e um watchdog de software.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>POR QUÊ?</term>
          <listitem>
            <para>
              Você pode usar esse cluster para fins de teste ou como uma configuração de cluster mínima que pode ser estendida no futuro.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>DEDICAÇÃO</term>
          <listitem>
            <para>
                A configuração de um cluster básico de alta disponibilidade leva cerca de 15 minutos, dependendo da velocidade da sua conexão de rede.
              </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>META</term>
          <listitem>
            <para>
                Comece a usar o SUSE Linux Enterprise High Availability de forma rápida e fácil.
              </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </abstract>
  </info>
  <section role="concept" xml:lang="pt-br" version="5.2" xml:id="ha-installing-cluster-usage-scenario">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Cenário de uso</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <para>
    Este guia descreve a configuração de um cluster mínimo de alta disponibilidade com as seguintes propriedades:
  </para>
    <itemizedlist>
      <listitem>
        <para condition="twonode">
        Dois nós de cluster com acesso SSH sem senha entre eles.
      </para>
      </listitem>
      <listitem>
        <para>
        Um endereço IP virtual flutuante para que os clientes se conectem à ferramenta de gerenciamento gráfico Hawk, seja qual for o nó em que o serviço é executado.
      </para>
      </listitem>
      <listitem>
        <para>
        Um SBD (STONITH Block Device) sem disco e um watchdog de software usado como mecanismo de fencing de nó para evitar cenários de split brain.
      </para>
      </listitem>
      <listitem condition="twonode">
        <para>
        O QDevice e o QNetd operando juntos para participar das decisões de quorum do cluster. O QDevice e o QNetd são necessários nessa configuração para o SBD sem disco lidar com os cenários de split brain no cluster de dois nós.
      </para>
      </listitem>
      <listitem>
        <para>
        Failover de recursos de um nó para outro em caso de falha no host ativo (configuração de modo <emphasis>ativo/passivo</emphasis>).
      </para>
      </listitem>
    </itemizedlist>
    <para>
    Esta é uma configuração de cluster simples com requisitos externos mínimos. Você pode usar esse cluster para fins de teste ou como uma configuração de cluster básica que pode ser estendida para um ambiente de produção no futuro.
  </para>
  </section>
  <section role="glue" xml:lang="pt-br" version="5.2" xml:id="ha-installing-cluster-overview">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Visão geral da instalação</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <para>
    Para instalar o cluster de alta disponibilidade descrito na <xref linkend="ha-installing-cluster-usage-scenario"/>, você deve executar as seguintes tarefas:
  </para>
    <orderedlist>
      <listitem>
        <para>
        Revise a <xref linkend="ha-requirements"/> para garantir que você tenha tudo o que precisa.
      </para>
      </listitem>
      <listitem>
        <para>
        Instale o SUSE Linux Enterprise High Availability nos nós do cluster seguindo a <xref linkend="ha-enabling-extension"/>.
      </para>
      </listitem>
      <listitem condition="twonode">
        <para>
        Instale o QNetd em um servidor que não seja cluster conforme a <xref linkend="ha-qdevice-setting-up-qnetd"/>.
      </para>
      </listitem>
      <listitem>
        <para>
        Inicialize o cluster no primeiro nó de acordo com a <xref linkend="ha-setting-up-first-node"/>.
      </para>
      </listitem>
      <listitem>
        <para>
        Consulte a <xref linkend="ha-adding-nodes"/> para adicionar outros nós ao cluster.
      </para>
      </listitem>
      <listitem>
        <para>
        Faça login na interface da Web do Hawk para monitorar o cluster seguindo a <xref linkend="ha-hawk-logging-in"/>.
      </para>
      </listitem>
      <listitem condition="twonode">
        <para>
        Verifique o status do QDevice e do QNetd conforme a <xref linkend="ha-qdevice-showing-quorum-status"/>.
      </para>
      </listitem>
      <listitem>
        <para>
        Execute testes básicos para garantir que o cluster funcione conforme o esperado. Consulte a <xref linkend="ha-testing-the-cluster-basic"/>.
      </para>
      </listitem>
      <listitem>
        <para>
        Leia a <xref linkend="ha-installing-cluster-next-steps"/> para obter orientação de como expandir o cluster para um ambiente de produção.
      </para>
      </listitem>
    </orderedlist>
  </section>
  <section role="reference" xml:lang="pt-br" version="5.2" xml:id="ha-requirements">
    <info>
      <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">Requisitos do sistema</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
       Esta seção descreve os requisitos do sistema para configuração mínima do SUSE Linux Enterprise High Availability.
      </para>
      </abstract>
    </info>
    <section xml:id="ha-requirements-hardware">
      <title>Requisitos de hardware</title>
      <variablelist>
        <varlistentry>
          <term>Servidores</term>
          <listitem>
            <para condition="twonode">
            Três servidores: dois para atuar como nós do cluster e um para executar o QNetd.
          </para>
            <para>
            Os servidores podem ser completamente vazios ou máquinas virtuais. Eles não exigem hardware idêntico (memória, espaço em disco, etc.), mas devem ter a mesma arquitetura. Clusters compatíveis com várias plataformas não são suportados.
          </para>
            <para>
            Consulte a seção <citetitle>System Requirements</citetitle> em <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.suse.com/download/sle-ha/"></link> para obter mais detalhes sobre o hardware do servidor.
          </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Placas de interface de rede (NICs, Network Interface Cards)</term>
          <listitem>
            <para>
            Pelo menos duas NICs por nó do cluster. Dessa forma, você pode configurar dois ou mais canais de comunicação para o cluster, usando um dos seguintes métodos:
          </para>
            <itemizedlist>
              <listitem>
                <para>
                Combinar as NICs em um vínculo de rede (preferencial). Nesse caso, você deve configurar o dispositivo vinculado em cada nó antes de inicializar o cluster.
              </para>
              </listitem>
              <listitem>
                <para>
                Criar um segundo canal de comunicação no Corosync. O script de configuração do cluster pode configurá-lo. Nesse caso, as duas NICs devem estar em sub-redes diferentes.
              </para>
              </listitem>
            </itemizedlist>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>STONITH (fencing de nó)</term>
          <listitem>
            <para>
            Para serem compatíveis, todos os clusters SUSE Linux Enterprise High Availability <emphasis>devem</emphasis> ter pelo menos um dispositivo de fencing de nó (STONITH) para evitar cenários de split brain. Ele pode ser um dispositivo físico (interruptor) ou um SBD (STONITH Block Device) em combinação com um watchdog. O SBD pode ser usado com armazenamento compartilhado ou no modo sem disco.
          </para>
            <para>
            A configuração mínima descrita neste guia usa um watchdog de software e um SBD sem disco, portanto, nenhum hardware adicional é necessário. Antes de usar esse cluster em um ambiente de produção, substitua o watchdog de software por um de hardware.
          </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="ha-requirements-software">
      <title>Requisitos de software</title>
      <variablelist>
        <varlistentry>
          <term>Sistema operacional</term>
          <listitem>
            <para>
            Todos os nós<phrase condition="twonode"> e o servidor QNetd</phrase> devem ter o SUSE Linux Enterprise Server instalado e registrado.
          </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Extensão de alta disponibilidade</term>
          <listitem>
            <para>
            A SUSE Linux Enterprise High Availability Extension requer um código de registro adicional.
          </para>
            <para>
            É possível habilitar essa extensão durante a instalação do SLES, ou posteriormente em um sistema em execução. Este guia explica como habilitar e registrar a extensão em um sistema em execução.
          </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="ha-requirements-network">
      <title>Requisitos de rede</title>
      <variablelist>
        <varlistentry>
          <term>Sincronização de Horário</term>
          <listitem>
            <para>
            Todos os sistemas devem ser sincronizados com um servidor NTP fora do cluster. O SUSE Linux Enterprise Server usa o <systemitem class="resource">chrony</systemitem> para NTP. Ao inicializar o cluster, você receberá um aviso caso o <systemitem class="resource">chrony</systemitem> não esteja em execução.
          </para>
            <para>
            Mesmo que os nós estejam sincronizados, ainda poderá ser difícil analisar os arquivos de registro e os relatórios do cluster se os nós tiverem fusos horários diferentes configurados.
          </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Nome de host e endereço IP</term>
          <listitem>
            <para>
            Todos os nós do cluster devem conseguir encontrar uns aos outros<phrase condition="twonode">, e o servidor QNetd,</phrase> pelo nome. Use os seguintes métodos para obter uma resolução de nomes confiável:
          </para>
            <itemizedlist>
              <listitem>
                <para>
                Use endereços IP estáticos.
              </para>
              </listitem>
              <listitem>
                <para>
                Liste todos os nós no arquivo <filename>/etc/hosts</filename> com os endereços IP, o FQDN e o nome de host abreviado.
              </para>
              </listitem>
            </itemizedlist>
            <para>
            Apenas o endereço IP principal é suportado em cada NIC.
          </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SSH</term>
          <listitem>
            <para>
            Todos os nós do cluster devem conseguir acessar uns aos outros<phrase condition="twonode">, e o servidor QNetd,</phrase> por SSH. Algumas operações de cluster também exigem autenticação SSH sem senha. Quando você inicializa o cluster, o script de configuração verifica as chaves SSH existentes e as gera, se não existirem.
          </para>
            <important>
              <title>Acesso por SSH de <systemitem class="username">root</systemitem> no SUSE Linux Enterprise 16</title>
              <para>
              No SUSE Linux Enterprise 16, o login por SSH de <systemitem class="username">root</systemitem> com senha está desabilitado por padrão.
            </para>
              <para>
              Em cada nó,<phrase condition="twonode"> e no servidor QNetd,</phrase> crie um usuário com privilégios de <command>sudo</command> ou configure a autenticação SSH sem senha para o usuário <systemitem class="username">root</systemitem> antes de inicializar o cluster.
            </para>
              <para>
              Se você inicializar o cluster com um usuário <command>sudo</command>, alguns comandos <systemitem>crmsh</systemitem> também precisarão de permissão de <command>sudo</command> sem senha.
            </para>
            </important>
          </listitem>
        </varlistentry>
        <varlistentry condition="twonode">
          <term>Rede separada para o QNetd</term>
          <listitem>
            <para>
            Recomendamos que os nós do cluster acessem o servidor QNetd por uma rede diferente daquela usada pelo Corosync. O ideal é que o servidor QNetd esteja em um rack separado do cluster; ou, pelo menos, em uma PSU separada, e não no mesmo segmento de rede que os canais de comunicação do Corosync.
          </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
  </section>
  <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-enabling-extension">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Habilitando a extensão de alta disponibilidade</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
        Este procedimento explica como instalar o SUSE Linux Enterprise High Availability em um SUSE Linux Enterprise Server existente. Você pode ignorar esse procedimento se já tiver instalado a extensão e os pacotes de alta disponibilidade durante a instalação do SLES com o Agama.
      </para>
      </abstract>
    </info>
    <itemizedlist>
      <title>Requisitos</title>
      <listitem>
        <para>
        O SUSE Linux Enterprise Server é instalado e registrado no SUSE Customer Center.
      </para>
      </listitem>
      <listitem>
        <para>
        Você tem um código de registro adicional para o SUSE Linux Enterprise High Availability.
      </para>
      </listitem>
    </itemizedlist>
    <para>
    Execute este procedimento em todas as máquinas que pretende usar como nós do cluster:
  </para>
    <procedure>
      <step>
        <para>
        Faça login como usuário <systemitem class="username">root</systemitem> ou como usuário com privilégios de <command>sudo</command>.
      </para>
      </step>
      <step>
        <para>
        Verifique se a extensão de alta disponibilidade já está habilitada:
      </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo SUSEConnect --list-extensions</command>
        </screen>
      </step>
      <step>
        <para>
        Verifique se os pacotes de alta disponibilidade já estão instalados:
      </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>zypper search ha_sles</command>
        </screen>
      </step>
      <step>
        <para>
        Habilite a SUSE Linux Enterprise High Availability Extension:
      </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo SUSEConnect -p sle-ha/<phrase><phrase os="sleha">16.0</phrase></phrase>/x86_64 -r <replaceable>HA_REGCODE</replaceable></command>
        </screen>
      </step>
      <step>
        <para>
        Instale os pacotes de alta disponibilidade:</para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo zypper install -t pattern ha_sles</command>
        </screen>
      </step>
    </procedure>
  </section>
  <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-qdevice-setting-up-qnetd">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Configurando o servidor QNetd</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
        O QNetd é o arbitrador que fornece um voto ao daemon QDevice executado nos nós do cluster. O servidor QNetd é executado fora do cluster, portanto, não é possível mover os recursos do cluster para esse servidor. O QNetd poderá suportar vários clusters se cada um tiver um nome exclusivo.
      </para>
      </abstract>
    </info>
    <itemizedlist>
      <title>Requisitos</title>
      <listitem>
        <para>
        O SUSE Linux Enterprise Server é instalado e registrado no SUSE Customer Center.
      </para>
      </listitem>
      <listitem>
        <para>
        Você tem um código de registro adicional para o SUSE Linux Enterprise High Availability.
      </para>
      </listitem>
    </itemizedlist>
    <para>
    Execute este procedimento em um servidor que não fará parte do cluster:
  </para>
    <procedure>
      <step>
        <para>
        Faça login como usuário <systemitem class="username">root</systemitem> ou como usuário com privilégios de <command>sudo</command>.
      </para>
      </step>
      <step>
        <para>
        Habilite a SUSE Linux Enterprise High Availability Extension:
      </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo SUSEConnect -p sle-ha/16.0/x86_64 -r <replaceable>HA_REGCODE</replaceable></command>
        </screen>
      </step>
      <step>
        <para>
        Instale o pacote <package>corosync-qnetd</package>:
      </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo zypper install corosync-qnetd</command>
        </screen>
        <para>
        Você não precisa iniciar manualmente o serviço <systemitem>corosync-qnetd</systemitem>. Ele é iniciado automaticamente quando você configura o QDevice no cluster.
      </para>
      </step>
    </procedure>
    <para>
    O servidor QNetd está pronto para aceitar conexões de um cliente QDevice (<systemitem>corosync-qdevice</systemitem>). O <systemitem>crmsh</systemitem> cuida da configuração adicional quando você conecta clientes QDevice.
  </para>
    <para>
        Por padrão, o serviço <systemitem>corosync-qnetd</systemitem> executa o daemon como usuário <systemitem>coroqnetd</systemitem> no grupo <systemitem>coroqnetd</systemitem>. Isso evita a execução do daemon como <systemitem class="username">root</systemitem>.
      </para>
  </section>
  <section role="glue" xml:lang="pt-br" version="5.2" xml:id="ha-setting-up-first-node">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Configurando o primeiro nó</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <para>
    O SUSE Linux Enterprise High Availability inclui scripts de configuração para simplificar a instalação do cluster. Para configurar o cluster no primeiro nó, use o script <command>crm cluster init</command>.
  </para>
    <section role="reference" xml:lang="pt-br" version="5.2" xml:id="ha-crmsh-overview-init-script">
      <info>
        <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Visão geral do script <command>crm cluster init</command></title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
        <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
          <para>
       O comando <command>crm cluster init</command> inicia um script que define os parâmetros básicos necessários para comunicação do cluster, resultando em um cluster de um nó em execução.
      </para>
        </abstract>
      </info>
      <para>
    O script verifica e configura os seguintes componentes:
  </para>
      <variablelist>
        <varlistentry>
          <term>NTP</term>
          <listitem>
            <para>
          Verifica se o <systemitem class="resource">chrony</systemitem> está configurado para ser iniciado no momento da inicialização. Do contrário, será exibida uma mensagem.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SSH</term>
          <listitem>
            <para>
          Detecta ou gera chaves SSH para login sem senha entre os nós do cluster.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Firewall</term>
          <listitem>
            <para>
          Abre as portas no firewall que são necessárias para a comunicação do cluster.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Csync2</term>
          <listitem>
            <para>
          Configura o Csync2 para replicar os arquivos de configuração para todos os nós em um cluster.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Corosync</term>
          <listitem>
            <para>
          Configura o sistema de comunicação do cluster.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SBD/watchdog</term>
          <listitem>
            <para>
          Verifica se há um watchdog e pergunta se é para configurar o SBD como mecanismo de fencing de nó.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Administração de cluster do Hawk</term>
          <listitem>
            <para>
          Habilita o serviço Hawk e exibe o URL para a interface da Web do Hawk.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>IP flutuante virtual</term>
          <listitem>
            <para>
          Pergunta se é para configurar um endereço IP virtual para a interface da Web do Hawk.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>QDevice/QNetd</term>
          <listitem>
            <para>
          Pergunta se é para configurar o QDevice e o QNetd para participar das decisões de quorum. Isso é recomendado para clusters com um número par de nós, principalmente para clusters de dois nós.
        </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <note>
        <title>Configurações padrão do Pacemaker</title>
        <para>
      As opções definidas pelo script <command>crm cluster init</command> podem não ser iguais às configurações padrão do Pacemaker. Você pode verificar as configurações que o script mudou em <filename>/var/log/crmsh/crmsh.log</filename>. As opções definidas durante o processo de inicialização podem ser modificadas mais tarde com o <systemitem>crmsh</systemitem>.
    </para>
      </note>
      <note>
        <title>Configuração de cluster para plataformas diferentes</title>
        <para>
      O script <command>crm cluster init</command> detecta o ambiente do sistema (por exemplo, Microsoft Azure) e ajusta determinadas configurações do cluster com base no perfil desse ambiente. Para obter mais informações, consulte o arquivo <filename>/etc/crm/profiles.yml</filename>.
    </para>
      </note>
    </section>
    <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-initializing-cluster">
      <info>
        <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">Inicializando o cluster no primeiro nó</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
        <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
          <para>
        Configure o cluster no primeiro nó com o script <command>crm cluster init</command>. O script solicita informações básicas sobre o cluster e define as configurações e os serviços necessários. Para obter mais informações, execute o comando <command>crm cluster init --help</command>.
      </para>
        </abstract>
      </info>
      <itemizedlist>
        <title>Requisitos</title>
        <listitem>
          <para>
        O SUSE Linux Enterprise High Availability está instalado e atualizado.
      </para>
        </listitem>
        <listitem>
          <para>
        Todos os nós têm pelo menos duas interfaces de rede ou um vínculo de rede, com os endereços IP estáticos listados no arquivo <filename>/etc/hosts</filename> junto com o FQDN e o nome de host abreviado de cada nó.
      </para>
        </listitem>
        <listitem condition="twonode">
          <para>
        O servidor QNetd está instalado. Se você fizer login no servidor QNetd como usuário <systemitem class="username">root</systemitem>, a autenticação SSH sem senha deverá ser habilitada.
      </para>
        </listitem>
      </itemizedlist>
      <para>
    Execute este procedimento apenas em um nó:
  </para>
      <procedure>
        <step>
          <para>
        Faça login no primeiro nó como usuário <systemitem class="username">root</systemitem> ou como usuário com privilégios de <command>sudo</command>.
      </para>
        </step>
        <step>
          <para>
        Inicie o script <command>crm cluster init</command>:
      </para>
          <screen>
            <prompt>&gt; </prompt>
            <command>sudo crm cluster init</command>
          </screen>
          <para>
        O script verifica se o <systemitem class="resource">chrony</systemitem> está em execução, abre as portas de firewall necessárias, configura o Csync2 e confere as chaves SSH. Se não houver chaves SSH disponíveis, o script vai gerá-las.
      </para>
        </step>
        <step>
          <para>
        Configure o Corosync para comunicação com o cluster:
      </para>
          <substeps>
            <step>
              <para>
            Insira um endereço IP para o primeiro canal de comunicação (<literal>ring0</literal>). Por padrão, o script propõe o endereço da primeira interface de rede disponível. Pode ser uma interface individual ou um dispositivo vinculado. Aceite esse endereço ou insira um diferente.
          </para>
            </step>
            <step>
              <para>
            Se o script detectar várias interfaces de rede, ele perguntará se você deseja configurar um segundo canal de comunicação (<literal>ring1</literal>). Se você configurou o primeiro canal com um dispositivo vinculado, pode recusar com <literal>n</literal>. Se você precisar configurar um segundo canal, confirme com <literal>y</literal> e insira o endereço IP de outra interface de rede. As duas interfaces devem estar em sub-redes diferentes.
          </para>
            </step>
          </substeps>
          <para>
        O script configura as portas de firewall padrão para comunicação com o Corosync.
      </para>
        </step>
        <step>
          <para>
        Escolha se deseja configurar o SBD como mecanismo de fencing de nó:
      </para>
          <substeps>
            <step>
              <para>
            Pressione <literal>y</literal> para confirmar que você deseja usar o SBD.</para>
            </step>
            <step>
              <para>
            Quando for solicitado o caminho para um dispositivo de blocos, insira <literal>none</literal> para configurar o SBD sem disco.</para>
            </step>
          </substeps>
          <para>
        O script configura o SBD, incluindo as definições de tempo de espera relevantes. Ao contrário do SBD baseado em disco, o SBD sem disco não exige um recurso de cluster STONITH.
      </para>
          <para>
        Se não houver um watchdog de hardware disponível, o script vai configurar o watchdog de software <systemitem>softdog</systemitem>.
      </para>
        </step>
        <step>
          <para>
        Configure um endereço IP virtual para administração do cluster com a interface da Web do Hawk:
      </para>
          <substeps>
            <step>
              <para>
            Pressione <literal>y</literal> para confirmar que você deseja configurar um endereço IP virtual.
          </para>
            </step>
            <step>
              <para>
            Insira um endereço IP não usado para definir como o IP de administração do Hawk.
          </para>
            </step>
          </substeps>
          <para>
        Em vez de fazer login no Hawk em um nó de cluster individual, você pode se conectar ao endereço IP virtual.
      </para>
        </step>
        <step>
          <para>
        Escolha se você deseja configurar o QDevice e o QNetd:
      </para>
          <substeps condition="twonode">
            <step>
              <para>
            Insira <literal>y</literal> para confirmar que você deseja configurar o QDevice e o QNetd.
          </para>
            </step>
            <step>
              <para>
            Insira o endereço IP ou o nome de host do servidor QNetd, com ou sem um nome de usuário.
          </para>
              <itemizedlist>
                <listitem>
                  <para>
                Se você incluir um nome de usuário não <systemitem class="username">root</systemitem>, será solicitado que forneça a senha, e o script configurará a autenticação SSH sem senha do nó para o servidor QNetd.
              </para>
                </listitem>
                <listitem>
                  <para>
                Se você omitir o nome de usuário, o script assumirá como padrão o usuário <systemitem class="username">root</systemitem>, portanto, a autenticação SSH sem senha já deve estar configurada para que o nó acesse o servidor QNetd.
              </para>
                </listitem>
              </itemizedlist>
              <para>
            Para os campos restantes, aceite os valores padrão:
          </para>
            </step>
            <step>
              <para>
            Aceite a porta proposta (<literal>5403</literal>) ou insira uma diferente.
          </para>
            </step>
            <step>
              <para>
            Escolha o algoritmo que determina como os votos são atribuídos. O padrão é <literal>ffsplit</literal>.
          </para>
            </step>
            <step>
              <para>
            Escolha o método que será usado quando um desempate for necessário. O padrão é <literal>lowest</literal>.
          </para>
            </step>
            <step>
              <para>
            Escolha se deseja habilitar o TLS para verificação de certificado do cliente. O padrão é <literal>on</literal> (tentar se conectar com TLS, mas se conectar sem TLS se não estiver disponível).
          </para>
            </step>
            <step performance="optional">
              <para>
            Insira comandos de heurística para afetar como os votos são determinados. Para ignorar esta etapa, deixe o campo em branco.
          </para>
            </step>
          </substeps>
          <para condition="twonode">
        O script configura o QDevice e o QNetd, incluindo as chaves SSH, os certificados de CA e de servidor e a porta do firewall. Ele também habilita os serviços necessários nos nós do cluster e no servidor QNetd.
      </para>
        </step>
      </procedure>
      <para>
    O script inicia os serviços do cluster para colocar o cluster online e habilitar o Hawk. O URL que será usado no Hawk é exibido na tela. Você também pode verificar o status do cluster com o comando <command>crm status</command>.
  </para>
      <important>
        <title>Senha segura para <systemitem>hacluster</systemitem></title>
        <para>
      O script <command>crm cluster init</command> cria um usuário e uma senha padrão do cluster. Substitua a senha padrão por uma segura assim que possível:
    </para>
        <screen>
          <prompt>&gt; </prompt>
          <command>sudo passwd hacluster</command>
        </screen>
      </important>
    </section>
  </section>
  <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-adding-nodes">
    <info>
      <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">Adicionando o segundo nó</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
        Adicione outros nós ao cluster com o script <command>crm cluster join</command>. O script precisa apenas de acesso a um nó do cluster existente para concluir a configuração básica na máquina atual automaticamente. Para obter mais informações, execute o comando <command>crm cluster join --help</command>.
      </para>
      </abstract>
    </info>
    <itemizedlist>
      <title>Requisitos</title>
      <listitem>
        <para>
        O SUSE Linux Enterprise High Availability está instalado e atualizado.
      </para>
      </listitem>
      <listitem>
        <para>
        Já existe um cluster executado em pelo menos um nó.
      </para>
      </listitem>
      <listitem>
        <para>
        Todos os nós têm pelo menos duas interfaces de rede ou um vínculo de rede, com os endereços IP estáticos listados no arquivo <filename>/etc/hosts</filename> junto com o FQDN e o nome de host abreviado de cada nó.
      </para>
      </listitem>
      <listitem>
        <para><emphasis>Se você fizer login como usuário <command>sudo</command>:</emphasis> O mesmo usuário deverá existir em todos os nós<phrase condition="twonode"> e no servidor QNetd</phrase>. Esse usuário deverá ter permissão de <command>sudo</command> sem senha.
      </para>
      </listitem>
      <listitem>
        <para><emphasis>Se você fizer login como usuário <systemitem class="username">root</systemitem>:</emphasis> A autenticação SSH sem senha deverá ser configurada em todos os nós<phrase condition="twonode"> e no servidor QNetd</phrase>.
      </para>
      </listitem>
    </itemizedlist>
    <para>
    Execute este procedimento em cada nó adicional:
  </para>
    <procedure>
      <step>
        <para>
     Faça login nesse nó como o mesmo usuário com o qual você configurou o primeiro nó.
    </para>
      </step>
      <step>
        <para>
     Inicie o script <command>crm cluster join</command>:
    </para>
        <itemizedlist>
          <listitem>
            <para>
          Se você configurar o primeiro nó como <systemitem class="username">root</systemitem>, poderá iniciar o script sem parâmetros adicionais:
        </para>
            <screen>
              <prompt role="root"># </prompt>
              <command>crm cluster join</command>
            </screen>
          </listitem>
          <listitem>
            <para>
          Se você configurar o primeiro nó como um usuário <command>sudo</command>, deverá especificar esse usuário com a opção <option>-c</option>:
        </para>
            <screen>
              <prompt>&gt; </prompt>
              <command>sudo crm cluster join -c <replaceable>USER@NODE1</replaceable></command>
            </screen>
          </listitem>
        </itemizedlist>
        <para>
      O script verifica se o <systemitem class="resource">chrony</systemitem> está em execução, abre as portas de firewall necessárias e configura o Csync2.
    </para>
      </step>
      <step>
        <para>
      Se você ainda não especificou o primeiro nó com <option>-c</option>, é solicitado o endereço IP ou o nome de host dele.
    </para>
      </step>
      <step>
        <para>
      Se você ainda não configurou a autenticação SSH sem senha entre os nós, é solicitada a <phrase condition="twonode">senha do primeiro nó.</phrase>
      
    </para>
      </step>
      <step>
        <para>
       Configure o Corosync para comunicação com o cluster:
     </para>
        <substeps>
          <step>
            <para>
           O script propõe um endereço IP para <literal>ring0</literal>. Esse endereço IP deve estar na mesma sub-rede que o endereço IP usado para <literal>ring0</literal> no primeiro nó. Se não estiver, insira o endereço IP correto.
         </para>
          </step>
          <step>
            <para>
           Se o cluster tiver dois canais de comunicação do Corosync configurados, o script solicitará um endereço IP para <literal>ring1</literal>. Esse endereço IP deve estar na mesma sub-rede que o endereço IP usado para <literal>ring1</literal> no primeiro nó.
         </para>
          </step>
        </substeps>
      </step>
    </procedure>
    <para>
    O script copia a configuração do cluster do primeiro nó, ajusta as configurações de tempo de espera para considerar o novo nó e coloca o novo nó online.
    </para>
    <para>
    Você pode verificar o status do cluster com o comando <command>crm status</command>.
  </para>
    <important>
      <title>Senha segura para <systemitem>hacluster</systemitem></title>
      <para>
      O script <command>crm cluster join</command> cria um usuário e uma senha padrão do cluster. Em cada nó, substitua a senha padrão por uma segura assim que possível:
    </para>
      <screen>
        <prompt>&gt; </prompt>
        <command>sudo passwd hacluster</command>
      </screen>
    </important>
  </section>
  <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-hawk-logging-in">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Fazendo login no Hawk</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
         O Hawk permite monitorar e administrar um cluster de alta disponibilidade usando um navegador da Web gráfico. Você também pode configurar um endereço IP virtual que permite aos clientes se conectarem ao Hawk, seja qual for o nó em que ele está em execução.
      </para>
      </abstract>
    </info>
    <itemizedlist>
      <title>Requisitos</title>
      <listitem>
        <para>
        A máquina cliente deve conseguir se conectar aos nós do cluster.
      </para>
      </listitem>
      <listitem>
        <para>
        A máquina cliente deve ter um navegador da Web gráfico com JavaScript e cookies habilitados.
      </para>
      </listitem>
    </itemizedlist>
    <para>
    Você pode executar este procedimento em qualquer máquina que tenha conexão com os nós do cluster:
  </para>
    <procedure>
      <step>
        <para>
        Inicie um navegador da Web e insira este URL:
      </para>
        <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
        <para>
        Substitua <replaceable>HAWKSERVER</replaceable> pelo endereço IP ou nome de host de um nó do cluster, ou pelo endereço IP virtual do Hawk, se foi configurado um.
      </para>
        <note>
          <title>Aviso de certificado</title>
          <para>
          Se um aviso de certificado for exibido quando você acessar o URL pela primeira vez, um certificado autoassinado estará em uso. Solicite ao operador do cluster os detalhes do certificado para verificá-lo. Para continuar mesmo assim, você pode adicionar uma exceção ao browser para ignorar o aviso.
        </para>
        </note>
      </step>
      <step>
        <para>
        Na tela de login do Hawk, insira o <guimenu>Nome de usuário</guimenu> e a <guimenu>Senha</guimenu> do usuário <systemitem class="username">hacluster</systemitem>.
      </para>
      </step>
      <step>
        <para>
        Clique em <guimenu>Entrar</guimenu>. Por padrão, a interface da Web do Hawk mostra a tela <guimenu>Status</guimenu>.
      </para>
      </step>
    </procedure>
    <figure xml:id="fig-ha-hawk-status">
      <title>Tela Status do Hawk</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha-hawk-status.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha-hawk-status.png" width="100%"/>
        </imageobject>
        <textobject role="description">
          <phrase> A tela Status mostra um recurso configurado: o endereço IP virtual <literal>admin-ip</literal> executado em um nó chamado <literal>alice</literal>. </phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-qdevice-showing-quorum-status">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Mostrando o status do quorum</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
        <para>
       Você pode verificar o status do QDevice e do QNetd de qualquer nó no cluster. Estes exemplos mostram um cluster com dois nós: <systemitem>alice</systemitem> e <systemitem>bob</systemitem>.
      </para>
      </abstract>
    </info>
    <example xml:id="ex-qdevice-status">
      <title>Mostrando o status do QDevice</title>
      <screen><prompt>&gt; </prompt><command>sudo crm corosync status quorum</command>
1 alice member
2 bob member

Quorum information
------------------
Date:             [...]
Quorum provider:  corosync_votequorum
Nodes:            2
Node ID:          2
Ring ID:          1.e
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   3
Highest expected: 3
Total votes:      3
Quorum:           2
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
         1          1    A,V,NMW alice
         2          1    A,V,NMW bob (local)
         0          1            Qdevice</screen>
    </example>
    <para>
    A seção <literal>Membership information</literal> mostra os seguintes códigos de status:
  </para>
    <variablelist>
      <varlistentry>
        <term><literal>A</literal> (ativo) ou <literal>NA</literal> (não ativo)</term>
        <listitem>
          <para>
          Mostra o status da conectividade entre o QDevice e o Corosync.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>V</literal> (com voto) ou <literal>NV</literal> (sem voto)</term>
        <listitem>
          <para>
          Mostra se o nó tem um voto. <literal>V</literal> significa que ambos os nós podem se comunicar entre si. Em um cenário de split brain, um nó é definido como <literal>V</literal> e o outro nó é definido como <literal>NV</literal>.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>MW</literal> (mestre vence) ou <literal>NMW</literal> (não mestre vence)</term>
        <listitem>
          <para>
          Mostra se o sinalizador <parameter>master_wins</parameter> está definido. Por padrão, o sinalizador não está definido, portanto, o status é <literal>NMW</literal>.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>NR</literal> (não registrado)</term>
        <listitem>
          <para>
          Mostra que o cluster não usa um dispositivo de quorum.
        </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <example xml:id="ex-qnetd-status">
      <title>Mostrando o status do QNetd</title>
      <screen><prompt>&gt; </prompt><command>sudo crm corosync status qnetd</command>
1 alice member
2 bob member

Cluster "hacluster":
    Algorithm:          Fifty-Fifty split (KAP Tie-breaker)
    Tie-breaker:        Node with lowest node ID
    Node ID 1:
        Client address:         ::ffff:192.168.122.185:45676
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.e
        Membership node list:   1, 2
        Heuristics:             Undefined (membership: Undefined, regular: Undefined)
        TLS active:             Yes (client certificate verified)
        Vote:                   ACK (ACK)
    Node ID 2:
        Client address:         ::ffff:192.168.100.168:55034
        HB interval:            8000ms
        Configured node list:   1, 2
        Ring ID:                1.e
        Membership node list:   1, 2
        Heuristics:             Undefined (membership: Undefined, regular: Undefined)
        TLS active:             Yes (client certificate verified)
        Vote:                   No change (ACK)</screen>
    </example>
  </section>
  <section role="glue" xml:lang="pt-br" version="5.2" xml:id="ha-testing-the-cluster-basic">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Testando o cluster</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <para>
    Os testes a seguir podem ajudar você a identificar problemas básicos na configuração do cluster. No entanto, um teste realista envolve casos de uso e cenários específicos. Antes de usar o cluster em um ambiente de produção, teste-o completamente de acordo com os seus casos de uso.
  </para>
    <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-testing-resource-failover">
      <info>
        <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Testando o failover de recursos</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
        <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
          <para>
        Verifique se o cluster move recursos para outro nó caso o nó atual esteja definido como <literal>standby</literal>. Este procedimento usa nós de exemplo chamados <systemitem>alice</systemitem> e <systemitem>bob</systemitem>, e um recurso IP virtual chamado <literal>admin-ip</literal> com o endereço IP de exemplo <literal>192.168.1.10</literal>.
      </para>
        </abstract>
      </info>
      <procedure>
        <step>
          <para>
        Abra dois terminais.
      </para>
        </step>
        <step>
          <para>
        No primeiro terminal, faça ping do endereço IP virtual:
      </para>
          <screen>
            <prompt>&gt; </prompt>
            <command>ping 192.168.1.10</command>
          </screen>
        </step>
        <step>
          <para>
        No segundo terminal, faça login em um dos nós do cluster.
      </para>
        </step>
        <step>
          <para>
        Verifique em qual nó o endereço IP virtual está em execução:
      </para>
          <screen><prompt>&gt; </prompt><command>sudo crm status</command>
[..]
Node List:
  * Online: [ alice bob ]

Full List of Resources:
  * admin-ip  (ocf:heartbeat:IPaddr2):    Started alice</screen>
        </step>
        <step>
          <para>
        Defina o modo de <systemitem>alice</systemitem> como standby:
      </para>
          <screen>
            <prompt>&gt; </prompt>
            <command>sudo crm node standby alice</command>
          </screen>
        </step>
        <step>
          <para>
        Verifique o status do cluster novamente. O recurso <literal>admin-ip</literal> deve ter migrado para <systemitem>bob</systemitem>:
      </para>
          <screen><prompt>&gt; </prompt><command>sudo crm status</command>
[...]
Node List:
  * Node alice: standby
  * Online: [ bob ]

Full List of Resources:
  * admin-ip  (ocf:heartbeat:IPaddr2):    Started bob</screen>
        </step>
        <step>
          <para>
        No primeiro terminal, você deve ver um fluxo contínuo de pings para o endereço IP virtual durante a migração. Isso mostra que a configuração do cluster e o endereço IP flutuante funcionam corretamente.
      </para>
        </step>
        <step>
          <para>
        Cancele o comando <command>ping</command> com <keycombo><keycap function="control"></keycap><keycap>C</keycap></keycombo>.
      </para>
        </step>
        <step>
          <para>
        No segundo terminal, coloque <systemitem>alice</systemitem> novamente online:
      </para>
          <screen>
            <prompt>&gt; </prompt>
            <command>sudo crm node online alice</command>
          </screen>
        </step>
      </procedure>
    </section>
    <section role="task" xml:lang="pt-br" version="5.2" xml:id="ha-testing-cluster-failures">
      <info>
        <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Testando falhas do cluster</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
        <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
          <para>
        O comando <command>crm cluster crash_test</command> simula falhas do cluster e relata os resultados.
      </para>
        </abstract>
      </info>
      <para>
    O comando suporta as seguintes verificações:
  </para>
      <variablelist>
        <varlistentry>
          <term>
            <option>--split-brain-iptables</option>
          </term>
          <listitem>
            <para>
          Simula um cenário de split brain bloqueando a porta do Corosync e verifica se é possível fazer fencing do nó conforme o esperado. Você deve instalar o pacote <package>iptables</package> antes de executar este teste.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--kill-sbd</option>/<option>--kill-corosync</option>/<option>--kill-pacemakerd</option></term>
          <listitem>
            <para>
          Encerra os daemons do SBD, Corosync ou Pacemaker. Após executar um desses testes, você encontrará um relatório no diretório <filename>/var/lib/crmsh/crash_test/</filename>. O relatório inclui a descrição de um caso de teste, o registro das ações e uma explicação dos possíveis resultados.
        </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>
            <option>--fence-node <replaceable>NODE</replaceable></option>
          </term>
          <listitem>
            <para>
          Delimita o nó específico passado pela linha de comando.
        </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
    Para obter mais informações, execute o comando <command>crm cluster crash_test --help</command>.
  </para>
      <para>
    O exemplo a seguir usa os nós chamados <systemitem>alice</systemitem> e <systemitem>bob</systemitem>, e testa o fencing de <systemitem>bob</systemitem>. Para observar a mudança de status de <systemitem>bob</systemitem> durante o teste, você pode fazer login no Hawk e navegar até <menuchoice><guimenu>Status</guimenu><guimenu>Nós</guimenu></menuchoice>.
  </para>
      <example>
        <title>Testando o cluster: fencing de nó</title>
        <screen><prompt>&gt; </prompt><command>sudo crm status</command>
[...]
Node List:
  * Online: [ alice bob ]

Active Resources:
  * admin-ip     (ocf:heartbeat:IPaddr2):    Started alice

<prompt>&gt; </prompt><command>sudo crm cluster crash_test --fence-node bob</command>

==============================================
Testcase:          Fence node bob
Fence action:      reboot
Fence timeout:     95

!!! WARNING WARNING WARNING !!!
THIS CASE MAY LEAD TO NODE BE FENCED.
TYPE Yes TO CONTINUE, OTHER INPUTS WILL CANCEL THIS CASE [Yes/No](No): <command>Yes</command>
INFO: Trying to fence node "bob"
INFO: Waiting 71s for node "bob" reboot...
INFO: Node "bob" will be fenced by "alice"!
INFO: Node "bob" was successfully fenced by "alice"</screen>
      </example>
    </section>
  </section>
  <section role="glue" xml:lang="pt-br" version="5.2" xml:id="ha-installing-cluster-next-steps">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Próximas etapas</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <para>
    Este guia descreve um cluster básico de alta disponibilidade que pode ser usado para fins de teste. Para expandir esse cluster para uso em ambientes de produção, mais etapas são recomendadas:
  </para>
    <variablelist>
      <varlistentry>
        <term>Adicionando mais nós</term>
        <listitem>
          <para>
          Adicione outros nós ao cluster usando o script <command>crm cluster join</command>.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Habilitando um watchdog de hardware</term>
        <listitem>
          <para>
          Antes de usar o cluster em um ambiente de produção, substitua <systemitem>softdog</systemitem> por um watchdog de hardware.
        </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Adicionando outros dispositivos STONITH</term>
        <listitem>
          <para>
          Para cargas de trabalho críticas, é altamente recomendável ter dois ou três dispositivos STONITH, usar dispositivos STONITH físicos ou SBD baseado em disco.
        </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section version="5.2" xml:id="legal-disclaimer">
    <info>
      <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Informações legais</title>
    </info>
    <para> Copyright© 2006 – <?dbtimestamp format="Y"?> SUSE LLC e colaboradores. Todos os direitos reservados. </para>
    <para>
    Permissão concedida para copiar, distribuir e/ou modificar este documento sob os termos da Licença GNU de Documentação Livre, Versão 1.2 ou (por sua opção) versão 1.3; com a Seção Invariante sendo estas informações de copyright e a licença. Uma cópia da versão 1.2 da licença está incluída na seção intitulada <quote>GNU Free Documentation License</quote> (Licença GNU de Documentação Livre).
  </para>
    <para>
    Para saber as marcas registradas da SUSE, visite <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.suse.com/company/legal/"></link>. Todas as marcas comerciais de terceiros pertencem a seus respectivos proprietários. Os símbolos de marca registrada (®, ™ etc.) indicam marcas registradas da SUSE e de suas afiliadas. Os asteriscos (*) indicam marcas registradas de terceiros.
  </para>
    <para>
    Todas as informações deste manual foram compiladas com a maior atenção possível aos detalhes. Entretanto, isso não garante uma precisão absoluta. A SUSE LLC, suas afiliadas, os autores ou tradutores não serão responsáveis por possíveis erros nem pelas consequências resultantes de tais erros.
  </para>
  </section>
  <appendix xmlns:its="http://www.w3.org/2005/11/its" version="5.2" role="legal" its:translate="no" xml:id="doc-gfdl-license">
    <info>
      <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">GNU Free Documentation License</title>
    </info>
    <para>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </para>
    <bridgehead renderas="sect4">
    0. PREAMBLE
  </bridgehead>
    <para>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </para>
    <para>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </para>
    <para>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </para>
    <bridgehead renderas="sect4">
    1. APPLICABILITY AND DEFINITIONS
  </bridgehead>
    <para>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </para>
    <para>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </para>
    <para>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </para>
    <para>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </para>
    <para>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </para>
    <para>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </para>
    <para>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </para>
    <para>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </para>
    <para>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </para>
    <para>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </para>
    <bridgehead renderas="sect4">
    2. VERBATIM COPYING
  </bridgehead>
    <para>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </para>
    <para>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </para>
    <bridgehead renderas="sect4">
    3. COPYING IN QUANTITY
  </bridgehead>
    <para>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </para>
    <para>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </para>
    <para>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </para>
    <para>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </para>
    <bridgehead renderas="sect4">
    4. MODIFICATIONS
  </bridgehead>
    <para>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </para>
    <orderedlist numeration="upperalpha" spacing="normal">
      <listitem>
        <para>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </para>
      </listitem>
      <listitem>
        <para>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </para>
      </listitem>
      <listitem>
        <para>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve all the copyright notices of the Document.
   </para>
      </listitem>
      <listitem>
        <para>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </para>
      </listitem>
      <listitem>
        <para>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </para>
      </listitem>
      <listitem>
        <para>
    Include an unaltered copy of this License.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </para>
      </listitem>
      <listitem>
        <para>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </para>
      </listitem>
      <listitem>
        <para>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </para>
      </listitem>
      <listitem>
        <para>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </para>
      </listitem>
      <listitem>
        <para>
    Preserve any Warranty Disclaimers.
   </para>
      </listitem>
    </orderedlist>
    <para>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </para>
    <para>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </para>
    <para>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </para>
    <para>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </para>
    <bridgehead renderas="sect4">
    5. COMBINING DOCUMENTS
  </bridgehead>
    <para>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </para>
    <para>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </para>
    <para>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </para>
    <bridgehead renderas="sect4">
    6. COLLECTIONS OF DOCUMENTS
  </bridgehead>
    <para>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </para>
    <para>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </para>
    <bridgehead renderas="sect4">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </bridgehead>
    <para>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </para>
    <para>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </para>
    <bridgehead renderas="sect4">
    8. TRANSLATION
  </bridgehead>
    <para>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </para>
    <para>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </para>
    <bridgehead renderas="sect4">
    9. TERMINATION
  </bridgehead>
    <para>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </para>
    <bridgehead renderas="sect4">
    10. FUTURE REVISIONS OF THIS LICENSE
  </bridgehead>
    <para>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.gnu.org/copyleft/"></link>.
 </para>
    <para>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </para>
    <bridgehead renderas="sect4">
    ADDENDUM: How to use this License for your documents
  </bridgehead>
    <screen>Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</screen>
    <para>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the &#x201C;with...Texts.&#x201D; line with this:
 </para>
    <screen>with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</screen>
    <para>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </para>
    <para>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
</para>
  </appendix>
  <glossary role="reference" xml:lang="pt-br" version="5.2" xml:id="ha-glossary">
    <info>
      <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Glossário de HA</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    </info>
    <glossentry>
      <glossterm>ativo/ativo, ativo/passivo</glossterm>
      <glossdef>
        <para>
      Como os recursos são executados nos nós. Ativo/passivo significa que os recursos são executados apenas no nó ativo, mas podem mover para o nó passivo em caso de falha no nó ativo. Ativo/ativo significa que todos os nós estão ativos ao mesmo tempo, e os recursos podem ser executados em (e movidos para) qualquer nó no cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-arbitrator">
      <glossterm>arbitrador</glossterm>
      <glossdef>
        <para>
      Um <emphasis>arbitrador</emphasis> é uma máquina executada fora do cluster que fornece uma instância adicional para cálculos de cluster. Por exemplo, <xref linkend="gloss-qnetd"/> fornece um voto para ajudar <xref linkend="gloss-qdevice"/> a participar das decisões de <xref linkend="gloss-quorum"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>CIB (Cluster Information Base)</glossterm>
      <glossdef>
        <para>
      Uma representação XML da configuração e do status de todo o cluster (opções, nós, recursos, restrições e relacionamentos entre os clusters). O gerenciador do CIB (<systemitem>pacemaker-based</systemitem>) mantém o CIB sincronizado com o cluster e processa as solicitações para modificá-lo.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-clone">
      <glossterm>clone</glossterm>
      <glossdef>
        <para>
      O <emphasis>clone</emphasis> é uma cópia idêntica de um nó existente, usada para simplificar a implantação de vários nós.
    </para>
        <para>
      No contexto de <xref linkend="gloss-resource"/> do cluster, o clone é um recurso que pode estar ativo em vários nós. Se o agente do recurso oferecer suporte ao procedimento, qualquer recurso poderá ser clonado.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>cluster</glossterm>
      <glossdef>
        <para>
      Um cluster de <emphasis>alta disponibilidade</emphasis> é um grupo de servidores (físicos ou virtuais) projetado principalmente para garantir a maior disponibilidade possível de aplicativos de dados e serviços. Não confunda com o cluster de <emphasis>alto desempenho</emphasis>, que compartilha a carga do aplicativo para acelerar os resultados.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>Gerenciador de volumes lógicos de cluster (LVM de cluster)</glossterm>
      <glossdef>
        <para>
      O termo <emphasis>LVM de cluster</emphasis> indica que o LVM é usado em um ambiente de cluster. Isso requer ajustes na configuração para proteger os metadados do LVM no armazenamento compartilhado.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-clus-part">
      <glossterm>partição de cluster</glossterm>
      <glossdef>
        <para>
      Uma partição de cluster ocorre quando a comunicação falha entre um ou mais nós e o restante do cluster. Os nós são divididos em partições, mas ainda estão ativos. Eles podem se comunicar apenas com os nós na mesma partição e não reconhecem os nós separados. Esse cenário é conhecido como <xref linkend="gloss-splitbrain"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>pilha do cluster</glossterm>
      <glossdef>
        <para>
      O conjunto de tecnologias e componentes de software que compõem um cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-col-con">
      <glossterm>restrição de colocation</glossterm>
      <glossdef>
        <para>
      Um tipo <xref linkend="gloss-resource-con"/> que especifica os recursos que podem ou não ser executados juntos em um nó.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>violação de simultaneidade</glossterm>
      <glossdef>
        <para>
      Um recurso que deve ser executado em apenas um nó no cluster e está sendo executado em vários nós.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-corosync">
      <glossterm>Corosync</glossterm>
      <glossdef>
        <para>
      O Corosync fornece informações confiáveis sobre o cluster referentes a mensagens, participação e quorum. Isso é feito pelo Corosync Cluster Engine, um sistema de comunicação em grupo.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-crm">
      <glossterm>CRM (gerenciador de recursos de cluster)</glossterm>
      <glossdef>
        <para>
      A entidade de gerenciamento responsável por coordenar todas as interações não locais em um cluster de alta disponibilidade. O SUSE Linux Enterprise High Availability usa o <xref linkend="gloss-pacemaker"/> como CRM. Ele interage com vários componentes: executores locais no próprio nó e em outros nós, CRMs não locais, comandos administrativos, funcionalidade de fencing e camada de participação.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm><systemitem>crmsh</systemitem> (shell de CRM)</glossterm>
      <glossdef>
        <para>
      O utilitário de linha de comando <emphasis><systemitem>crmsh</systemitem></emphasis> gerencia o cluster, os nós e os recursos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>Csync2</glossterm>
      <glossdef>
        <para>
      Uma ferramenta de sincronização para replicar arquivos de configuração por todos os nós do cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-dc">
      <glossterm>DC (coordenador designado)</glossterm>
      <glossdef>
        <para>
      O daemon <systemitem>pacemaker-controld</systemitem> é o controlador do cluster, que coordena todas as ações. Esse daemon tem uma instância em cada nó do cluster, mas apenas uma instância é escolhida para atuar como DC. A escolha do DC é feita quando os serviços do cluster são iniciados, ou se o DC atual falhar ou sair do cluster. O DC decide se deve ser feita uma alteração em todo o cluster, como fencing de nó ou movimentação de recursos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>desastre</glossterm>
      <glossdef>
        <para>
      Uma interrupção inesperada da infraestrutura crítica causada por forças da natureza, humanos, falhas de hardware ou bugs de software.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-dis-rec">
      <glossterm>recuperação de desastre</glossterm>
      <glossdef>
        <para>
      O processo responsável por restaurar uma função ao estado normal e estável após um desastre.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>plano de recuperação de desastre</glossterm>
      <glossdef>
        <para>
      Uma estratégia de recuperação após um desastre com impacto mínimo na infraestrutura de TI.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>DLM (Distributed Lock Manager)</glossterm>
      <glossdef>
        <para>
      O DLM coordena os acessos aos recursos compartilhados no cluster, por exemplo, gerenciando o bloqueio de arquivos em sistemas de arquivos em cluster para aumentar o desempenho e a disponibilidade.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-drbd">
      <glossterm>DRBD</glossterm>
      <glossdef>
        <para><trademark class="registered">DRBD</trademark> é um dispositivo de blocos projetado para criar clusters de alta disponibilidade. Ele replica os dados de um dispositivo principal para dispositivos secundários a fim de garantir que todas as cópias dos dados permaneçam idênticas.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>cluster existente</glossterm>
      <glossdef>
        <para>
      O termo <emphasis>cluster existente</emphasis> é usado em referência a qualquer cluster que tenha pelo menos um nó. Um cluster existente tem uma configuração básica do <xref linkend="gloss-corosync"/> que define os canais de comunicação, mas não necessariamente já tem uma configuração de recursos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="glo-failover">
      <glossterm>failover</glossterm>
      <glossdef>
        <para>
      Ocorre quando um recurso ou nó falha em uma máquina e os recursos afetados são movidos para outro nó.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>domínio de failover</glossterm>
      <glossdef>
        <para>
      Um subconjunto nomeado de nós do cluster qualificados para executar um recurso em caso de falha em um nó.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-fencing">
      <glossterm>fencing</glossterm>
      <glossdef>
        <para>
      Impede que membros do cluster isolados ou com falha acessem um recurso compartilhado. Há duas classes de fencing: no <emphasis>nível do recurso</emphasis> e no <emphasis>nível do nó</emphasis>. O fencing no nível do recurso garante acesso exclusivo a um recurso. O fencing no nível do nó impede que um nó com falha acesse recursos compartilhados e que os recursos sejam executados em um nó com status incerto. Geralmente, isso é feito redefinindo ou desligando o nó.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>GFS2</glossterm>
      <glossdef>
        <para>
      O sistema de arquivos global GFS2 (Global File System 2) é um sistema de arquivos em disco compartilhado para clusters de computadores Linux. O GFS2 permite que todos os nós tenham acesso direto e simultâneo ao mesmo armazenamento em blocos compartilhado. O GFS2 não tem um modo operacional desconectado nem funções de cliente ou de servidor. Todos os nós em um cluster GFS2 operam como pares. O GFS2 oferece suporte para até 32 nós do cluster. Para usar o GFS2 no cluster, é necessário um hardware para permitir o acesso ao armazenamento compartilhado e um gerenciador de bloqueio para controlar o acesso ao armazenamento.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>group</glossterm>
      <glossdef>
        <para>
      Os grupos de recursos contêm vários recursos que precisam estar juntos, ser iniciados em sequência e ser interrompidos na ordem inversa.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>Hawk (HA Web Konsole)</glossterm>
      <glossdef>
        <para>
      Uma interface baseada na Web e fácil de usar para monitorar e administrar um cluster de alta disponibilidade em máquinas Linux ou não Linux. É possível acessar o Hawk de qualquer máquina que tenha conexão com os nós do cluster usando um navegador da Web gráfico.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-heuristics">
      <glossterm>heurística</glossterm>
      <glossdef>
        <para>O <xref linkend="gloss-qdevice"/> suporta o uso de um conjunto de comandos (<emphasis>heurística</emphasis>) que é executado localmente na inicialização dos serviços do cluster, na alteração da participação do cluster, na conexão bem-sucedida com o servidor <xref linkend="gloss-qnetd"/> ou, opcionalmente, em horários regulares. O resultado é usado em cálculos para determinar a partição que deve ter <xref linkend="gloss-quorum"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>knet (kronosnet)</glossterm>
      <glossdef>
        <para>
      Uma camada de abstração de rede que suporta redundância, segurança, tolerância a falhas e failover rápido de links de rede. No SUSE Linux Enterprise High Availability 16, o <emphasis>knet</emphasis> é o protocolo de transporte padrão para os canais de comunicação do <xref linkend="gloss-corosync"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>cluster local</glossterm>
      <glossdef>
        <para>
      Um único cluster em um local (por exemplo, todos os nós localizados em um data center). A latência da rede é mínima. Normalmente, todos os nós acessam o armazenamento de forma síncrona.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>executor local</glossterm>
      <glossdef>
        <para>
      O executor local está localizado entre o <xref linkend="gloss-pacemaker"/> e os recursos em cada nó. Por meio do daemon <systemitem class="daemon">pacemaker-execd</systemitem>, o Pacemaker pode iniciar, parar e monitorar os recursos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>local</glossterm>
      <glossdef>
        <para>
      No contexto do cluster inteiro, <emphasis>local</emphasis> pode se referir ao local físico dos nós (por exemplo, todos os nós podem estar no mesmo data center). No contexto de <xref linkend="gloss-loc-con"/>, <emphasis>local</emphasis> se refere aos nós nos quais um recurso pode ou não ser executado.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-loc-con">
      <glossterm>restrição de local</glossterm>
      <glossdef>
        <para>
      Um tipo de <xref linkend="gloss-resource-con"/> que define os nós nos quais um recurso pode ou não ser executado.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>meta-atributos (opções de recursos)</glossterm>
      <glossdef>
        <para>
      Parâmetros que instruem como o <xref linkend="gloss-crm"/> deve tratar um <xref linkend="gloss-resource"/> específico. Por exemplo, você pode definir a prioridade ou a função de destino de um recurso.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>cluster de área metropolitana</glossterm>
      <glossdef>
        <para>
      Um único cluster que pode se estender por vários edifícios ou data centers, com todos os locais conectados por Fibre Channel. A latência da rede costuma ser baixa. O armazenamento é frequentemente replicado usando espelhamento ou replicação síncrona.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>vínculo de dispositivo de rede</glossterm>
      <glossdef>
        <para>
      O vínculo de dispositivo de rede combina duas ou mais interfaces de rede em um único dispositivo vinculado para aumentar a largura de banda e/ou fornecer redundância. Durante o uso do <xref linkend="gloss-corosync"/>, o dispositivo vinculado não é gerenciado pelo software de cluster. Portanto, o dispositivo vinculado deve ser configurado em cada nó do cluster que possa precisar acessá-lo.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>nó</glossterm>
      <glossdef>
        <para>
      Qualquer servidor (físico ou virtual) que seja membro de um cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-ord-con">
      <glossterm>restrição de ordem</glossterm>
      <glossdef>
        <para>
      Um tipo de <xref linkend="gloss-resource-con"/> que define a sequência de ações.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-pacemaker">
      <glossterm>Pacemaker</glossterm>
      <glossdef>
        <para>
      Pacemaker é o <xref linkend="gloss-crm"/> no SUSE Linux Enterprise High Availability, ou o <quote>cérebro</quote> que reage aos eventos que ocorrem no cluster. Os eventos podem ser nós que entram ou saem do cluster, falhas de recursos ou atividades programadas, como manutenção. O daemon <systemitem>pacemakerd</systemitem> inicia e monitora todos os outros daemons relacionados.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>parâmetros (atributos de instância)</glossterm>
      <glossdef>
        <para>
      Os parâmetros determinam qual instância de um serviço o <xref linkend="gloss-resource"/> controla.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>programador</glossterm>
      <glossdef>
        <para>
      O programador é implementado como <systemitem class="daemon">pacemaker-schedulerd</systemitem>. Quando uma transição de cluster é necessária, o <systemitem class="daemon">pacemaker-schedulerd</systemitem> calcula o próximo estado esperado do cluster e determina as ações que precisam ser programadas para alcançar o próximo estado.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>primitivo</glossterm>
      <glossdef>
        <para>
      Um recurso primitivo é o tipo de recurso mais básico do cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-prom-clone">
      <glossterm>clone promovível</glossterm>
      <glossdef>
        <para>
      Os clones promovíveis são um tipo especial de recurso de <xref linkend="gloss-clone"/> que pode ser promovido. As instâncias ativas desses recursos são divididas em dois estados: promovido e não promovido (também conhecidos como <quote>ativo e passivo</quote> ou <quote>principal e secundário</quote>).
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-qdevice">
      <glossterm>QDevice</glossterm>
      <glossdef>
        <para>
      O QDevice e o <xref linkend="gloss-qnetd"/> participam das decisões de <xref linkend="gloss-quorum"/>. O daemon <systemitem>corosync-qdevice</systemitem> é executado em cada nó do cluster e se comunica com o QNetd para fornecer um número configurável de votos, permitindo que um cluster suporte mais falhas de nós do que o permitido pelas regras de quorum padrão.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-qnetd">
      <glossterm>QNetD</glossterm>
      <glossdef>
        <para>
      O QNetD é um <xref linkend="gloss-arbitrator"/> executado fora do cluster. O daemon <systemitem>corosync-qnetd</systemitem> fornece um voto ao daemon <systemitem>corosync-qdevice</systemitem> em cada nó para ajudá-lo a participar das decisões de quorum.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-quorum">
      <glossterm>quorum</glossterm>
      <glossdef>
        <para>
      Uma <xref linkend="gloss-clus-part"/> será definida com quorum (como <emphasis>quorum atingido</emphasis>) se tiver a maioria dos nós (ou <quote>votos</quote>). O quorum distingue exatamente uma partição. Isso faz parte do algoritmo para impedir a execução de várias partições ou nós desconectados (<quote>split brain</quote>) que causa a corrupção de dados e de serviços. O quorum é um pré-requisito para o fencing, que depois garante a exclusividade do quorum.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>RA (agente de recurso)</glossterm>
      <glossdef>
        <para>
      Um script que atua como proxy para gerenciar um <xref linkend="gloss-resource"/> (por exemplo, para iniciar, interromper ou monitorar um recurso). O SUSE Linux Enterprise High Availability oferece suporte a vários tipos de agentes de recursos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>ReaR (Relax and Recover)</glossterm>
      <glossdef>
        <para>
      Um conjunto de ferramentas de administrador para criar imagens de <xref linkend="gloss-dis-rec"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-resource">
      <glossterm>recurso</glossterm>
      <glossdef>
        <para>
      Qualquer tipo de serviço ou aplicativo conhecido do <xref linkend="gloss-pacemaker"/>, por exemplo, endereço IP, sistema de arquivos ou banco de dados. O termo <emphasis>recurso</emphasis> também é usado para o <xref linkend="gloss-drbd"/>, em que ele nomeia um conjunto de dispositivos de blocos que usam uma conexão comum para replicação.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-resource-con">
      <glossterm>restrição de recursos</glossterm>
      <glossdef>
        <para>
      As restrições de recursos especificam em quais nós do cluster os recursos podem ser executados, em que ordem os recursos são carregados e de quais outros recursos um recurso específico depende.
    </para>
        <para>
      Consulte também <xref linkend="gloss-col-con"/>, <xref linkend="gloss-loc-con"/> e <xref linkend="gloss-ord-con"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>conjunto de recursos</glossterm>
      <glossdef>
        <para>
      Como um formato alternativo para definir as restrições de local, colocation ou ordem, você pode usar os <emphasis>conjuntos de recursos</emphasis>, em que os primitivos são agrupados em um único conjunto. Durante a criação de uma restrição, você pode especificar vários recursos aos quais a restrição será aplicada.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>modelo de recurso</glossterm>
      <glossdef>
        <para>
      Para ajudar a criar muitos recursos com configurações semelhantes, você pode definir um modelo de recurso. Após sua definição, ele poderá ser mencionado nos primitivos ou em determinados tipos de restrições. Se um modelo for mencionado em um primitivo, o primitivo herdará todas as operações, os atributos de instância (parâmetros), os meta-atributos e os atributos de utilização definidos no modelo.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-sbd">
      <glossterm>SBD (STONITH Block Device)</glossterm>
      <glossdef>
        <para>
      O SBD fornece um mecanismo de <xref linkend="gloss-fencing"/> de nó por meio da troca de mensagens pelo armazenamento em blocos compartilhado. Como alternativa, ele pode ser usado no modo sem disco. Nos dois casos, ele precisa de um <xref linkend="gloss-watchdog"/> de hardware ou de software em cada nó para garantir que os nós com comportamento inadequado sejam realmente interrompidos.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-splitbrain">
      <glossterm>split brain</glossterm>
      <glossdef>
        <para>
      Um cenário no qual os nós do cluster são divididos em dois ou mais grupos que não se reconhecem (seja por uma falha de software ou de hardware). O <xref linkend="gloss-stonith"/> evita que um cenário de split brain afete gravemente todo o cluster. Esse cenário também é conhecido como <emphasis>cluster particionado</emphasis>.
    </para>
        <para>
      O termo <emphasis>split brain</emphasis> também é usado no <xref linkend="gloss-drbd"/>, mas significa que os nós contêm dados diferentes.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>SPOF (ponto único de falha)</glossterm>
      <glossdef>
        <para>
      Qualquer componente de um cluster que, se falhar, acionará a falha de todo o cluster.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-stonith">
      <glossterm>STONITH</glossterm>
      <glossdef>
        <para>
      Acrônimo em inglês para <emphasis>shoot the other node in the head</emphasis>. Refere-se ao mecanismo de <xref linkend="gloss-fencing"/> que desliga um nó com comportamento inadequado para evitar que ele cause problemas no cluster. Em um cluster <xref linkend="gloss-pacemaker"/>, o STONITH é gerenciado pelo subsistema de fencing <systemitem>pacemaker-fenced</systemitem>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>transição</glossterm>
      <glossdef>
        <para>
      A movimentação planejada de recursos para outros nós em um cluster. Consulte também <xref linkend="glo-failover"/>.
    </para>
      </glossdef>
    </glossentry>
    <glossentry>
      <glossterm>utilização</glossterm>
      <glossdef>
        <para>
      Informa ao CRM a capacidade que um determinado <xref linkend="gloss-resource"/> exige do nó.
    </para>
      </glossdef>
    </glossentry>
    <glossentry xml:id="gloss-watchdog">
      <glossterm>watchdog</glossterm>
      <glossdef>
        <para>O <xref linkend="gloss-sbd"/> precisa de um watchdog em cada nó para garantir que os nós com comportamento inadequado sejam realmente interrompidos. O SBD grava regularmente um pulso de serviço no watchdog para <quote>alimentá-lo</quote>. Se o SBD parar de alimentar o watchdog, o hardware forçará a reinicialização do sistema. Isso protege o próprio processo do SBD contra falhas, por exemplo, travar em um erro de E/S.
    </para>
      </glossdef>
    </glossentry>
  </glossary>
</article>
