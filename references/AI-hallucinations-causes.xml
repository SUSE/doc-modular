<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ai-hallucinations-causes"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>What causes AI hallucinations?</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        The most common causes of hallucinations are:
      </para>
    </abstract>
  </info>
  <itemizedlist>
    <listitem>
      <para>
        <emphasis role="bold">Ambiguous prompts.</emphasis> Vague queries can
        lead to random or inaccurate answers.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">Lack of clear context.</emphasis> When the
        language model lacks context, it can fabricate answers.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">Long generation length.</emphasis> The longer the
        generated response, the higher the chance that hallucinations can
        happen.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">No retrieval-augmented process.</emphasis> LLMs
        without access to external sources&mdash;such as databases or search
        engines&mdash;can produce errors when they need to generate specific
        information.
      </para>
    </listitem>
  </itemizedlist>
</topic>
