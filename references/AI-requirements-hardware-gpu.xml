<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ai-hardware-requirements-gpu"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>GPU hardware for AI/ML workloads</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        To run AI/ML workloads, such as training machine learning models or
        running inference workloads, deploy cluster nodes with compatible
        &nvidia; GPUs to gain acceleration.
      </para>
    </abstract>
  </info>
  <section xml:id="ai-reqs-hw-gpu-operator">
    <title>Using the &nvoperator;</title>
    <para>
      Configuring and managing nodes with hardware resources can require
      multiple configurations for software components. These include drivers,
      container runtimes and libraries. To use &nvidia; GPUs in a &kube;
      cluster, you need to configure the &nvoperator;. Because GPU is a special
      resource in the cluster, you need to install the following components to
      enable deployment of workloads for processing on the GPU.
    </para>
    <itemizedlist>
      <listitem>
        <para>
          &nvidia; drivers (to enable CUDA)
        </para>
      </listitem>
      <listitem>
        <para>
          &kube; device plug-in
        </para>
      </listitem>
      <listitem>
        <para>
          Container runtime
        </para>
      </listitem>
      <listitem>
        <para>
          Other tools to provide capabilities such as monitoring or automatic
          node labeling
        </para>
      </listitem>
    </itemizedlist>
    <para>
      To ensure that the &nvoperator; is installed correctly, the &kube; cluster
      must meet the following prerequisites:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          All worker nodes must run the same operating system version to use the
          &nvidia; GPU Driver container.
        </para>
      </listitem>
      <listitem>
        <para>
          Nodes must be configured with a container engine, such as &docker;
          (CE/EE), &containerd; or &podman;.
        </para>
      </listitem>
      <listitem>
        <para>
          Nodes should be equipped with &nvidia; GPUs.
        </para>
      </listitem>
      <listitem>
        <para>
          Nodes should have &nvidia; drivers installed.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="ai-reqs-hw-supported-gpus">
    <title>Supported GPUs</title>
    <para>
      The &nvoperator; is compatible with a range of &nvidia; GPUs. For a full
      list of supported GPUs, refer to
      <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html">&nvoperator;
      Platform Support</link>.
    </para>
  </section>
</topic>
