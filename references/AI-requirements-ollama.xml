<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ai-requirements-ollama"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>&ollama; requirements</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        The version of &ollama; provided with &productname; is optimized for
        &nvidia; GPU hardware. This section guides you through the steps for
        configuring &ollama; on an &nvidia;-enabled system, including necessary
        configurations for both the hardware and software.
      </para>
    </abstract>
  </info>
  <tip>
    <title>General recommendations</title>
    <itemizedlist>
      <listitem>
        <para>
          <emphasis role="bold">Run &ollama; on &nvidia; GPU nodes.</emphasis>
          Since &ollama; is GPU-optimized, using the power of &nvidia; GPUs is
          essential for maximum performance. This ensures that the application
          runs efficiently and fully uses the hardware capabilities.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="bold">Assign applications to specific
          nodes.</emphasis> &productname; provides a mechanism to assign
          applications, such as &ollama;, to specific nodes. For more details,
          refer to
          <link xlink:href="&dsc;/suse-ai/1.0/html/AI-deployment-intro/index.html#ai-gpu-nodes-assigning"/>.
        </para>
      </listitem>
    </itemizedlist>
  </tip>
  <section xml:id="ai-reqs-ollama-hw">
    <title>Hardware requirements</title>
    <variablelist role="tabs">
      <varlistentry>
        <term>&nvidia; GPU</term>
        <listitem>
          <itemizedlist>
            <listitem>
              <para>
                The recommended GPU models include Tesla, A100, V100, RTX 30
                series, or other compatible &nvidia; GPUs.
              </para>
            </listitem>
            <listitem>
              <para>
                Ensure that the CUDA Compute Capability of your GPU is
                compatible with the required version of &ollama;.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>RAM</term>
        <listitem>
          <para>
            At least 16&nbsp;GB of RAM is recommended. However, higher amounts
            (32&nbsp;GB or more) may be necessary for larger models or
            workloads.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Disk space</term>
        <listitem>
          <para>
            At least 50&nbsp;GB of free disk space is recommended for storing
            the container images and any data files processed by &ollama;.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="ai-reqs-ollama-sw">
    <title>Software requirements</title>
    <variablelist role="tabs">
      <varlistentry>
        <term>&nvidia; &docker; (nvidia-docker)</term>
        <listitem>
          <itemizedlist>
            <listitem>
              <para>
                You must install <package>nvidia-docker</package> (the &nvidia;
                Container Toolkit) to allow &docker; containers to use the GPU.
                Refer to
                <link
                xlink:href="&dsc;/suse-ai/1.0/html/NVIDIA-Operator-installation/index.html"/>
                for more details.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>CUDA Toolkit</term>
        <listitem>
          <para>
            You must install the CUDA version supported by your GPU model. For
            most recent GPUs, CUDA 11.0 or later is required. Refer to
            <link
            xlink:href="https://developer.nvidia.com/cuda-toolkit">CUDA
            Toolkit installation guide</link> for more details.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>&nvidia; driver</term>
        <listitem>
          <para>
            Install the &nvidia; driver compatible with your GPU model. Its
            version must be compatible with the installed CUDA toolkit.
          </para>
          <tip>
            <para>
              You can check your GPU driver version by running the
              <command>nvidia-smi</command> command.
            </para>
          </tip>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
</topic>
