%entities; ++]>++

GPU hardware for AI/ML workloads

To run AI/ML workloads, such as training machine learning models or
running inference workloads, deploy cluster nodes with compatible
&nvidia; GPUs to gain acceleration.

Using the &nvoperator;

Configuring and managing nodes with hardware resources can require
multiple configurations for software components. These include drivers,
container runtimes and libraries. To use &nvidia; GPUs in a &kube;
cluster, you need to configure the &nvoperator;. Because GPU is a
special resource in the cluster, you need to install the following
components to enable deployment of workloads for processing on the GPU.

&nvidia; drivers (to enable CUDA)

&kube; device plug-in

Container runtime

Other tools to provide capabilities such as monitoring or automatic node
labeling

To ensure that the &nvoperator; is installed correctly, the &kube;
cluster must meet the following prerequisites:

All worker nodes must run the same operating system version to use the
&nvidia; GPU Driver container.

Nodes must be configured with a container engine, such as &docker;
(CE/EE), &containerd; or &podman;.

Nodes should be equipped with &nvidia; GPUs.

Nodes should have &nvidia; drivers installed.

Supported GPUs

The &nvoperator; is compatible with a range of &nvidia; GPUs. For a full
list of supported GPUs, refer to &nvoperator; Platform Support.
