<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_glossary.xml -->

<topic xml:id="ha-glossary"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
   <title>HA glossary</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
  </info>
  <glossentry><glossterm>active/active, active/passive</glossterm>
    <glossdef>
    <para>
      How services run on the nodes. An active/passive scenario means that services run on the
      active node and the passive node waits for the active node to fail. Active/active means that
      each node is active and passive at the same time. For example, it has <emphasis>some</emphasis>
      services running, but can take over other services from the other node. Compare with
      primary/secondary and dual-primary in &drbd; terminology.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>arbitrator</glossterm>
    <glossdef>
    <!--para>
      Additional instance in a &geo; cluster that helps to reach consensus
      about decisions such as failover of resources across sites. Arbitrators
      are single machines that run one or more booth instances in a special
      mode.
    </para-->
    <para>
      In regular clusters, <quote>arbitrator</quote> refers to &qnet;. &qnet; can be configured
      along with &qdevice; to participate in quorum decisions. This is recommended for clusters
      with an even number of nodes.
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>bindnetaddr (bind network address)</glossterm>
    <glossdef>
    <para>
      The network address the &corosync; executive should bind to.
    </para>
    </glossdef>
  </glossentry-->
  <!--glossentry xml:id="gloss-booth"><glossterm>booth</glossterm>
    <glossdef>
    <para>
      The instance that manages the failover process between the sites of a
      &geo; cluster. It aims to get multi-site resources active on one and
      only one site. This is achieved by using so-called tickets that are
      treated as failover domain between cluster sites, in case a site should
      be down.
    </para>
    </glossdef>
  </glossentry-->
  <!--glossentry><glossterm>boothd (booth daemon)</glossterm>
    <glossdef>
    <para>
      Each of the participating clusters and arbitrators in a &geo; cluster
      runs a service, the <systemitem class="daemon">boothd</systemitem>. It
      connects to the booth daemons running at the other sites and exchanges
      connectivity details.
    </para>
    </glossdef>
  </glossentry-->
  <glossentry><glossterm>CIB (cluster information base)</glossterm>
    <glossdef>
    <para>
      A representation of the whole cluster configuration and status (cluster options, nodes,
      resources, constraints and the relationship to each other). It is written in XML and resides
      in memory. A primary CIB is kept and maintained on the <xref linkend="gloss-dc"/> and
      replicated to the other nodes. Normal read and write operations on the CIB are serialized
      through the primary CIB.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-clone"><glossterm>clone</glossterm>
    <glossdef>
    <para>
      A <emphasis>clone</emphasis> can refer to an identical copy of an existing node.
      Cloning nodes can make deploying multiple nodes simpler.
    </para>
    <para>
      In the context of a cluster <xref linkend="gloss-resource"/>, a clone is a resource that can
      be active on multiple nodes. Any resource can be cloned if its resource agent supports it.
      A <xref linkend="gloss-prom-clone"/> is a special type of clone resource that can be promoted.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>cluster</glossterm>
    <glossdef>
    <para>
      A <emphasis>high-availability</emphasis> cluster is a group of computers (real or virtual)
      designed primarily to secure the highest possible availability of services. Not to be confused
      with a <emphasis>high-performance</emphasis> cluster, which shares the application load to
      achieve faster results.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Cluster logical volume manager (&clvm;)</glossterm>
    <glossdef>
    <para>
      The term <quote>&clvm;</quote> indicates that LVM is being used in a cluster environment.
      This requires configuration adjustments to protect the LVM metadata on shared storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>cluster partition</glossterm>
    <glossdef>
    <para>
      Whenever communication fails between one or more nodes and the rest of the cluster, a cluster
      partition occurs. The nodes of a cluster are split into partitions but still active. They can
      only communicate with nodes in the same partition and are unaware of the separated nodes. As
      the loss of the nodes on the other partition cannot be confirmed, a split-brain scenario
      develops (see also <xref linkend="gloss-splitbrain"/>).
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>cluster site</glossterm>
    <glossdef>
    <para>
      In &geo; clustering, a cluster site (or just <quote>site</quote>) is a group of
      nodes in the same physical location, managed by <xref linkend="gloss-booth"/>.
    </para>
    </glossdef>
  </glossentry-->
  <glossentry>
    <glossterm>cluster stack</glossterm>
    <glossdef>
    <para>
      The ensemble of software technologies and components that compose a cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-col-con">
    <glossterm>colocation constraint</glossterm>
    <glossdef>
    <para>
      Colocation constraints tell the cluster which resources can or cannot run together on a node.
      See also <xref linkend="gloss-resource-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>concurrency violation</glossterm>
    <glossdef>
    <para>
      A resource that should be running on only one node in the cluster is running on several nodes.
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>conntrack tools</glossterm>
    <glossdef>
    <para>
      Allow interaction with the in-kernel connection tracking system for enabling stateful packet
      inspection for iptables. Used by &sleha; to synchronize the connection status between
      cluster nodes.
    </para>
    </glossdef>
  </glossentry-->
  <glossentry><glossterm>&corosync;</glossterm>
    <glossdef>
    <para>
      &corosync; provides reliable messaging, membership and quorum information about the cluster.
      This is handled by the &corosync; Cluster Engine, a group communication system.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-crm"><glossterm>CRM (cluster resource manager)</glossterm>
    <glossdef>
    <para>
      The management entity responsible for coordinating all non-local interactions in a &ha; cluster.
      &sleha; uses <xref linkend="gloss-pacemaker"/> as the CRM. It interacts with several components:
      local resource managers on its own node and on the other nodes, non-local CRMs, administrative
      commands, the fencing functionality, and the membership layer.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&crmshell; (&crmsh;)</glossterm>
    <glossdef>
    <para>
      The command-line utility <quote>&crmsh;</quote> manages the cluster, nodes and resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&csync;</glossterm>
    <glossdef>
    <para>
      &corosync; provides reliable messaging, membership and quorum information about the cluster.
      This is handled by the &corosync; Cluster Engine, a group communication system.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-dc"><glossterm>DC (designated coordinator)</glossterm>
    <glossdef>
    <para>
      The DC is elected from all nodes in the cluster. This happens if there is no DC yet or if the
      current DC leaves the cluster for any reason. The DC is the only entity in the cluster that
      can decide that a cluster-wide change needs to be performed, such as fencing a node or moving
      resources. All other nodes get their configuration and resource allocation information from
      the current DC.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>disaster</glossterm>
    <glossdef>
    <para>
      An unexpected interruption of critical infrastructure induced by nature, humans, hardware
      failure, or software bugs.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Disaster Recovery</glossterm>
    <glossdef>
    <para>
      Disaster recovery is the process by which a function is restored to the normal, steady state
      after a disaster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Disaster Recover Plan</glossterm>
    <glossdef>
    <para>
      A strategy to recover from a disaster with minimum impact on IT infrastructure.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&dlm; (&dlm.long;)</glossterm>
    <glossdef>
    <para>
      &dlm; coordinates disk access for clustered file systems and administers file locking to
      increase performance and availability.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&drbd;</glossterm>
    <glossdef>
    <para>
      <trademark class="registered">&drbd;</trademark> is a block device designed for building
      &ha; clusters. The whole block device is mirrored via a dedicated network and is seen as a
      network RAID-1.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>existing cluster</glossterm>
    <glossdef>
    <para>
      The term <quote>existing cluster</quote> is used to refer to any cluster that consists of at
      least one node. Existing clusters have a basic &corosync; configuration that defines the
      communication channels, but do not necessarily have resource configuration yet.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="glo-failover"><glossterm>failover</glossterm>
    <glossdef>
    <para>
      Occurs when a resource or node fails on one machine and the affected resources move to
      another node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>failover domain</glossterm>
    <glossdef>
    <para>
      A named subset of cluster nodes that are eligible to run a cluster service if a node fails.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>fencing</glossterm>
    <glossdef>
    <para>
      Prevents access to a shared resource by isolated or failing cluster members. There are two
      classes of fencing: resource-level fencing and node-level fencing. Resource-level fencing
      ensures exclusive access to a resource. Node-level fencing prevents a failed node from
      accessing shared resources and prevents resources from running on a node whose status is
      uncertain. This is usually done by resetting or powering off the node.
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>&geo; cluster (geographically dispersed cluster)</glossterm>
    <glossdef>
    <para>
      Consists of multiple, &geo.dispersed; sites with a local cluster
      each. The sites communicate via IP. Failover across the sites is
      coordinated by a higher-level entity, the booth. &geo; clusters need
      to cope with limited network bandwidth and high latency. Storage is
      replicated asynchronously.
    </para>
    </glossdef>
  </glossentry-->
  <glossentry><glossterm>&gfs;</glossterm>
    <glossdef>
    <para>
      Global File System 2 or &gfs; is a shared disk file system for Linux computer clusters.
      &gfs; allows all nodes to have direct concurrent access to the same shared block storage.
      &gfs; has no disconnected operating mode, and no client or server roles. All nodes in a &gfs;
      cluster function as peers. &gfs; supports up to 32 cluster nodes. Using &gfs; in a cluster
      requires hardware to allow access to the shared storage, and a lock manager to control access
      to the storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>group</glossterm>
    <glossdef>
    <para>
      Resource groups contain multiple resources that need to be located together, started
      sequentially and stopped in the reverse order.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&hawk; (&haweb;)</glossterm>
    <glossdef>
    <para>
      A user-friendly Web-based interface for monitoring and administering a &ha; cluster from
      Linux or non-Linux machines. &hawk; can be accessed from any machine that can connect to the
      cluster nodes, using a graphical Web browser.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-heuristics"><glossterm>heuristics</glossterm>
    <glossdef>
    <para>
      <xref linkend="gloss-qdevice"/> supports using a set of commands (<quote>heuristics</quote>).
      The commands run locally on start-up of cluster services, cluster membership change,
      successful connection to the <xref linkend="gloss-qnetd"/> server, or, optionally, at regular
      times. The result is sent to the &qnet; server, where it is used in calculations to determine
      which partition should have <xref linkend="gloss-quorum"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>knet</glossterm>
    <glossdef>
    <para>
      TODO
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-lb"><glossterm>load balancing</glossterm>
    <glossdef>
    <para>
      The ability to make a cluster of servers appear as one large, fast server to outside clients.
      This is called a <emphasis>virtual server</emphasis>. It consists of one or more load balancers
      dispatching incoming requests and several real servers running the actual services.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>local cluster</glossterm>
    <glossdef>
    <para>
      A single cluster in one location (for example, all nodes are located in one data center).
      Network latency is minimal. Storage is typically accessed synchronously by all nodes.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>location</glossterm>
    <glossdef>
    <para>
      In the context of a whole cluster, <quote>location</quote> can refer to the physical
      location of nodes (for example, all nodes might be located in the same data center).
    </para>
    <para>
      In the context of a <emphasis>location constraint</emphasis>, <quote>location</quote>
      refers to the nodes on which a resource can or cannot run.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-loc-con"><glossterm>location constraint</glossterm>
    <glossdef>
    <para>
      Location constraints define the nodes on which a resource can or cannot run.
      See also <xref linkend="gloss-resource-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>LRM (local resource manager)</glossterm>
    <glossdef>
    <para>
      The local resource manager is located between &pace; and the resources on each node. It is
      implemented as the <systemitem class="daemon">pacemaker-execd</systemitem> daemon. Through
      this daemon, &pace; can start, stop and monitor resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>meta attributes (resource options)</glossterm>
    <glossdef>
    <para>
      Parameters that tell the CRM how to treat a specific <xref linkend="gloss-resource"/>.
      For example, the priority or the target role.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>metro cluster</glossterm>
    <glossdef>
    <para>
      A single cluster that can stretch over multiple buildings or data centers, with all sites
      connected by Fibre Channel. Network latency is usually low. Storage is frequently replicated
      (mirroring or synchronous replication).
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>network device bonding</glossterm>
    <glossdef>
    <para>
      Network device bonding combines two or more network interfaces into a single bonded device to
      increase bandwidth and/or provide redundancy. When using &corosync;, the bonded device is not
      managed by the cluster software. Therefore, the bonded device must be configured on every
      cluster node that might need to access it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>node</glossterm>
    <glossdef>
    <para>
      Any computer (real or virtual) that is a member of a cluster and invisible to the user.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-ord-con"><glossterm>order constraint</glossterm>
    <glossdef>
    <para>
      Order constraints define the sequence of actions. See also <xref linkend="gloss-resource-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-pacemaker"><glossterm>&pace;</glossterm>
    <glossdef>
    <para>
      &pace; is the <xref linkend="gloss-crm"/> in &sleha;. It is implemented as the cluster
      controller daemon <systemitem class="daemon">pacemaker-controld</systemitem>, which has an
      instance on each cluster node. All cluster decision-making is centralized by electing one of
      the <systemitem class="daemon">pacemaker-controld</systemitem> instances to act as a primary.
      If the elected <systemitem class="daemon">pacemaker-controld</systemitem> process fails
      (or the node it runs on fails), a different instance is elected as the primary.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>parameters (instance attributes)</glossterm>
    <glossdef>
    <para>
      Parameters determine which instance of a service the <xref linkend="gloss-resource"/> controls.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>PE (policy engine)</glossterm>
    <glossdef>
    <para>
      The policy engine is implemented as the <systemitem class="daemon">pacemaker-schedulerd</systemitem>
      daemon. When a cluster transition is needed, based on the current state and configuration,
      <systemitem class="daemon">pacemaker-schedulerd</systemitem> calculates the expected next
      state of the cluster. It determines what actions need to be scheduled to achieve the next state.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>primitive</glossterm>
    <glossdef>
    <para>
      A primitive resource is the most basic type of cluster <xref linkend="gloss-resource"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-prom-clone"><glossterm>promotable clone</glossterm>
    <glossdef>
    <para>
      Promotable clones are a special type of <xref linkend="gloss-clone"/> resource that can be
      promoted. Active instances of these resources are divided into two states: active and passive.
      These are also sometimes called primary and secondary.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qdevice"><glossterm>&qdevice;</glossterm>
    <glossdef>
    <para>
      &qdevice; and &qnet; participate in quorum decisions. &qnet; is the arbitrator that helps
      &qdevice; provide a configurable number of votes, allowing a cluster to sustain more node
      failures than the standard quorum rules allow.
      TODO: improve this
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qnetd"><glossterm>&qnet;</glossterm>
    <glossdef>
    <para>
      A systemd service (a daemon, the <quote>&qnet; server</quote>) which is not part of the cluster.
      &qnet; provides a vote to the <systemitem class="daemon">corosync-qdevice</systemitem> daemon
      to help it participate in quorum decisions.
      TODO: improve this
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-quorum"><glossterm>quorum</glossterm>
    <glossdef>
    <para>
      In a cluster, a cluster partition is defined to have quorum (be <quote>quorate</quote>) if it
      has the majority of nodes (or votes). Quorum distinguishes exactly one partition. It is part
      of the algorithm to prevent several disconnected partitions or nodes from proceeding and
      causing data and service corruption (split brain). Quorum is a prerequisite for fencing,
      which then ensures that quorum is indeed unique.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>RA (resource agent)</glossterm>
    <glossdef>
    <para>
      A script acting as a proxy to manage a resource (for example, to start, stop or monitor a
      resource). &sleha; supports different kinds of resource agents.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&rear; (Relax and Recover)</glossterm>
    <glossdef>
    <para>
      An administrator tool set for creating disaster recovery images.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource"><glossterm>resource</glossterm>
    <glossdef>
    <para>
      Any type of service or application that is known to &pace;. Examples include an IP address,
      a file system, or a database.
    </para>
    <para>
      The term <quote>resource</quote> is also used for &drbd;, where it names a set of block devices
      that are using a common connection for replication.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource-con"><glossterm>resource constraint</glossterm>
    <glossdef>
    <para>
      Resource constraints specify which cluster nodes resources can run on, what order resources
      load in, and what other resources a specific resource is dependent on.
    </para>
    <para>
      See also <xref linkend="gloss-col-con"/>, <xref linkend="gloss-loc-con"/> and
      <xref linkend="gloss-ord-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource set</glossterm>
    <glossdef>
    <para>
      As an alternative format for defining location, colocation or order constraints, you can use
      <emphasis>resource sets</emphasis>, where primitives are grouped together in one set. When
      creating a constraint, you can specify multiple resources for the constraint to apply to.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource template</glossterm>
    <glossdef>
    <para>
      To help create many resources with similar configurations, you can define a resource template.
      After being defined, it can be referenced in primitives or in certain types of constraints.
      If a template is referenced in a primitive, the primitive inherits all operations, instance
      attributes (parameters), meta attributes and utilization attributes defined in the template.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>RRP (redundant ring protocol)</glossterm>
    <glossdef>
    <para>
      Allows the use of multiple redundant local area networks for resilience against partial or
      total network faults. This way, cluster communication can still be kept up as long as a single
      network is operational. &corosync; supports the Totem Redundant Ring Protocol.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&sbd; (&sbd.long;)</glossterm>
    <glossdef>
    <para>
      Provides a node fencing mechanism through the exchange of messages via shared block storage
      (SAN, iSCSI, FCoE, etc.). Can also be used in diskless mode. Needs a hardware or software
      watchdog on each node to ensure that misbehaving nodes are really stopped.
      TODO: Improve this
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>SFEX (shared disk file exclusiveness)</glossterm>
    <glossdef>
    <para>
      SFEX provides storage protection over SAN.
    </para>
    </glossdef>
  </glossentry-->
  <glossentry xml:id="gloss-splitbrain"><glossterm>split-brain</glossterm>
    <glossdef>
    <para>
      A scenario in which the cluster nodes are divided into two or more groups that do not know
      about each other (either through a software or hardware failure). &stonith; prevents a
      split-brain situation from badly affecting the entire cluster. Also known as a
      <quote>partitioned cluster</quote> scenario.
    </para>
    <para>
      The term <literal>split brain</literal> is also used in &drbd; but means that the two nodes
      contain different data.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>SPOF (single point of failure)</glossterm>
    <glossdef>
    <para>
      Any component of a cluster that, if it fails, triggers the failure of the entire cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-stonith"><glossterm>&stonith;</glossterm>
    <glossdef>
    <para>
      The acronym for <quote>Shoot the other node in the head</quote>. It refers to the fencing
      mechanism that shuts down a misbehaving node to prevent it from causing trouble in a cluster.
      In a &pace; cluster, the implementation of node-level fencing is &stonith;. For this, &pace;
      comes with a fencing subsystem, <systemitem class="daemon">pacemaker-fenced</systemitem>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>switchover</glossterm>
    <glossdef>
    <para>
      Planned moving of services to other nodes in a cluster. See also <xref linkend="glo-failover"/>.
    </para>
    </glossdef>
  </glossentry>
  <!--glossentry><glossterm>ticket</glossterm>
    <glossdef>
    <para>
      A component used in &geo; clusters. A ticket grants the right to run
      certain resources on a specific cluster site. A ticket can only be owned
      by one site at a time. Resources can be bound to a certain ticket by
      dependencies. Only if the defined ticket is available at a site, the
      respective resources are started. Vice versa, if the ticket is removed,
      the resources depending on that ticket are automatically stopped.
    </para>
    </glossdef>
  </glossentry-->
  <!--glossentry><glossterm>unicast</glossterm>
    <glossdef>
    <para>
      A technology for sending messages to a single network destination. In &corosync;,
      unicast is implemented as UDP-unicast (UDPU).
    </para>
    </glossdef>
  </glossentry-->
  <glossentry><glossterm>utilization</glossterm>
    <glossdef>
    <para>
      Tells the CRM what capacity a certain <xref linkend="gloss-resource"/> requires from a node.
    </para>
    </glossdef>
  </glossentry>
</topic>
