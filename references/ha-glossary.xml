<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_glossary.xml -->

<topic xml:id="ha-glossary"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
   <title>HA glossary</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
  </info>
  <glossentry><glossterm>active/active, active/passive</glossterm>
    <glossdef>
    <para>
      How services run on the nodes. An active/passive scenario means that services run on the
      active node and migrate to the passive node if the active node fails. Active/active means that
      each node is active and passive at the same time. For example, it has <emphasis>some</emphasis>
      services running, but can take over other services from another node. Compare with
      primary/secondary and dual-primary in <xref linkend="gloss-drbd"/> terminology.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-arbitrator"><glossterm>arbitrator</glossterm>
    <glossdef>
    <para>
      An <emphasis>arbitrator</emphasis> is a machine running outside the cluster to provide an
      additional instance for cluster calculations. For example, <xref linkend="gloss-qnetd"/>
      provides a vote to help <xref linkend="gloss-qdevice"/> participate in
      <xref linkend="gloss-quorum"/> decisions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>CIB (cluster information base)</glossterm>
    <glossdef>
    <para>
      A representation of the whole cluster configuration and status (cluster options, nodes,
      resources, constraints and the relationship to each other). It is written in XML and resides
      in memory. A primary CIB is kept and maintained on the <xref linkend="gloss-dc"/> and
      replicated to the other nodes. Normal read and write operations on the CIB are serialized
      through the primary CIB.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-clone"><glossterm>clone</glossterm>
    <glossdef>
    <para>
      A <emphasis>clone</emphasis> is an identical copy of an existing node, used to make
      deploying multiple nodes simpler.
    </para>
    <para>
      In the context of a cluster <xref linkend="gloss-resource"/>, a clone is a resource that can
      be active on multiple nodes. Any resource can be cloned if its resource agent supports it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>cluster</glossterm>
    <glossdef>
    <para>
      A <emphasis>high-availability</emphasis> cluster is a group of computers (real or virtual)
      designed primarily to secure the highest possible availability of services. Not to be confused
      with a <emphasis>high-performance</emphasis> cluster, which shares the application load to
      achieve faster results.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Cluster logical volume manager (&clvm;)</glossterm>
    <glossdef>
    <para>
      The term <emphasis>&clvm;</emphasis> indicates that LVM is being used in a cluster environment.
      This requires configuration adjustments to protect the LVM metadata on shared storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-clus-part"><glossterm>cluster partition</glossterm>
    <glossdef>
    <para>
      A cluster partition occurs when communication fails between one or more nodes and the rest of
      the cluster. The nodes are split into partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the separated nodes. This
      is known as a <xref linkend="gloss-splitbrain"/> scenario.
    </para>
    </glossdef>
  </glossentry>
  <glossentry>
    <glossterm>cluster stack</glossterm>
    <glossdef>
    <para>
      The ensemble of software technologies and components that make up a cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-col-con">
    <glossterm>colocation constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that specifies which resources can or cannot
      run together on a node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>concurrency violation</glossterm>
    <glossdef>
    <para>
      A resource that should be running on only one node in the cluster is running on several nodes.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-corosync"><glossterm>&corosync;</glossterm>
    <glossdef>
    <para>
      &corosync; provides reliable messaging, membership and quorum information about the cluster.
      This is handled by the &corosync; Cluster Engine, a group communication system.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-crm"><glossterm>CRM (cluster resource manager)</glossterm>
    <glossdef>
    <para>
      The management entity responsible for coordinating all non-local interactions in a &ha; cluster.
      &sleha; uses <xref linkend="gloss-pacemaker"/> as the CRM. It interacts with several components:
      local resource managers on its own node and on the other nodes, non-local CRMs, administrative
      commands, the fencing functionality, and the membership layer.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&crmsh; (&crmshell;)</glossterm>
    <glossdef>
    <para>
      The command-line utility <emphasis>&crmsh;</emphasis> manages the cluster, nodes and resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&csync;</glossterm>
    <glossdef>
    <para>
      A synchronization tool for replicating configuration files across all nodes in the cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-dc"><glossterm>DC (designated coordinator)</glossterm>
    <glossdef>
    <para>
      The DC is elected from the cluster nodes when the cluster services start, or if the current DC
      leaves the cluster for any reason. The DC is the only entity in the cluster that can decide
      that a cluster-wide change must be performed, such as fencing a node or moving resources. All
      other nodes get their configuration and resource allocation information from the current DC.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>disaster</glossterm>
    <glossdef>
    <para>
      An unexpected interruption of critical infrastructure caused by nature, humans, hardware
      failure, or software bugs.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-dis-rec"><glossterm>disaster recovery</glossterm>
    <glossdef>
    <para>
      The process by which a function is restored to the normal, steady state after a disaster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Disaster Recovery Plan</glossterm>
    <glossdef>
    <para>
      A strategy to recover from a disaster with the minimum impact on IT infrastructure.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&dlm; (&dlm.long;)</glossterm>
    <glossdef>
    <para>
      &dlm; coordinates disk access for clustered file systems and administers file locking to
      increase performance and availability.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-drbd"><glossterm>&drbd;</glossterm>
    <glossdef>
    <para>
      <trademark class="registered">&drbd;</trademark> is a block device designed for building
      &ha; clusters. It replicates data on a primary device to secondary devices in a way that
      ensures all copies of the data remain identical.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>existing cluster</glossterm>
    <glossdef>
    <para>
      The term <emphasis>existing cluster</emphasis> is used to refer to any cluster that consists
      of at least one node. An existing cluster has a basic <xref linkend="gloss-corosync"/>
      configuration that defines the communication channels, but does not necessarily have resource
      configuration yet.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="glo-failover"><glossterm>failover</glossterm>
    <glossdef>
    <para>
      Occurs when a resource or node fails on one machine and the affected resources move to
      another node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>failover domain</glossterm>
    <glossdef>
    <para>
      A named subset of cluster nodes that are eligible to run a cluster service if a node fails.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-fencing"><glossterm>fencing</glossterm>
    <glossdef>
    <para>
      Prevents access to a shared resource by isolated or failing cluster members. There are two
      classes of fencing: <emphasis>resource-level</emphasis> fencing and <emphasis>node-level</emphasis>
      fencing. Resource-level fencing ensures exclusive access to a resource. Node-level fencing
      prevents a failed node from accessing shared resources and prevents resources from running on
      a node with an uncertain status. This is usually done by resetting or powering off the node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&gfs;</glossterm>
    <glossdef>
    <para>
      Global File System 2 (&gfs;) is a shared disk file system for Linux computer clusters.
      &gfs; allows all nodes to have direct concurrent access to the same shared block storage.
      &gfs; has no disconnected operating mode, and no client or server roles. All nodes in a &gfs;
      cluster function as peers. &gfs; supports up to 32 cluster nodes. Using &gfs; in a cluster
      requires hardware to allow access to the shared storage, and a lock manager to control access
      to the storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>group</glossterm>
    <glossdef>
    <para>
      Resource groups contain multiple resources that need to be located together, started
      sequentially and stopped in the reverse order.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&hawk; (&haweb;)</glossterm>
    <glossdef>
    <para>
      A user-friendly Web-based interface for monitoring and administering a &ha; cluster from
      Linux or non-Linux machines. &hawk; can be accessed from any machine that can connect to the
      cluster nodes, using a graphical Web browser.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-heuristics"><glossterm>heuristics</glossterm>
    <glossdef>
    <para>
      <xref linkend="gloss-qdevice"/> supports using a set of commands (<emphasis>heuristics</emphasis>)
      that run locally on start-up of cluster services, cluster membership change, successful
      connection to the <xref linkend="gloss-qnetd"/> server, or optionally at regular times. The
      result is used in calculations to determine which partition should have <xref linkend="gloss-quorum"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>knet (kronosnet)</glossterm>
    <glossdef>
    <para>
      A network abstraction layer supporting redundancy, security, fault tolerance, and fast fail-over of
      network links.In &sleha; 16, <emphasis>knet</emphasis> is the default transport mechanism for the
      <xref linkend="gloss-corosync"/> communication channels.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-lb"><glossterm>load balancing</glossterm>
    <glossdef>
    <para>
      The ability to make a cluster of servers appear as one large, fast server to outside clients.
      This is called a <emphasis>virtual server</emphasis>. It consists of one or more load balancers
      dispatching incoming requests and several real servers running the actual services.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>local cluster</glossterm>
    <glossdef>
    <para>
      A single cluster in one location (for example, all nodes are located in one data center).
      Network latency is minimal. Storage is typically accessed synchronously by all nodes.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>location</glossterm>
    <glossdef>
    <para>
      In the context of a whole cluster, <emphasis>location</emphasis> can refer to the physical
      location of nodes (for example, all nodes might be located in the same data center). In the
      context of a <xref linkend="gloss-loc-con"/>, <emphasis>location</emphasis> refers to the
      nodes on which a resource can or cannot run.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-loc-con"><glossterm>location constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that defines the nodes on which a resource can
      or cannot run.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>LRM (local resource manager)</glossterm>
    <glossdef>
    <para>
      The local resource manager is located between <xref linkend="gloss-pacemaker"/> and the
      resources on each node. Through the <systemitem class="daemon">pacemaker-execd</systemitem>
      daemon, &pace; can start, stop and monitor resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>meta attributes (resource options)</glossterm>
    <glossdef>
    <para>
      Parameters that tell the <xref linkend="gloss-crm"/> how to treat a specific
      <xref linkend="gloss-resource"/>. For example, you might define a resource's priority
      or target role.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>metro cluster</glossterm>
    <glossdef>
    <para>
      A single cluster that can stretch over multiple buildings or data centers, with all sites
      connected by Fibre Channel. Network latency is usually low. Storage is frequently replicated
      using mirroring or synchronous replication.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>network device bonding</glossterm>
    <glossdef>
    <para>
      Network device bonding combines two or more network interfaces into a single bonded device to
      increase bandwidth and/or provide redundancy. When using <xref linkend="gloss-corosync"/>, the
      bonded device is not managed by the cluster software. Therefore, the bonded device must be
      configured on every cluster node that might need to access it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>node</glossterm>
    <glossdef>
    <para>
      Any computer (real or virtual) that is a member of a cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-ord-con"><glossterm>order constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that defines the sequence of actions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-pacemaker"><glossterm>&pace;</glossterm>
    <glossdef>
    <para>
      &pace; is the <xref linkend="gloss-crm"/> in &sleha;. It is implemented as the cluster
      controller daemon <systemitem class="daemon">pacemaker-controld</systemitem>, which has an
      instance on each cluster node. All cluster decision-making is centralized by electing one of
      the <systemitem class="daemon">pacemaker-controld</systemitem> instances to act as a primary.
      If the elected <systemitem class="daemon">pacemaker-controld</systemitem> process fails
      (or the node it runs on fails), a different instance is elected as the primary.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>parameters (instance attributes)</glossterm>
    <glossdef>
    <para>
      Parameters determine which instance of a service the <xref linkend="gloss-resource"/> controls.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>PE (policy engine)</glossterm>
    <glossdef>
    <para>
      The policy engine is implemented as <systemitem class="daemon">pacemaker-schedulerd</systemitem>.
      When a cluster transition is needed, <systemitem class="daemon">pacemaker-schedulerd</systemitem>
      calculates the expected next state of the cluster and determines what actions need to be
      scheduled to achieve the next state.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>primitive</glossterm>
    <glossdef>
    <para>
      A primitive resource is the most basic type of cluster resource.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-prom-clone"><glossterm>promotable clone</glossterm>
    <glossdef>
    <para>
      Promotable clones are a special type of <xref linkend="gloss-clone"/> resource that can be
      promoted. Active instances of these resources are divided into two states: active and passive
      (or primary and secondary).
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qdevice"><glossterm>&qdevice;</glossterm>
    <glossdef>
    <para>
      &qdevice; and <xref linkend="gloss-qnetd"/> participate in <xref linkend="gloss-quorum"/>
      decisions. The <systemitem>corosync-qdevice</systemitem> daemon runs on each cluster node and
      communicates with &qnet; to provide a configurable number of votes, allowing a cluster to
      sustain more node failures than the standard quorum rules allow.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qnetd"><glossterm>&qnet;</glossterm>
    <glossdef>
    <para>
      &qnet; is an <xref linkend="gloss-arbitrator"/> that runs outside the cluster.
      The <systemitem>corosync-qnetd</systemitem> daemon provides a vote to the
      <systemitem>corosync-qdevice</systemitem> daemon on each node to help it participate
      in quorum decisions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-quorum"><glossterm>quorum</glossterm>
    <glossdef>
    <para>
      A <xref linkend="gloss-clus-part"/> is defined to have quorum (be <emphasis>quorate</emphasis>)
      if it has the majority of nodes (or votes). Quorum distinguishes exactly one partition. This
      is part of the algorithm to prevent several disconnected partitions or nodes from proceeding
      and causing data and service corruption (split brain). Quorum is a prerequisite for fencing,
      which then ensures that quorum is unique.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>RA (resource agent)</glossterm>
    <glossdef>
    <para>
      A script acting as a proxy to manage a <xref linkend="gloss-resource"/> (for example, to
      start, stop or monitor a resource). &sleha; supports different kinds of resource agents.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>&rear; (Relax and Recover)</glossterm>
    <glossdef>
    <para>
      An administrator tool set for creating <xref linkend="gloss-dis-rec"/> images.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource"><glossterm>resource</glossterm>
    <glossdef>
    <para>
      Any type of service or application that is known to <xref linkend="gloss-pacemaker"/>, for
      example, an IP address, a file system, or a database. The term <emphasis>resource</emphasis>
      is also used for <xref linkend="gloss-drbd"/>, where it names a set of block devices that use
      a common connection for replication.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource-con"><glossterm>resource constraint</glossterm>
    <glossdef>
    <para>
      Resource constraints specify which cluster nodes resources can run on, what order resources
      load in, and what other resources a specific resource is dependent on.
    </para>
    <para>
      See also <xref linkend="gloss-col-con"/>, <xref linkend="gloss-loc-con"/> and
      <xref linkend="gloss-ord-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource set</glossterm>
    <glossdef>
    <para>
      As an alternative format for defining location, colocation or order constraints, you can use
      <emphasis>resource sets</emphasis>, where primitives are grouped together in one set. When
      creating a constraint, you can specify multiple resources for the constraint to apply to.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource template</glossterm>
    <glossdef>
    <para>
      To help create many resources with similar configurations, you can define a resource template.
      After being defined, it can be referenced in primitives or in certain types of constraints.
      If a template is referenced in a primitive, the primitive inherits all operations, instance
      attributes (parameters), meta attributes and utilization attributes defined in the template.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>RRP (redundant ring protocol)</glossterm>
    <glossdef>
    <para>
      Allows the use of multiple redundant local area networks for resilience against partial or
      total network faults. This way, cluster communication can still be kept up as long as a single
      network is operational. &corosync; supports the Totem Redundant Ring Protocol.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-sbd"><glossterm>&sbd; (&sbd.long;)</glossterm>
    <glossdef>
    <para>
      &sbd; provides a node <xref linkend="gloss-fencing"/> mechanism through the exchange of
      messages via shared block storage. Alternatively, it can be used in diskless mode. In either
      case, it needs a hardware or software <xref linkend="gloss-watchdog"/> on each node to ensure
      that misbehaving nodes are really stopped.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-splitbrain"><glossterm>split brain</glossterm>
    <glossdef>
    <para>
      A scenario in which the cluster nodes are divided into two or more groups that do not know
      about each other (either through a software or hardware failure). <xref linkend="gloss-stonith"/>
      prevents a split-brain scenario from badly affecting the entire cluster. Also known as a
      <emphasis>partitioned cluster</emphasis> scenario.
    </para>
    <para>
      The term <emphasis>split brain</emphasis> is also used in <xref linkend="gloss-drbd"/> but
      means that the nodes contain different data.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>SPOF (single point of failure)</glossterm>
    <glossdef>
    <para>
      Any component of a cluster that, if it fails, triggers the failure of the entire cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-stonith"><glossterm>&stonith;</glossterm>
    <glossdef>
    <para>
      An acronym for <emphasis>shoot the other node in the head</emphasis>. It refers to the
      <xref linkend="gloss-fencing"/> mechanism that shuts down a misbehaving node to prevent it
      from causing trouble in a cluster. In a <xref linkend="gloss-pacemaker"/> cluster, &stonith;
      is managed by the fencing subsystem <systemitem>pacemaker-fenced</systemitem>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>switchover</glossterm>
    <glossdef>
    <para>
      The planned moving of services to other nodes in a cluster. See also <xref linkend="glo-failover"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>utilization</glossterm>
    <glossdef>
    <para>
      Tells the CRM what capacity a certain <xref linkend="gloss-resource"/> requires from a node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-watchdog"><glossterm>watchdog</glossterm>
    <glossdef>
    <para>
      <xref linkend="gloss-sbd"/> needs a watchdog on each node to ensure that misbehaving nodes are
      really stopped. &sbd; <quote>feeds</quote> the watchdog by regularly writing a service pulse
      to it. If &sbd; stops feeding the watchdog, the hardware enforces a system restart. This
      protects against failures of the &sbd; process itself, such as becoming stuck on an I/O error.
    </para>
    </glossdef>
  </glossentry>
</topic>
