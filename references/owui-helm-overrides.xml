<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="owui-helm-overrides"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Examples of &owui; &helm; chart override files</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <xi:include href="../snippets/helm-chart-overrides-intro.xml"/>
    </abstract>
  </info>
  <example xml:id="owui-helm-overrides-ollama">
    <title>&owui; override file with &ollama; included</title>
    <para>
      The following override file installs &ollama; during the &owui;
      installation. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable> with
      your &kube; namespace.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
image:
  registry: dp.apps.rancher.io
  repository: containers/open-webui
  tag: 0.3.32
  pullPolicy: IfNotPresent
ollamaUrls:
- http://open-webui-ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath1"/>
ollama:
  enabled: true
  image:
    registry: dp.apps.rancher.io
    repository: containers/ollama
    tag: 0.3.6
    pullPolicy: IfNotPresent
  ingress:
    enabled: false
  defaultModel: "gemma:2b"
  ollama:
    models:<co xml:id="co-ollama-models"/>
      - "gemma:2b"
      - "llama3.1"
    gpu:<co xml:id="co-ollama-gpu"/>
      enabled: true
      type: 'nvidia'
      number: 1
    persistentVolume:<co xml:id="co-ollama-persistent1"/>
      enabled: true
      storageClass: local-path<co xml:id="co-ollama-localpath2"/>
pipelines:
  enabled: False
  persistence:
    storageClass: local-path<co xml:id="co-ollama-localpath3"/>
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530
- name: INSTALL_NLTK_DATASETS<co xml:id="co-ollama-extravars-nltk"/>
  value: "true"</screen>
    <calloutlist>
      <callout arearefs="co-ollama-models">
        <para>
          Specifies that two large language models (LLM) will be loaded in
          &ollama; when the container starts.
        </para>
      </callout>
      <callout arearefs="co-ollama-gpu">
        <para>
          Enables GPU support for &ollama;. The <option>type</option> must be
          <literal>nvidia</literal> because &nvidia; GPUs are the only supported
          devices. <option>number</option> must be between 1 and the number of
          &nvidia; GPUs present on the system.
        </para>
      </callout>
      <callout arearefs="co-ollama-persistent1">
        <para>
          Without the <option>persistentVolume</option> option enabled, changes
          made to &ollama;&mdash;such as downloading other LLM&mdash; are lost
          when the container is restarted.
        </para>
      </callout>
      <callout arearefs="co-ollama-localpath1 co-ollama-localpath2 co-ollama-localpath3">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution suitable for
          persistent storage, such as &sstorage;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars-nltk">
        <para>
          Installs the <emphasis>natural language toolkit</emphasis> (NLTK)
          datasets for &ollama;. Refer to <link
          xlink:href="https://www.nltk.org/index.html"/> for licensing
          information.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example>
    <title>&owui; override file with &ollama; installed separately</title>
    <para>
      The following override file installs &ollama; separately from the &owui;
      installation. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable> with
      your &kube; namespace.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
image:
  registry: dp.apps.rancher.io
  repository: containers/open-webui
  tag: 0.3.32
  pullPolicy: IfNotPresent
ollamaUrls:
- http://ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath4"/>
ollama:
  enabled: false
pipelines:
  enabled: False
  persistence:
    storageClass: local-path<co xml:id="co-ollama-localpath5"/>
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars1"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530</screen>
    <calloutlist>
      <callout arearefs="co-ollama-localpath4 co-ollama-localpath5">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution suitable for
          persistent storage, such as &sstorage;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars1">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
    </calloutlist>
  </example>
</topic>
