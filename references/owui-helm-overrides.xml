<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="owui-helm-overrides"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Examples of &owui; &helm; chart override files</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <xi:include href="../snippets/helm-chart-overrides-intro.xml"/>
    </abstract>
  </info>
  <example xml:id="owui-helm-overrides-ollama">
    <title>&owui; override file with &ollama; included</title>
    <para>
      The following override file installs &ollama; during the &owui;
      installation.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
  <phrase condition="deployment_airgap">imageRegistry: <replaceable>LOCAL_DOCKER_REGISTRY_URL</replaceable>:5043</phrase>
ollamaUrls:
- http://open-webui-ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath1"/>
ollama:
  enabled: true
  ingress:
    enabled: false
  defaultModel: "gemma:2b"
  ollama:
    models:<co xml:id="co-ollama-models"/>
      - "gemma:2b"
      - "llama3.1"
    gpu:<co xml:id="co-ollama-gpu"/>
      enabled: true
      type: 'nvidia'
      number: 1
    persistentVolume:<co xml:id="co-ollama-persistent1"/>
      enabled: true
      storageClass: local-path
pipelines:
  enabled: true
  persistence:
    storageClass: local-path
  extraEnvVars: <co xml:id="co-pipelines-extraenvvars"/>
    - name: PIPELINES_URLS <co xml:id="co-pipelines-extraenvvars-pipelines-urls"/>
      value: "https://raw.githubusercontent.com/SUSE/suse-ai-observability-extension/refs/heads/main/integrations/oi-filter/suse_ai_filter.py"
    - name: OTEL_SERVICE_NAME <co xml:id="co-pipelines-extraenvvars-otel-service-name"/>
      value: "Open WebUI"
    - name: OTEL_EXPORTER_HTTP_OTLP_ENDPONT <co xml:id="co-pipelines-extraenvvars-otel-exporter-http-otlp-endpoint"/>
      value: "http://opentelemetry-collector.suse-observability.svc.cluster.local:4318"
    - name: PRICING_JSON <co xml:id="co-pipelines-extraenvvars-pricing-json"/>
      value: "https://raw.githubusercontent.com/SUSE/suse-ai-observability-extension/refs/heads/main/integrations/oi-filter/pricing.json"
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "1024m"
  host: suse-ollama-webui<co xml:id="co-ollama-webui"/>
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530
- name: INSTALL_NLTK_DATASETS<co xml:id="co-ollama-extravars-nltk"/>
  value: "true"
- name: OMP_NUM_THREADS
  value: "1"
- name: OPENAI_API_KEY <co xml:id="co-extraenvvars-openai-api-key"/>
  value: "0p3n-w3bu!"</screen>
    <calloutlist>
      <callout arearefs="co-ollama-localpath1">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution more suitable
          for persistent storage. To use &sstorage;, specify
          <literal>longhorn</literal>.
        </para>
      </callout>
      <callout arearefs="co-ollama-models">
        <para>
          Specifies that two large language models (LLM) will be loaded in
          &ollama; when the container starts.
        </para>
      </callout>
      <callout arearefs="co-ollama-gpu">
        <para>
          Enables GPU support for &ollama;. The <option>type</option> must be
          <literal>nvidia</literal> because &nvidia; GPUs are the only supported
          devices. <option>number</option> must be between 1 and the number of
          &nvidia; GPUs present on the system.
        </para>
      </callout>
      <callout arearefs="co-ollama-persistent1">
        <para>
          Without the <option>persistentVolume</option> option enabled, changes
          made to &ollama;&mdash;such as downloading other LLM&mdash; are lost
          when the container is restarted.
        </para>
      </callout>
      <callout arearefs="co-pipelines-extraenvvars">
        <para>
          The environment variables that you are making available for the
          pipeline's runtime container.
        </para>
      </callout>
      <callout arearefs="co-pipelines-extraenvvars-pipelines-urls">
        <para>
          A list of pipeline URLs to be downloaded and installed by default.
          Individual URLs are separated by a semicolon <literal>;</literal>.
        </para>
        <para condition="deployment_airgap">
          For air-gapped deployments, you need to provide the pipelines at URLs
          that are accessible from the local host, such as an internal GitLab
          instance.
        </para>
      </callout>
      <callout arearefs="co-pipelines-extraenvvars-otel-service-name">
        <para>
          The service name that appears in traces and topological
          representations in &sobservability;.
        </para>
      </callout>
      <callout arearefs="co-pipelines-extraenvvars-otel-exporter-http-otlp-endpoint">
        <para>
          The endpoint for the &otelemetry; collector. Make sure to use the HTTP
          port of your collector.
        </para>
      </callout>
      <callout arearefs="co-pipelines-extraenvvars-pricing-json">
        <para>
          A file for the model multipliers in cost estimation. You can customize
          it to match your actual infrastructure experimentally.
        </para>
        <para condition="deployment_airgap">
          For air-gapped deployments, you need to provide the pipelines at URLs
          that are accessible from the local host, such as an internal GitLab
          instance.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
      <callout arearefs="co-ollama-webui">
        <para>
          Specifies the host name for the &owui; Web UI.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars-nltk">
        <para>
          Installs the <emphasis>natural language toolkit</emphasis> (NLTK)
          datasets for &ollama;. Refer to
          <link
          xlink:href="https://www.nltk.org/index.html"/> for
          licensing information.
        </para>
      </callout>
      <callout arearefs="co-extraenvvars-openai-api-key">
        <para>
          API key value for communication between &owui; and &owui; Pipelines.
          The default value is <quote>0p3n-w3bu!</quote>.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="owui-ollama-deploy-separate">
    <title>&owui; override file with &ollama; installed separately</title>
    <para>
      The following override file installs &ollama; separately from the &owui;
      installation.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
  <phrase condition="deployment_airgap">imageRegistry: <replaceable>LOCAL_DOCKER_REGISTRY_URL</replaceable>:5043</phrase>
ollamaUrls:
- http://ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath4"/>
ollama:
  enabled: false
pipelines:
  enabled: False
  persistence:
    storageClass: local-path<co xml:id="co-ollama-localpath5"/>
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars1"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530
- name: ENABLE_OTEL<co xml:id="co-owui-otel1"/>
  value: "true"
- name: OTEL_EXPORTER_OTLP_ENDPOINT<co xml:id="co-owui-otel2"/>
  value: http://opentelemetry-collector.observability.svc.cluster.local:4317<co xml:id="co-llama-otel"/>
- name: OMP_NUM_THREADS
  value: "1"</screen>
    <calloutlist>
      <callout arearefs="co-ollama-localpath4 co-ollama-localpath5">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution suitable for
          persistent storage, such as &sstorage;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars1">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
      <callout arearefs="co-owui-otel1 co-owui-otel2">
        <para>
          These values are optional, required only to receive telemetry data
          from &owui;.
        </para>
      </callout>
      <callout arearefs="co-llama-otel">
        <para>
          The URL of the &otelemetry; Collector installed by the user.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="owui-ollama-deploy-pipelines">
    <title>&owui; override file with pipelines enabled</title>
    <para>
      The following override file installs &ollama; separately and enables
      &owui; pipelines. This simple filter adds a limit to the number of
      question and answer turns during the LLM chat.
    </para>
    <tip>
      <para>
        Pipelines normally require additional configuration provided either via
        environment variables or specified in the &owui; Web UI.
      </para>
    </tip>
<screen>global:
  imagePullSecrets:
  - application-collection
  <phrase condition="deployment_airgap">imageRegistry: <replaceable>LOCAL_DOCKER_REGISTRY_URL</replaceable>:5043</phrase>
ollamaUrls:
- http://ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path
ollama:
  enabled: false
pipelines:
  enabled: true
  persistence:
    storageClass: local-path
  extraEnvVars:
  - name: PIPELINES_URLS <co xml:id="co-extraenvvars-pipelines-urls"/>
    value: "https://raw.githubusercontent.com/SUSE/suse-ai-observability-extension/refs/heads/main/integrations/oi-filter/conversation_turn_limit_filter.py"
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui
  tls: true
[...]</screen>
    <calloutlist>
      <callout arearefs="co-extraenvvars-pipelines-urls">
        <para>
          A list of pipeline URLs to be downloaded and installed by default.
          Individual URLs are separated by a semicolon <literal>;</literal>.
        </para>
        <para condition="deployment_airgap">
          For air-gapped deployments, you need to provide the pipelines at URLs
          that are accessible from the local host, such as an internal GitLab
          instance.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="owui-ollama-deploy-vllm" condition="deployment_standard">
    <title>&owui; override file with a connection to &vllm;</title>
    <para>
      The following example shows how to extend the
      <literal>extraEnvVars</literal> section of the &owui; override file to
      connect to &vllm;. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable>
      with your &kube; namespace.
    </para>
    <tip>
      <para>
        Find more details about installing &vllm; in
        <xref linkend="vllm-installing"/>.
      </para>
    </tip>
<screen>extraEnvVars:
[...]
- name: OPENAI_API_BASE_URL
  value: "http://vllm-router-service.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:80/v1"
- name: OPENAI_API_KEY
  value: "dummy" <co xml:id="co-vllm-apikey"/></screen>
    <calloutlist>
      <callout arearefs="co-vllm-apikey">
        <para>
          &owui; will require you to provide the &openai; API key.
        </para>
      </callout>
    </calloutlist>
    <para>
      If the &owui; installation has pipelines enabled besides the &vllm;
      deployment, you can extend the <literal>extraEnvVars</literal> section as
      follows.
    </para>
<screen>extraEnvVars:
[...]
- name: OPENAI_API_BASE_URLS
  value: "http://open-webui-pipelines.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:9099;http://vllm-router-service.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:80/v1"
- name: OPENAI_API_KEYS
  value: "0p3n-w3bu!;dummy"</screen>
  </example>
</topic>
