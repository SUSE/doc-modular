<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="owui-helm-overrides"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Examples of &owui; &helm; chart override files</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <xi:include href="../snippets/helm-chart-overrides-intro.xml"/>
    </abstract>
  </info>
  <example xml:id="owui-helm-overrides-ollama">
    <title>&owui; override file with &ollama; included</title>
    <para>
      The following override file installs &ollama; during the &owui;
      installation. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable> with
      your &kube; namespace.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
  <phrase condition="deployment_airgap">imageRegistry: <replaceable>LOCAL_DOCKER_REGISTRY_URL</replaceable>:5043</phrase>
ollamaUrls:
- http://open-webui-ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath1"/>
ollama:
  enabled: true
  ingress:
    enabled: false
  defaultModel: "gemma:2b"
  ollama:
    models:<co xml:id="co-ollama-models"/>
      - "gemma:2b"
      - "llama3.1"
    gpu:<co xml:id="co-ollama-gpu"/>
      enabled: true
      type: 'nvidia'
      number: 1
    persistentVolume:<co xml:id="co-ollama-persistent1"/>
      enabled: true
      storageClass: local-path<co xml:id="co-ollama-localpath2"/>
pipelines:
  enabled: False
  persistence:
    storageClass: local-path<co xml:id="co-ollama-localpath3"/>
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui<co xml:id="co-ollama-webui"/>
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530
- name: INSTALL_NLTK_DATASETS<co xml:id="co-ollama-extravars-nltk"/>
  value: "true"</screen>
    <calloutlist>
      <callout arearefs="co-ollama-models">
        <para>
          Specifies that two large language models (LLM) will be loaded in
          &ollama; when the container starts.
        </para>
      </callout>
      <callout arearefs="co-ollama-gpu">
        <para>
          Enables GPU support for &ollama;. The <option>type</option> must be
          <literal>nvidia</literal> because &nvidia; GPUs are the only supported
          devices. <option>number</option> must be between 1 and the number of
          &nvidia; GPUs present on the system.
        </para>
      </callout>
      <callout arearefs="co-ollama-persistent1">
        <para>
          Without the <option>persistentVolume</option> option enabled, changes
          made to &ollama;&mdash;such as downloading other LLM&mdash; are lost
          when the container is restarted.
        </para>
      </callout>
      <callout arearefs="co-ollama-localpath1 co-ollama-localpath2 co-ollama-localpath3">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution suitable for
          persistent storage, such as &sstorage;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
      <callout arearefs="co-ollama-webui">
        <para>
          Specifies the host name for the &owui; Web UI.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars-nltk">
        <para>
          Installs the <emphasis>natural language toolkit</emphasis> (NLTK)
          datasets for &ollama;. Refer to
          <link
          xlink:href="https://www.nltk.org/index.html"/> for
          licensing information.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="owui-ollama-deploy-separate">
    <title>&owui; override file with &ollama; installed separately</title>
    <para>
      The following override file installs &ollama; separately from the &owui;
      installation. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable> with
      your &kube; namespace.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
  <phrase condition="deployment_airgap">imageRegistry: <replaceable>LOCAL_DOCKER_REGISTRY_URL</replaceable>:5043</phrase>
ollamaUrls:
- http://ollama.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:11434
persistence:
  enabled: true
  storageClass: local-path<co xml:id="co-ollama-localpath4"/>
ollama:
  enabled: false
pipelines:
  enabled: False
  persistence:
    storageClass: local-path<co xml:id="co-ollama-localpath5"/>
ingress:
  enabled: true
  class: ""
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  host: suse-ollama-webui
  tls: true
extraEnvVars:
- name: DEFAULT_MODELS<co xml:id="co-ollama-extravars1"/>
  value: "gemma:2b"
- name: DEFAULT_USER_ROLE
  value: "user"
- name: WEBUI_NAME
  value: "SUSE AI"
- name: GLOBAL_LOG_LEVEL
  value: INFO
- name: RAG_EMBEDDING_MODEL
  value: "sentence-transformers/all-MiniLM-L6-v2"
- name: VECTOR_DB
  value: "milvus"
- name: MILVUS_URI
  value: http://milvus.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:19530
- name: ENABLE_OTEL<co xml:id="co-owui-otel1"/>
  value: "true"
- name: OTEL_EXPORTER_OTLP_ENDPOINT<co xml:id="co-owui-otel2"/>
  value: http://opentelemetry-collector.observability.svc.cluster.local:4317<co xml:id="co-llama-otel"/></screen>
    <calloutlist>
      <callout arearefs="co-ollama-localpath4 co-ollama-localpath5">
        <para>
          Use <option>local-path</option> storage only for testing purposes. For
          production use, we recommend using a storage solution suitable for
          persistent storage, such as &sstorage;.
        </para>
      </callout>
      <callout arearefs="co-ollama-extravars1">
        <para>
          Specifies the default LLM for &ollama;.
        </para>
      </callout>
      <callout arearefs="co-owui-otel1 co-owui-otel2">
        <para>
          These values are optional, required only to receive telemetry data
          from &owui;.
        </para>
      </callout>
      <callout arearefs="co-llama-otel">
        <para>
          The URL of the &otelemetry; Collector installed by the user.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="owui-ollama-deploy-vllm">
    <title>&owui; override file with a connection to &vllm;</title>
    <para>
      The following example shows how to extend the
      <literal>extraEnvVars</literal> section of the &owui; override file to
      connect to &vllm;. Replace <replaceable>SUSE_AI_NAMESPACE</replaceable>
      with your &kube; namespace.
    </para>
    <tip>
      <para>
        Find more details about installing &vllm; in
        <xref
        linkend="vllm-installing"/>.
      </para>
    </tip>
<screen>extraEnvVars:
[...]
- name: OPENAI_API_BASE_URL
  value: "http://vllm-router-service.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:80/v1"
- name: OPENAI_API_KEY
  value: "dummy" <co xml:id="co-vllm-apikey"/></screen>
    <calloutlist>
      <callout arearefs="co-vllm-apikey">
        <para>
          &owui; will require you to provide the &openai; API key.
        </para>
      </callout>
    </calloutlist>
    <para>
      If the &owui; installation has pipelines enabled besides the &vllm;
      deployment, you can extend the <literal>extraEnvVars</literal> section as
      follows.
    </para>
<screen>extraEnvVars:
[...]
- name: OPENAI_API_BASE_URLS
  value: "http://open-webui-pipelines.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:9099;http://vllm-router-service.<replaceable>SUSE_AI_NAMESPACE</replaceable>.svc.cluster.local:80/v1"
- name: OPENAI_API_KEYS
  value: "0p3n-w3bu!;dummy"</screen>
  </example>
</topic>
