<?xml version="1.0"?>
<!DOCTYPE topic [
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
  %entities;
]>

<topic xml:id="qemu-host-installation" role="reference" xml:lang="en" xmlns="http://docbook.org/ns/docbook" version="5.2" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
  <title>Setting up a &kvm; &vmhost;</title>
  <info xmlns:d="http://docbook.org/ns/docbook">
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker/>
      <dm:translation>yes</dm:translation>
    </dm:docmanager>
</info>
  <para>
    This section documents how to set up and use &productname; &productnumber;
    as a &qemu;-&kvm; based virtual machine host.
  </para>
  <tip>
    <title>Resources</title>
    <para>
      The virtual guest system needs the same hardware resources as if it were
      installed on a physical machine. The more guests you plan to run on the
      host system, the more hardware resources&mdash;CPU, disk, memory and
      network&mdash;you need to add to the &vmhost;.
    </para>
  </tip>
  <section xml:id="kvm-host-cpu">
    <title>CPU support for virtualization</title>

    <para>
      To run &kvm;, your CPU must support virtualization, and virtualization
      needs to be enabled in BIOS. The file <filename>/proc/cpuinfo</filename>
      includes information about your CPU features.
    </para>

    <para>
      To find out whether your system supports virtualization, see
      <xref linkend="sec-kvm-requires-hardware"/>.
    </para>
  </section>
  <section xml:id="kvm-host-soft">
    <title>Required software</title>

    <para>
      The &kvm; host requires several packages to be installed. To install all
      necessary packages, do the following:
    </para>

    <procedure>
      <step>
        <para>
        install the <package>patterns-server-kvm_server</package> and
        <package>patterns-server-kvm_tools</package>. Refer to <xref linkend="virt-components-installation-patterns"/>
        </para>
      </step>
      <step>
        <para>
          Create a <guimenu>Network Bridge</guimenu>. If you do
          not plan to dedicate an additional physical network card to your
          virtual guests, network bridge is a standard way to connect the guest
          machines to the network. Refer to <xref linkend="libvirt-networks-bridged"/>.
        </para>
      </step>
      <step>
        <para>
          After all the required packages are installed (and new network setup
          activated), try to load the &kvm; kernel module relevant for your CPU
          type&mdash;<systemitem>kvm_intel</systemitem> or
          <systemitem>kvm_amd</systemitem>:
        </para>
<screen>&prompt.root;modprobe kvm_amd</screen>
        <para>
          Check if the module is loaded into memory:
        </para>
        <screen>&prompt.user;lsmod | grep kvm
kvm_amd               237568  20
kvm                  1376256  17 kvm_amd</screen>
        <para>
                Now the &kvm; host is ready to serve &kvm; &vmguest;s. <!-- TOFIX For more
                information, see <xref linkend="cha-qemu-running"/>. -->
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="kvm-host-virtio">
    <title>&kvm; host-specific features</title>

    <para>
      You can improve the performance of &kvm;-based &vmguest;s by letting them
      fully use specific features of the &vmhost;'s hardware
      (<emphasis>paravirtualization</emphasis>). This section introduces
      techniques to make the guests access the physical host's hardware
      directly&mdash;without the emulation layer&mdash;to make the most use of
      it.
    </para>

    <tip>
      <para>
        Examples included in this section assume basic knowledge of the
        <command>qemu-system-<replaceable>ARCH</replaceable></command> command
        line options. <!-- TOFIX For more information, see
        <xref linkend="cha-qemu-running"/>. -->
      </para>
    </tip>

    <section xml:id="kvm-virtio-scsi">
      <title>Using the host storage with <systemitem>virtio-scsi</systemitem></title>
      <para>
        <systemitem>virtio-scsi</systemitem> is an advanced storage stack for
        &kvm;. It replaces the former <systemitem>virtio-blk</systemitem> stack
        for SCSI devices pass-through. It has several advantages over
        <systemitem>virtio-blk</systemitem>:
      </para>
      <variablelist>
        <varlistentry>
          <term>Improved scalability</term>
          <listitem>
            <para>
              &kvm; guests have a limited number of PCI controllers, which
              results in a limited number of attached devices.
              <systemitem>virtio-scsi</systemitem> solves this limitation by
              grouping multiple storage devices on a single controller. Each
              device on a <systemitem>virtio-scsi</systemitem> controller is
              represented as a logical unit, or <emphasis>LUN</emphasis>.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Standard command set</term>
          <listitem>
            <para>
              <systemitem>virtio-blk</systemitem> uses a small set of commands
              that need to be known to both the
              <systemitem>virtio-blk</systemitem> driver and the virtual
              machine monitor, and so introducing a new command requires
              updating both the driver and the monitor.
            </para>
            <para>
              By comparison, <systemitem>virtio-scsi</systemitem> does not
              define commands, but rather a transport protocol for these
              commands following the industry-standard SCSI specification. This
              approach is shared with other technologies, such as Fibre
              Channel, ATAPI and USB devices.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Device naming</term>
          <listitem>
            <para>
              <systemitem>virtio-blk</systemitem> devices are presented inside
              the guest as
              <filename>/dev/vd<replaceable>X</replaceable></filename>, which
              is different from device names in physical systems and may cause
              migration problems.
            </para>
            <para>
              <systemitem>virtio-scsi</systemitem> keeps the device names
              identical to those on physical systems, making the virtual
              machines easily relocatable.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>SCSI device pass-through</term>
          <listitem>
            <para>
              For virtual disks backed by a whole LUN on the host, it is
              preferable for the guest to send SCSI commands directly to the
              LUN (pass-through). This is limited in
              <systemitem>virtio-blk</systemitem>, as guests need to use the
              virtio-blk protocol instead of SCSI command pass-through, and,
              moreover, it is not available for Windows guests.
              <systemitem>virtio-scsi</systemitem> natively removes these
              limitations.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <section>
        <title><systemitem>virtio-scsi</systemitem> usage</title>
        <para>
          &kvm; supports the SCSI pass-through feature with the
          <systemitem>virtio-scsi-pci</systemitem> device:
        </para>
<screen>&prompt.root;qemu-system-x86_64 [...] \
-device virtio-scsi-pci,id=scsi</screen>
      </section>
    </section>

    <section xml:id="kvm-qemu-vnet">
      <title>Accelerated networking with <systemitem>vhost-net</systemitem></title>
      <para>
        The <systemitem>vhost-net</systemitem> module is used to accelerate
        &kvm;'s paravirtualized network drivers. It provides better latency and
        greater network throughput. Use the <literal>vhost-net</literal> driver
        by starting the guest with the following example command line:
      </para>
<screen>&prompt.root;qemu-system-x86_64 [...] \
-netdev tap,id=guest0,vhost=on,script=no \
-net nic,model=virtio,netdev=guest0,macaddr=00:16:35:AF:94:4B</screen>
      <para>
        <literal>guest0</literal> is an identification string of the
        vhost-driven device.
      </para>
    </section>

    <section xml:id="kvm-qemu-multiqueue">
      <title>Scaling network performance with multiqueue virtio-net</title>
      <para>
        As the number of virtual CPUs increases in &vmguest;s, &qemu; offers a
        way of improving the network performance using
        <emphasis>multiqueue</emphasis>. Multiqueue virtio-net scales the
        network performance by allowing &vmguest; virtual CPUs to transfer
        packets in parallel. Multiqueue support is required on both the
        &vmhost; and &vmguest; sides.
      </para>
      <tip>
        <title>Performance benefit</title>
        <para>
          The multiqueue virtio-net solution is most beneficial in the
          following cases:
        </para>
        <itemizedlist mark="bullet" spacing="normal">
          <listitem>
            <para>
              Network traffic packets are large.
            </para>
          </listitem>
          <listitem>
            <para>
              &vmguest; has many connections active at the same time, mainly
              between the guest systems, or between the guest and the host, or
              between the guest and an external system.
            </para>
          </listitem>
          <listitem>
            <para>
              The number of active queues is equal to the number of virtual
              CPUs in the &vmguest;.
            </para>
          </listitem>
        </itemizedlist>
      </tip>
      <note>
        <para>
          While multiqueue virtio-net increases the total network throughput,
          it increases CPU consumption as it uses of the virtual CPU's power.
        </para>
      </note>
      <procedure xml:id="kvm-qemu-mq-enable">
        <title>How to enable multiqueue virtio-net</title>
        <para>
          The following procedure lists important steps to enable the
          multiqueue feature with <command>qemu-system-ARCH</command>. It
          assumes that a tap network device with multiqueue capability
          (supported since kernel version 3.8) is set up on the &vmhost;.
        </para>
        <step>
          <para>
            In <command>qemu-system-ARCH</command>, enable multiqueue for the
            tap device:
          </para>
<screen>-netdev tap,vhost=on,queues=<replaceable>2*N</replaceable></screen>
          <para>
            where <literal>N</literal> stands for the number of queue pairs.
          </para>
        </step>
        <step>
          <para>
            In <command>qemu-system-ARCH</command>, enable multiqueue and
            specify MSI-X (Message Signaled Interrupt) vectors for the
            virtio-net-pci device:
          </para>
<screen>-device virtio-net-pci,mq=on,vectors=<replaceable>2*N+2</replaceable></screen>
          <para>
            where the formula for the number of MSI-X vectors results from: N
            vectors for TX (transmit) queues, N for RX (receive) queues, one
            for configuration purposes, and one for possible VQ (vector
            quantization) control.
          </para>
        </step>
        <step>
          <para>
            In &vmguest;, enable multiqueue on the relevant network interface
            (<literal>eth0</literal> in this example):
          </para>
<screen>&prompt.sudo;ethtool -L eth0 combined 2*N</screen>
        </step>
      </procedure>
      <para>
        The resulting <command>qemu-system-ARCH</command> command line looks
        similar to the following example:
      </para>
<screen>qemu-system-x86_64 [...] -netdev tap,id=guest0,queues=8,vhost=on \
-device virtio-net-pci,netdev=guest0,mq=on,vectors=10</screen>
      <para>
        The <literal>id</literal> of the network device
        (<literal>guest0</literal>) needs to be identical for both options.
      </para>
      <para>
        Inside the running &vmguest;, specify the following command with
        &rootuser; privileges:
      </para>
<screen>&prompt.sudo;ethtool -L eth0 combined 8</screen>
      <para>
        Now the guest system networking uses the multiqueue support from the
        <command>qemu-system-ARCH</command> hypervisor.
      </para>
    </section>

    <section xml:id="kvm-vfio">
      <title>VFIO: secure direct access to devices</title>
      <para>
        Directly assigning a PCI device to a &vmguest; (PCI pass-through)
        avoids performance issues caused by avoiding any emulation in
        performance-critical paths. VFIO replaces the traditional &kvm;
        &pciback; device assignment. A prerequisite for this feature is a
        &vmhost; configuration as described in
        <xref linkend="ann-vt-io-require"/>.
      </para>
      <para>
        To be able to assign a PCI device via VFIO to a &vmguest;, you need to
        find out which IOMMU Group it belongs to. The
        <xref linkend="gloss-vt-acronym-iommu"/> (input/output memory
        management unit that connects a direct memory access-capable I/O bus to
        the main memory) API supports the notion of groups. A group is a set of
        devices that can be isolated from all other devices in the system.
        Groups are therefore the unit of ownership used by
        <xref linkend="vt-io-vfio"/>.
      </para>
      <procedure>
        <title>Assigning a PCI device to a &vmguest; via VFIO</title>
        <step>
          <para>
            Identify the host PCI device to assign to the guest.
          </para>
<screen>&prompt.sudo;lspci -nn
[...]
00:10.0 Ethernet controller [0200]: Intel Corporation 82576 \
Virtual Function [8086:10ca] (rev 01)
[...]</screen>
          <para>
            Note down the device ID, <literal>00:10.0</literal> in this example,
            and the vendor ID (<literal>8086:10ca</literal>).
          </para>
        </step>
        <step>
          <para>
            Find the IOMMU group of this device:
          </para>
<screen>&prompt.sudo;readlink /sys/bus/pci/devices/0000\:00\:10.0/iommu_group
../../../kernel/iommu_groups/20</screen>
          <para>
            The IOMMU group for this device is <literal>20</literal>. Now you
            can check the devices belonging to the same IOMMU group:
          </para>
<screen>&prompt.sudo;ls -l /sys/bus/pci/devices/0000\:01\:10.0/iommu_group/devices/
[...] 0000:00:1e.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0
[...] 0000:01:10.0 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.0
[...] 0000:01:10.1 -&gt; ../../../../devices/pci0000:00/0000:00:1e.0/0000:01:10.1</screen>
        </step>
        <step>
          <para>
            Unbind the device from the device driver:
          </para>
<screen>&prompt.sudo;echo "0000:01:10.0" &gt; /sys/bus/pci/devices/0000\:01\:10.0/driver/unbind</screen>
        </step>
        <step>
          <para>
            Bind the device to the vfio-pci driver using the vendor ID from
            step 1:
          </para>
<screen>&prompt.sudo;echo "8086 153a" &gt; /sys/bus/pci/drivers/vfio-pci/new_id</screen>
          <para>
            A new device
            <filename>/dev/vfio/<replaceable>IOMMU_GROUP</replaceable></filename>
            is created as a result, <filename>/dev/vfio/20</filename> in this
            case.
          </para>
        </step>
        <step>
          <para>
            Change the ownership of the newly created device:
          </para>
<screen>&prompt.sudo;chown qemu.qemu /dev/vfio/<replaceable>DEVICE</replaceable></screen>
        </step>
        <step>
          <para>
            Now run the &vmguest; with the PCI device assigned.
          </para>
<screen>&prompt.sudo;qemu-system-<replaceable>ARCH</replaceable> [...] -device
     vfio-pci,host=00:10.0,id=<replaceable>ID</replaceable></screen>
        </step>
      </procedure>
      <important>
        <title>No hotplugging</title>
        <para>
          As of &productname; &productnumber;, hotplugging of PCI devices
          passed to a &vmguest; via VFIO is not supported.
        </para>
      </important>
      <para>
        You can find more detailed information on the
        <xref linkend="vt-io-vfio"/> driver in the
        <filename>/usr/src/linux/Documentation/vfio.txt</filename> file
        (package <systemitem>kernel-source</systemitem> needs to be installed).
      </para>
    </section>

    <section xml:id="kvm-qemu-virtfs">
     <title>VirtFS: sharing directories between host and guests</title>
     <para>
      &vmguest;s normally run in a separate computing space&mdash;they are
      provided their own memory range, dedicated CPUs, and file system space.
      The ability to share parts of the &vmhost;'s file system makes the
      virtualization environment more flexible by simplifying mutual data
      exchange. Network file systems, such as CIFS and NFS, have been the
      traditional way of sharing directories. But as they are not
      specifically designed for virtualization purposes, they suffer from
      major performance and feature issues.
     </para>
     <note>
      <para>
       <emphasis>SELinux Requirement:</emphasis> For <literal>security_model=mapped</literal>,
       configure SELinux context:
      </para>
       <screen>&prompt.root; semanage fcontext -a -t virtiofsd_t "/tmp(/.*)?"
&prompt.root; restorecon -Rv /tmp</screen>
     </note>

     <section xml:id="virtiofs-host-configuration">
      <title>Host Configuration</title>
      <para>
       There is nothing special to do with form the host side, be sure that
       <package>virtiofsd</package> is installed. The &vmguest; xml libvirt
       file should have a configuration like:
      </para>
      <screen>
&lt;filesystem type="virtiofs" accessmode="mapped"/&gt;
       &lt;driver="virtiofs"/&gt;
       &lt;source dir="/tmp"/&gt;
       &lt;target dir="/mnt/hosttmp"/&gt;
       &lt;alias name="fs0"/&gt;
       &lt;address type="pci" domain="0x0000" bus="0x01" slot="0x00" function="0x0"/&gt;
              &lt;/filesystem&gt;</screen>
      <section xml:id="virtiofs-accessmodes">
       <title>Access Mode Options for virtiofs</title>
       <para>
        The <literal>accessmode</literal> attribute in the <literal>&lt;filesystem&gt;</literal> element
        defines how guest file permissions map to host permissions. Only two values are valid:
       </para>
       <table>
        <title>virtiofs Access Mode Options</title>
        <tgroup cols="3">
         <colspec colname="mode" colwidth="1*"/>
         <colspec colname="description" colwidth="2*"/>
         <colspec colname="security_impact" colwidth="2*"/>
         <thead>
          <row>
           <entry><literal>accessmode</literal></entry>
           <entry>Description</entry>
           <entry>Security Implications</entry>
          </row>
         </thead>
         <tbody>
          <row>
           <entry><literal>mapped</literal></entry>
           <entry>
            <para>
             The default mode. Maps guest UIDs/GIDs to host UIDs/GIDs using a translation table.
             Guest files appear as if owned by a dedicated "virtiofs" user on the host (typically UID 1000).
            </para>
            <para>
             Example: Guest user <literal>uid=1000</literal> writes to host <literal>/tmp</literal> →
             Host file appears as owned by <literal>virtiofsd</literal> user (not the guest user).
            </para>
           </entry>
           <entry>
            <para>
             <emphasis>Recommended for all environments</emphasis>.
            </para>
            <para>
             Prevents guest users from directly accessing host user accounts.
            </para>
            <para>
             <emphasis>Does not require matching host users</emphasis>.
            </para>
           </entry>
          </row>
          <row>
           <entry><literal>passthrough</literal></entry>
           <entry>
            <para>
             Guest UIDs/GIDs are used directly on the host. The guest must have matching users on the host.
            </para>
            <para>
             Example: Guest user <literal>uid=1000</literal> writes to host <literal>/tmp</literal> →
             Host file appears as owned by the user with <literal>uid=1000</literal>.
            </para>
           </entry>
           <entry>
            <para>
             <emphasis>Only for trusted guests</emphasis> (e.g., same-tenant cloud environments).
            </para>
            <para>
             <emphasis>Requires matching host users</emphasis> (e.g., host must have <literal>useradd -u 1000 guestuser</literal>).
            </para>
            <para>
             <emphasis>Security risk:</emphasis> Compromised guest can directly access host user accounts.
            </para>
           </entry>
          </row>
          <row>
           <entry><literal>none</literal></entry>
           <entry>
            <para>
             <emphasis>Invalid value</emphasis> (common documentation error).
            </para>
            <para>
             <literal>virtiofsd</literal> rejects this option with error:
             <literal>security_model=none is not supported</literal>.
            </para>
            <para>
             <emphasis>Always use <literal>mapped</literal> (default) or <literal>passthrough</literal></emphasis>.
            </para>
           </entry>
           <entry>
            <para>
             <emphasis>Causes immediate configuration failure</emphasis>.
            </para>
           </entry>
          </row>
         </tbody>
        </tgroup>
       </table>
       <note>
        <para>
         <emphasis>Key Configuration Rule:</emphasis>
         <literal>accessmode='mapped'</literal> must match the host's
         <literal>security_model=mapped</literal> in <literal>virtiofsd</literal>.
         Mismatched modes cause mount failures with errors like:
         <literal>Failed to set security context: Operation not permitted</literal>.
        </para>
       </note>
      </section>
     </section>
     <section xml:id="virtiofs-guest-configuration">
      <title>Guest Configuration</title>
      <para>
       On the guest, load the kernel module and mount the file system:
      </para>
      <example>
      <title>Guest Mount Command</title>
       <screen>&prompt.root; modprobe virtiofs
&prompt.root; mount -t virtiofs -o dax,cache=none host_tmp /mnt/hosttmp</screen>
       <para>
        <emphasis>Required options:</emphasis>
      </para>
        <itemizedlist>
         <listitem>
          <para><literal>virtiofs</literal></para>
         </listitem>
         <listitem>
          <para><literal>dax</literal>: Enables direct access for performance (recommended)</para>
         </listitem>
         <listitem>
          <para><literal>cache=none</literal>: Matches host configuration to prevent data loss</para>
         </listitem>
        </itemizedlist>
      </example>
      <note>
       <para>
        <emphasis>Guest Kernel Check:</emphasis> Verify module is loaded:
       </para>
        <screen>&prompt.sudo; lsmod | grep virtiofs</screen>
      </note>
     </section>
     <section xml:id="troubleshooting">
      <title>Troubleshooting Common Issues</title>
      <itemizedlist>
       <listitem>
        <para><literal>Permission denied</literal>: Check SELinux context (see <xref linkend="virtiofs-host-configuration"/>)</para>
       </listitem>
       <listitem>
        <para><literal>Mount fails with 9p</literal>: Verify you used <literal>-t virtiofs</literal> (not <literal>9p</literal>)</para>
       </listitem>
       <listitem>
        <para>Guest writes not syncing: Add <literal>cache=none</literal> to mount options</para>
       </listitem>
      </itemizedlist>
     </section>
    </section>

    <section xml:id="kvm-qemu-ksm">
      <title>KSM: sharing memory pages between guests</title>
      <para>
        Kernel Same Page Merging (<xref linkend="gloss-vt-acronym-ksm"/>) is a
        Linux kernel feature that merges identical memory pages from multiple
        running processes into one memory region. Because &kvm; guests run as
        processes under Linux, <xref linkend="gloss-vt-acronym-ksm"/> provides
        the memory overcommit feature to hypervisors for more efficient use of
        memory. Therefore, if you need to run multiple virtual machines on a
        host with limited memory, <xref linkend="gloss-vt-acronym-ksm"/> may be
        helpful to you.
      </para>
      <para>
        <xref linkend="gloss-vt-acronym-ksm"/> stores its status information in
        the files under the <filename>/sys/kernel/mm/ksm</filename> directory:
      </para>
<screen>&prompt.user;ls -1 /sys/kernel/mm/ksm
full_scans
merge_across_nodes
pages_shared
pages_sharing
pages_to_scan
pages_unshared
pages_volatile
run
sleep_millisecs</screen>
      <para>
        For more information on the meaning of the
        <filename>/sys/kernel/mm/ksm/*</filename> files, see
        <filename>/usr/src/linux/Documentation/vm/ksm.txt</filename> (package
        <systemitem>kernel-source</systemitem>).
      </para>
      <para>
        To use <xref linkend="gloss-vt-acronym-ksm"/>, do the following.
      </para>
      <procedure>
        <step>
          <para>
            Although &productnameshort; includes
            <xref linkend="gloss-vt-acronym-ksm"/> support in the kernel, it is
            disabled by default. To enable it, run the following command:
          </para>
<screen>&prompt.root;echo 1 &gt; /sys/kernel/mm/ksm/run</screen>
        </step>
        <step>
          <para>
            Now run several &vmguest;s under &kvm; and inspect the content of
            files <filename>pages_sharing</filename> and
            <filename>pages_shared</filename>, for example:
          </para>
<screen>&prompt.user;while [ 1 ]; do cat /sys/kernel/mm/ksm/pages_shared; sleep 1; done
13522
13523
13519
13518
13520
13520
13528</screen>
        </step>
      </procedure>
    </section>
  </section>
</topic>
