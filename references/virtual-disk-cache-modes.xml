<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<!-- refers to legacy doc: <add github link to legacy doc piece, if applicable> -->
<!-- point back to this document with a similar comment added to your legacy doc piece -->
<!-- refer to README.md for file and id naming conventions -->
<!-- metadata is dealt with on the assembly level -->
<topic xml:id="virtual-disk-cache-modes"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Virtual disk cache modes</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        A virtual disk is an essential part of a virtual machine (VM). To speed up disk reading and
        writing operations, you can enable a caching mechanism. This article describes available
        disk caching modes and how they behave with regards to data integrity and live migration of
        VMs.
      </para>
    </abstract>
  </info>
  <important>
    <para>
      If you do not specify a cache mode, <literal>writeback</literal> is used by default.
    </para>
  </important>
  <variablelist>
    <title>Virtual disk cache modes</title>
    <varlistentry xml:id="cache-writeback">
      <term>writeback</term>
      <listitem>
        <para>
          <literal>writeback</literal> uses the host page cache. Writes are reported to the guest
          as completed when they are placed in the host cache. Cache management handles commitment
          to the storage device, which can happen later. The guest's virtual storage adapter is informed of the
          <emphasis>writeback</emphasis> cache and therefore expected to send flush commands as
          needed to manage data integrity.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="cache-writethrough">
      <term>writethrough</term>
      <listitem>
        <para>
          Writes are reported as completed only when the data has been committed to the storage
          device. The guest's virtual storage adapter is informed that there is no
          <emphasis>writeback</emphasis> cache, so the guest do not need to send flush commands to
          manage data integrity.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="cache-none">
      <term>none</term>
      <listitem>
        <para>
          The host cache is bypassed, and reads and writes happen directly between the hypervisor
          and the storage device. Because the actual storage device may report a write as completed
          when the data is still placed in its write queue only, the guest's virtual storage
          adapter is informed that there is a <emphasis>writeback</emphasis> cache. This mode is
          equivalent to direct access to the host's disk.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="cache-unsafe">
      <term>unsafe</term>
      <listitem>
        <para>
          Similar to the <emphasis>writeback</emphasis> mode, except all flush commands from the
          guests are ignored. Using this mode implies that the user prioritizes performance gain
          over the risk of data loss in case of a host failure. This mode can be useful during
          guest installation, but not for production workloads.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="cache-directsync">
      <term>directsync</term>
      <listitem>
        <para>
          Writes are reported as completed only when the data has been committed to the storage
          device and the host cache is bypassed. Similar to <emphasis>writethrough</emphasis>, this
          mode can be useful for guests that do not send flushes when needed.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="cache-modes-data-integrity">
    <title>Cache modes and data integrity</title>
    <variablelist>
      <varlistentry>
        <term>writethrough, none, directsync</term>
        <listitem>
          <para>
            These modes are considered to be safest when the guest operating system uses flushes as
            needed. For unsafe or unstable guests, use <emphasis>writethough</emphasis> or
            <emphasis>directsync</emphasis>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>writeback</term>
        <listitem>
          <para>
            This mode informs the guest of the presence of a write cache, and it relies on the
            guest to send flush commands as needed to maintain data integrity within its disk
            image. This mode exposes the guest to data loss when the host fails. It is caused by
            the gap between the moment a write is reported as completed and the time the write
            being committed to the storage device.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>unsafe</term>
        <listitem>
          <para>
            This mode is similar to <emphasis>writeback</emphasis> caching, except the guest flush
            commands are ignored. This means a higher risk of data loss due to host failure.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="cache-modes-live-migration">
    <title>Cache modes and live migration</title>
    <para>
      The caching of storage data restricts the configurations that support live migration.
      Currently, only <literal>raw</literal> and <literal>qcow2</literal> image formats can be used
      for live migration. If a clustered file system is used, all cache modes support live
      migration. Otherwise the only cache mode that supports live migration on read/write shared
      storage is <literal>none</literal>.
    </para>
    <para>
      The &libvirt; management layer includes checks for migration compatibility based on several
      factors. If the guest storage is hosted on a clustered file system, is read-only, or is
      marked shareable, then the cache mode is ignored when determining if migration can be
      allowed. Otherwise &libvirt; does not allow migration unless the cache mode is set to
      <literal>none</literal>. However, this restriction can be overridden with the
      <quote>--unsafe</quote> option to the migration APIs, which is also supported by
      <command>virsh</command>. For example
    </para>
<screen>&prompt.user;virsh migrate --live --unsafe</screen>
    <tip>
      <para>
        The cache mode <literal>none</literal> is required for the AIO mode setting
        <literal>native</literal>. If another cache mode is used, the AIO mode is silently switched
        back to the default <literal>threads</literal>.
      </para>
    </tip>
  </section>
</topic>
