<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="vllm-helm-overrides"
 role="reference" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Examples of &vllm; &helm; chart override files</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <xi:include href="../snippets/helm-chart-overrides-intro.xml"/>
    </abstract>
  </info>
  <example xml:id="vllm-helm-overrides-minimal">
    <title>Minimal configuration</title>
    <para>
      The following override file installs &vllm; using a model that is publicly
      available.
    </para>
<screen>global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  modelSpec:
  - name: "phi3-mini-4k"
    registry: "dp.apps.rancher.io"
    repository: "containers/vllm-openai"
    tag: "0.9.1"
    imagePullPolicy: "IfNotPresent"
    modelURL: "microsoft/Phi-3-mini-4k-instruct"
    replicaCount: 1
    requestCPU: 6
    requestMemory: "16Gi"
    requestGPU: 1</screen>
    <procedure>
      <title>Validating the installation</title>
      <step>
        <para>
          Pulling the images can take a long time. You can monitor the status of
          &vllm; installation by running the following command:
        </para>
<screen>&prompt.user;<command>kubectl get pods -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command>
  
NAME                                           READY   STATUS    RESTARTS   AGE
[...]
vllm-deployment-router-7588bf995c-5jbkf        1/1     Running   0          8m9s
vllm-phi3-mini-4k-deployment-vllm-79d6fdc-tx7  1/1     Running   0          8m9s</screen>
        <para>
          Pods for the &vllm; deployment should transition to states
          <literal>Ready</literal> and <literal>Running</literal>.
        </para>
      </step>
    </procedure>
    <procedure>
      <title>Validating the stack</title>
      <step>
        <para>
          Expose the <literal>vllm-router-service</literal> port to the host
          machine:
        </para>
<screen>&prompt.user;<command>kubectl port-forward svc/vllm-router-service \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> 30080:80</command>
        </screen>
      </step>
      <step>
        <para>
          Query the &openai;-compatible API to list the available models:
        </para>
<screen>&prompt.user;<command>curl -o- http://localhost:30080/v1/models</command>
        </screen>
      </step>
      <step>
        <para>
          Send a query to the &openai; <filename>/completion</filename> endpoint
          to generate a completion for a prompt:
        </para>
<screen>&prompt.user;<command>curl -X POST http://localhost:30080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-3-mini-4k-instruct",
    "prompt": "Once upon a time,",
    "max_tokens": 10
  }'</command>

# example output of generated completions
{
    "id": "cmpl-3dd11a3624654629a3828c37bac3edd2",
    "object": "text_completion",
    "created": 1757530703,
    "model": "microsoft/Phi-3-mini-4k-instruct",
    "choices": [
        {
            "index": 0,
            "text": " in a bustling city full of concrete and",
            "logprobs": null,
            "finish_reason": "length",
            "stop_reason": null,
            "prompt_logprobs": null
        }
    ],
    "usage": {
        "prompt_tokens": 5,
        "total_tokens": 15,
        "completion_tokens": 10,
        "prompt_tokens_details": null
    },
    "kv_transfer_params": null
}</screen>
      </step>
    </procedure>
  </example>
  <example xml:id="vllm-helm-overrides-basic">
    <title>Basic configuration</title>
    <para>
      The following &vllm; override file include basic configuration options.
    </para>
    <itemizedlist>
      <title>Prerequisites</title>
      <listitem>
        <para>
          Access to a &huggingface; token (<literal>HF_TOKEN</literal>).
        </para>
      </listitem>
      <listitem>
        <para>
          The model <literal>meta-llama/Llama-3.1-8B-Instruct</literal> from
          this example is a gated model that requires you to accept the
          agreement to access it. For more information, see
          <link xlink:href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The <literal>runtimClassName</literal> specified here is
          <literal>nvidia</literal>.
        </para>
      </listitem>
      <listitem>
        <para>
          This example assumes that the cluster has a default storage class. If
          there is no default <literal>storageClass</literal>, provide a
          <literal>storageClass:</literal> entry for each
          <literal>modelSpec</literal>.
        </para>
      </listitem>
    </itemizedlist>
<screen># vllm_custom_overrides.yaml
global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  runtimeClassName: "nvidia"
  modelSpec:
  - name: "llama3" <co xml:id="co-vllm-basic-name"/>
    registry: "dp.apps.rancher.io" <co xml:id="co-vllm-basic-registry"/>
    repository: "containers/vllm-openai" <co xml:id="co-vllm-basic-repository"/>
    tag: "0.9.1" <co xml:id="co-vllm-basic-tag"/>
    imagePullPolicy: "IfNotPresent"
    modelURL: "meta-llama/Llama-3.1-8B-Instruct" <co xml:id="co-vllm-basic-modelURL"/>
    replicaCount: 1 <co xml:id="co-vllm-basic-replicaCount"/>
    requestCPU: 10 <co xml:id="co-vllm-basic-requestCPU"/>
    requestMemory: "16Gi" <co xml:id="co-vllm-basic-requestMemory"/>
    requestGPU: 1 <co xml:id="co-vllm-basic-requestGPU"/>
    pvcStorage: "50Gi" <co xml:id="co-vllm-basic-pvcStorage"/>
    pvcAccessMode:
      - ReadWriteOnce

    vllmConfig:
      enableChunkedPrefill: false <co xml:id="co-vllm-basic-enableChunkedPrefill"/>
      enablePrefixCaching: false <co xml:id="co-vllm-basic-enablePrefixCaching"/>
      maxModelLen: 4096 <co xml:id="co-vllm-basic-maxModelLen"/>
      dtype: "bfloat16" <co xml:id="co-vllm-basic-dtype"/>
      extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"] <co xml:id="co-vllm-basic-extraArgs"/>

    hf_token: <replaceable>HF_TOKEN</replaceable> <co xml:id="co-vllm-basic-hfToken"/></screen>
    <calloutlist>
      <callout arearefs="co-vllm-basic-name">
        <para>
          The unique identifier for your model deployment.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-registry">
        <para>
          The &docker; image registry containing the model's serving engine
          image.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-repository">
        <para>
          The &docker; image repository containing the model's serving engine
          image.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-tag">
        <para>
          The version of the model image to use.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-modelURL">
        <para>
          The URL pointing to the model on &huggingface; or another hosting
          service.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-replicaCount">
        <para>
          The number of replicas for the deployment which allows scaling for
          load.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-requestCPU">
        <para>
          The amount of CPU resources requested per replica.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-requestMemory">
        <para>
          Memory allocation for the deployment. Sufficient memory is required to
          load the model.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-requestGPU">
        <para>
          The number of GPUs to allocate for the deployment.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-pvcStorage">
        <para>
          The Persistent Volume Claim (PVC) size for model storage.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-enableChunkedPrefill">
        <para>
          Optimizes performance by prefetching model chunks.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-enablePrefixCaching">
        <para>
          Enables caching of prompt prefixes to speed up inference for repeated
          prompts.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-maxModelLen">
        <para>
          The maximum sequence length the model can handle.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-dtype">
        <para>
          The data type for model weights, such as <literal>bfloat16</literal>
          for mixed-precision inference and faster performance on modern GPUs.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-extraArgs">
        <para>
          Additional command-line arguments for &vllm;, such as disabling
          request logging or setting GPU memory utilization.
        </para>
      </callout>
      <callout arearefs="co-vllm-basic-hfToken">
        <para>
          Your &huggingface; token for accessing gated models. Replace
          <replaceable>HF_TOKEN</replaceable> with your actual token.
        </para>
      </callout>
    </calloutlist>
  </example>
  <example xml:id="vllm-helm-overrides-prefetched">
    <title>Loading pre-fetched model from persistent storage</title>
    <para>
      Pre-fetching models to a Persistent Volume Claim (PVC) prevents repeated
      downloads from &huggingface; during pod startup. The process involves
      creating a PVC and a job to fetch the model. This PVC is mounted at
      <filename>/models</filename>, where the pre-fetch job stores the model
      weights. Subsequently, the &vllm; <literal>modelURL</literal> is set to
      this path, which ensures that the model is loaded locally instead of being
      downloaded when the pod starts.
    </para>
    <procedure>
      <step>
        <para>
          Define a PVC for model weights using the following YAML specification.
        </para>
<screen># pvc-models.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: models-pvc
  namespace: <replaceable>SUSE_AI_NAMESPACE</replaceable>
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 50Gi # Adjust size based on your model
  storageClassName: <replaceable>STORAGE_CLASS</replaceable></screen>
        <para>
          Save it as <filename>pvc-models.yaml</filename> and apply with
          <command>kubectl apply -f pvc-models.yaml</command>.
        </para>
      </step>
      <step>
        <para>
          Create a secret resource for the &huggingface; token.
        </para>
<screen>&prompt.user;<command>kubectl create secret -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  generic huggingface-credentials \
  --from-literal=HUGGING_FACE_HUB_TOKEN=<replaceable>HF_TOKEN</replaceable></command></screen>
      </step>
      <step>
        <para>
          Create a YAML specification for pre-fetching the model and save it as
          <filename>job-prefetch-llama3.1-8b.yaml</filename>.
        </para>
<screen># job-prefetch-llama3.1-8b.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: prefetch-llama3.1-8b
  namespace: <replaceable>SUSE_AI_NAMESPACE</replaceable>
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: hf-download
        image: python:3.10-slim
        env:
        - name: HF_TOKEN
          valueFrom: { secretKeyRef: { name: huggingface-credentials, key: <replaceable>HUGGING_FACE_HUB_TOKEN</replaceable> } }
        - name: HF_HUB_ENABLE_HF_TRANSFER
          value: "1"
        - name: HF_HUB_DOWNLOAD_TIMEOUT
          value: "60"
        command: ["bash","-lc"]
        args:
        - |
          set -e
          echo "Logging in..."
          echo "Installing Hugging Face CLI..."
          pip install "huggingface_hub[cli]"
          pip install "hf_transfer"
          hf auth login --token "${HF_TOKEN}"
          echo "Downloading Llama 3.1 8B Instruct to /models/llama-3.1-8b-it ..."
          hf download meta-llama/Llama-3.1-8B-Instruct --local-dir /models/llama-3.1-8b-it
        volumeMounts:
        - name: models
          mountPath: /models
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc</screen>
        <para>
          Apply the specification with the following commands:
        </para>
<screen>&prompt.user;<command>kubectl apply -f job-prefetch-llama3.1-8b.yaml</command>
&prompt.user;<command>kubectl -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  wait --for=condition=complete job/prefetch-llama3.1-8b</command></screen>
      </step>
      <step>
        <para>
          Update custom &vllm; override file with support for PVC.
        </para>
<screen># vllm_custom_overrides.yaml
global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  runtimeClassName: "nvidia"
  modelSpec:
  - name: "llama3"
    registry: "dp.apps.rancher.io"
    repository: "containers/vllm-openai"
    tag: "0.9.1"
    imagePullPolicy: "IfNotPresent"
    modelURL: "/models/llama-3.1-8b-it"
    replicaCount: 1

    requestCPU: 10
    requestMemory: "16Gi"
    requestGPU: 1

    extraVolumes:
      - name: models-pvc
        persistentVolumeClaim:
          claimName: models-pvc <co xml:id="co-vllm-prefetched-extraVolumes-claimName"/>

    extraVolumeMounts:
      - name: models-pvc
        mountPath: /models <co xml:id="co-vllm-prefetched-extraVolumeMounts-mountPath"/>

    vllmConfig:
      maxModelLen: 4096

    hf_token: <replaceable>HF_TOKEN</replaceable></screen>
        <calloutlist>
          <callout arearefs="co-vllm-prefetched-extraVolumes-claimName">
            <para>
              Specify your PVC name.
            </para>
          </callout>
          <callout arearefs="co-vllm-prefetched-extraVolumeMounts-mountPath">
            <para>
              The mount path must match the base directory of the
              <literal>servingEngineSpec.modelSpec.modeURL</literal> value
              specified above.
            </para>
          </callout>
        </calloutlist>
        <para>
          Save it as <filename>vllm_custom_overrides.yaml</filename> and apply
          with <command>kubectl apply -f vllm_custom_overrides.yaml</command>.
        </para>
      </step>
      <step>
        <para>
          The following example lists mounted PVCs for a pod.
        </para>
<screen>&prompt.user;<command>kubectl exec -it vllm-llama3-deployment-vllm-858bd967bd-w26f7 \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> -- ls -l /models</command>
drwxr-xr-x 1 root root 608 Aug 22 16:29 llama-3.1-8b-it</screen>
      </step>
    </procedure>
  </example>
  <example xml:id="vllm-helm-overrides-multiple">
    <title>Configuration with multiple models</title>
    <para>
      This example shows how to configure multiple models to run on different
      GPUs.
    </para>
    <note>
      <title>Ray is not supported</title>
      <para>
        Ray is currently not supported. Therefore, sharding a single large model
        across multiple GPUs is not supported.
      </para>
    </note>
<screen># vllm_custom_overrides.yaml
global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  modelSpec:
  - name: "llama3"
    registry: "dp.apps.rancher.io"
    repository: "containers/vllm-openai"
    tag: "0.9.1"
    imagePullPolicy: "IfNotPresent"
    modelURL: "meta-llama/Llama-3.1-8B-Instruct"
    replicaCount: 1
    requestCPU: 10
    requestMemory: "16Gi"
    requestGPU: 1
    pvcStorage: "50Gi"
    vllmConfig:
      maxModelLen: 4096
    hf_token: <replaceable>HF_TOKEN_FOR_LLAMA_31</replaceable>

  - name: "mistral"
    registry: "dp.apps.rancher.io"
    repository: "containers/vllm-openai"
    tag: "0.9.1"
    imagePullPolicy: "IfNotPresent"
    modelURL: "mistralai/Mistral-7B-Instruct-v0.2"
    replicaCount: 1
    requestCPU: 10
    requestMemory: "16Gi"
    requestGPU: 1
    pvcStorage: "50Gi"
    vllmConfig:
      maxModelLen: 4096
    hf_token: <replaceable>HF_TOKEN_FOR_MISTRAL</replaceable></screen>
  </example>
  <example xml:id="vllm-helm-overrides-offloading">
    <title>CPU offloading</title>
    <para>
      This example demonstrates how to enable KV cache offloading to the CPU
      using &lmcache; in a &vllm; deployment. You can enable &lmcache; and set
      the CPU offloading buffer size using the <literal>lmcacheConfig</literal>
      field. In the following example, the buffer is set to 20&nbsp;GB, but you
      can adjust this value based on your workload.
    </para>
    <warning>
      <title>Experimental Features</title>
      <para>
        Setting <literal>lmcacheConfig.enabled</literal> to
        <literal>true</literal> implicitly enables the
        <literal>LMCACHE_USE_EXPERIMENTAL</literal> flag for &lmcache;. These
        experimental features are only supported on newer GPU generations. It is
        not recommended to enable them without a compelling reason.
      </para>
    </warning>
<screen># vllm_custom_overrides.yaml
global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  runtimeClassName: "nvidia"
  modelSpec:
  - name: "mistral"
    registry: "dp.apps.rancher.io"
    repository: "containers/lmcache-vllm-openai"
    tag: "0.3.2"
    imagePullPolicy: "IfNotPresent"
    modelURL: "mistralai/Mistral-7B-Instruct-v0.2"
    replicaCount: 1
    requestCPU: 10
    requestMemory: "40Gi"
    requestGPU: 1
    pvcStorage: "50Gi"
    pvcAccessMode:
      - ReadWriteOnce
    vllmConfig:
      maxModelLen: 32000

    lmcacheConfig:
      enabled: false
      cpuOffloadingBufferSize: "20"

    hf_token: <replaceable>HF_TOKEN</replaceable></screen>
  </example>
  <example xml:id="vllm-helm-overrides-lmcache">
    <title>Shared remote KV cache storage with &lmcache;</title>
    <para>
      This example shows how to enable remote KV cache storage using &lmcache;
      in a &vllm; deployment. The configuration defines a
      <literal>cacheserverSpec</literal> and uses two replicas. Remember to
      replace the placeholder values for <literal>hf_token</literal> and
      <literal>storageClass</literal> before applying the configuration.
    </para>
    <warning>
      <title>Experimental features</title>
      <para>
        Setting <literal>lmcacheConfig.enabled</literal> to
        <literal>true</literal> implicitly enables the
        <literal>LMCACHE_USE_EXPERIMENTAL</literal> flag for &lmcache;. These
        experimental features are only supported on newer GPU generations. It is
        not recommended to enable them without a compelling reason.
      </para>
    </warning>
<screen># vllm_custom_overrides.yaml
global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  runtimeClassName: "nvidia"
  modelSpec:
  - name: "mistral"
    registry: "dp.apps.rancher.io"
    repository: "containers/lmcache-vllm-openai"
    tag: "0.3.2"
    imagePullPolicy: "IfNotPresent"
    modelURL: "mistralai/Mistral-7B-Instruct-v0.2"
    replicaCount: 2
    requestCPU: 10
    requestMemory: "40Gi"
    requestGPU: 1
    pvcStorage: "50Gi"
    storageClass: <replaceable>STORAGE_CLASS</replaceable>
    vllmConfig:
      enablePrefixCaching: true
      maxModelLen: 16384
    lmcacheConfig:
      enabled: false
      cpuOffloadingBufferSize: "20"
    hf_token: <replaceable>HF_TOKEN</replaceable>
    initContainer:
      name: "wait-for-cache-server"
      image: "dp.apps.rancher.io/containers/lmcache-vllm-openai:0.3.2"
      command: ["/bin/sh", "-c"]
      args:
        - |
          timeout 60 bash -c '
          while true; do
            /opt/venv/bin/python3 /workspace/LMCache/examples/kubernetes/health_probe.py $(RELEASE_NAME)-cache-server-service $(LMCACHE_SERVER_SERVICE_PORT) &amp;&amp; exit 0
            echo "Waiting for LMCache server..."
            sleep 2
          done'
cacheserverSpec:
  replicaCount: 1
  containerPort: 8080
  servicePort: 81
  serde: "naive"
  registry: "dp.apps.rancher.io"
  repository: "containers/lmcache-vllm-openai"
  tag: "0.3.2"
  resources:
    requests:
      cpu: "4"
      memory: "8G"
    limits:
      cpu: "4"
      memory: "10G"
  labels:
    environment: "cacheserver"
    release: "cacheserver"
routerSpec:
  resources:
    requests:
      cpu: "1"
      memory: "2G"
    limits:
      cpu: "1"
      memory: "2G"
  routingLogic: "session"
  sessionKey: "x-user-id"
</screen>
  </example>
</topic>
