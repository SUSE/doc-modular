<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ai-gpu-nodes-assigning"
 role="concept" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Assigning GPU nodes to applications</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        When deploying a containerized application to &kube;, you need to ensure
        that containers requiring GPU resources are run on appropriate worker
        nodes. For example, &ollama;, a core component of &productname;, can
        deeply benefit from the use of GPU acceleration. This topic describes
        how to satisfy this requirement by explicitly requesting GPU resources
        and labeling worker nodes for configuring the node selector.
      </para>
    </abstract>
  </info>
  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        &kube; cluster&mdash;such as &rke2;&mdash;must be available and
        configured with more than one worker node in which certain nodes have
        &nvidia; GPU resources and others do not.
      </para>
    </listitem>
    <listitem>
      <para>
        This document assumes that any kind of deployment to the &kube; cluster
        is done using &helm; charts.
      </para>
    </listitem>
  </itemizedlist>
  <section xml:id="ai-gpu-node-labeling">
    <title>Labeling GPU nodes</title>
    <para>
      To distinguish nodes with the GPU support from non-GPU nodes, &kube; uses
      <emphasis>labels</emphasis>. Labels are used for relevant metadata and
      should not be confused with annotations that provide simple information
      about a resource. It is possible to manipulate labels with the
      <command>kubectl</command> command, as well as by tweaking configuration
      files from the nodes. If an IaC tool such as Terraform is used, labels can
      be inserted in the node resource configuration files.
    </para>
    <para>
      To label a single node, use the following command:
    </para>
<screen>&prompt.user;<command>kubectl label node <replaceable>GPU_NODE_NAME</replaceable> accelerator=nvidia-gpu</command></screen>
    <para>
      To achieve the same result by tweaking the <filename>node.yaml</filename>
      node configuration, add the following content and apply the changes with
      <command>kubectl apply -f node.yaml</command>:
    </para>
<screen>apiVersion: v1
kind: Node
metadata:
  name: node-name
  labels:
    accelerator: nvidia-gpu</screen>
    <tip>
      <title>Labeling multiple nodes</title>
      <para>
        To label multiple nodes, use the following command:
      </para>
<screen>&prompt.user;<command>kubectl label node \
  <replaceable>GPU_NODE_NAME1</replaceable> \
  <replaceable>GPU_NODE_NAME2</replaceable> ... \
  accelerator=nvidia-gpu</command></screen>
    </tip>
    <tip>
      <para>
        If Terraform is being used as an IaC tool, you can add labels to a group
        of nodes by editing the <filename>.tf</filename> files and adding the
        following values to a resource:
      </para>
<screen>resource "node_group" "example" {
  labels = {
    "accelerator" = "nvidia-gpu"
  }
}</screen>
    </tip>
    <para>
      To check if the labels are correctly applied, use the following command:
    </para>
<screen>&prompt.user;<command>kubectl get nodes --show-labels</command></screen>
  </section>
  <section xml:id="ai-gpu-node-assign">
    <title>Assigning GPU nodes</title>
    <para>
      The matching between a container and a node is configured by the explicit
      resource allocation and the use of labels and node selectors. The use
      cases described below focus on &nvidia; GPUs.
    </para>
    <section xml:id="ai-gpu-passthru">
      <title>Enable GPU passthrough</title>
      <para>
        Containers are isolated from the host environment by default. For the
        containers that rely on the allocation of GPU resources, their &helm;
        charts must enable GPU passthrough so that the container can access and
        use the GPU resource. Without enabling the GPU passthrough, the
        container may still run, but it can only use the main CPU for all
        computations. Refer to <link
        xlink:href="https://documentation.suse.com/suse-ai/1.0/html/AI-deployment-intro/index.html#ollama-helmchart">&ollama;
        &helm; chart</link> for an example of the configuration required for GPU
        acceleration.
      </para>
    </section>
    <section xml:id="ai-gpu-assign-request-resources">
      <title>Assignment by resource request</title>
      <para>
        After the &nvoperator; is configured on a node, you can instantiate
        applications requesting the resource <literal>nvidia.com/gpu</literal>
        provided by the operator. Add the following content to your
        <filename>values.yaml</filename> file. Specify the number of GPUs
        according to your setup.
      </para>
<screen>resources:
  requests:
    nvidia.com/gpu: 1
  limits:
    nvidia.com/gpu: 1</screen>
    </section>
    <section xml:id="ai-gpu-assign-labels">
      <title>Assignment by labels and node selectors</title>
      <para>
        If affected cluster nodes are labeled with a label such as
        <literal>accelerator=nvidia-gpu</literal>, you can configure the node
        selector to check for the label. In this case, use the following values
        in your <filename>values.yaml</filename> file.
      </para>
<screen>nodeSelector:
  accelerator: nvidia-gpu</screen>
    </section>
  </section>
  <section xml:id="ai-gpu-assign-verify">
    <title>Verifying &ollama; GPU assignment</title>
    <para>
      If the GPU is correctly detected, the &ollama; container logs this
      event:
    </para>
<screen>
| [...] source=routes.go:1172 msg="Listening on :11434 (version 0.0.0)"                                              │
│ [...] source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2502346830/runners                       │
│ [...] source=payload.go:44 msg="Dynamic LLM libraries [cuda_v12 cpu cpu_avx cpu_avx2]"                             │
│ [...] source=gpu.go:204 msg="looking for compatible GPUs"                                                          │
│ [...] source=types.go:105 msg="inference compute" id=GPU-c9ad37d0-d304-5d2a-c2e6-d3788cd733a7 library=cuda compute │</screen>
  </section>
</topic>
