<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<!-- refers to legacy doc: <add github link to legacy doc piece, if applicable> -->
<!-- point back to this document with a similar comment added to your legacy doc piece -->
<!-- refer to README.md for file and id naming conventions -->
<!-- metadata is dealt with on the assembly level -->
<topic xml:id="nvidia-operator-installation"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Installing the &nvoperator; on the &rke2; cluster</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        The &nvidia; operator allows administrators of &k8s; clusters to manage
        GPUs just like CPUs. It includes everything needed for pods to be able
        to operate GPUs.
      </para>
    </abstract>
  </info>
  <section xml:id="nvidia-operator-host-requirements">
    <title>Host OS requirements</title>
    <para>
      To expose the GPU to the pod correctly, the &nvidia; kernel drivers and
      the <filename>libnvidia-ml</filename> library must be correctly installed
      in the host OS. The &nvidia; Operator can automatically install drivers
      and libraries on specific operating systems. Check the &nvidia;
      documentation for information on
      <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-operating-systems-and-kubernetes-platforms">supported
      operating system releases</link>. Installation of the &nvidia; components
      on your host OS is out of the scope of this document. Refer to the
      &nvidia; documentation for instructions.
    </para>
    <para>
      The following three commands should return a correct output if the kernel
      driver is correctly installed.
    </para>
    <procedure>
      <step>
        <para>
          <command>lsmod | grep nvidia</command> returns a list of &nvidia;
          kernel modules. For example:
        </para>
<screen>nvidia_uvm           2129920  0
nvidia_drm            131072  0
nvidia_modeset       1572864  1 nvidia_drm
video                  77824  1 nvidia_modeset
nvidia               9965568  2 nvidia_uvm,nvidia_modeset
ecc                    45056  1 nvidia</screen>
      </step>
      <step>
        <para>
          <command>cat /proc/driver/nvidia/version</command> returns the NVRM
          and GCC version of the driver. For example:
        </para>
<screen>NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  555.42.06
  Release Build  (abuild@host)  Thu Jul 11 12:00:00 UTC 2024
  GCC version:  gcc version 7.5.0 (SUSE Linux)</screen>
      </step>
      <step>
        <para>
          <command>find /usr/ -iname libnvidia-ml.so</command> returns a path to
          the <filename>libnvidia-ml.so</filename> library. For example:
        </para>
<screen>/usr/lib64/libnvidia-ml.so</screen>
        <para>
          This library is used by &kube; components to interact with the kernel
          driver.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="nvidia-operator-installation-procedure">
    <title>Operator installation</title>
    <para>
      Once the OS is ready and &rke2a; is running, adjust &rke2a; nodes:
    </para>
    <procedure>
      <step>
        <para>
          On the agent nodes of &rke2a;, run the following command:
        </para>
<screen>&prompt.root;echo PATH=$PATH:/usr/local/nvidia/toolkit &gt;&gt; /etc/default/rke2-agent</screen>
      </step>
      <step>
        <para>
          On the server nodes of &rke2a;, run the following command:
        </para>
<screen>&prompt.root;echo PATH=$PATH:/usr/local/nvidia/toolkit &gt;&gt; /etc/default/rke2-server</screen>
      </step>
    </procedure>
    <para>
      Then install the &nvoperator; using the following YAML manifest.
    </para>
<screen>apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: gpu-operator
  namespace: kube-system
spec:
  repo: https://helm.ngc.nvidia.com/nvidia
  chart: gpu-operator
  targetNamespace: gpu-operator
  createNamespace: true
  valuesContent: |-
    toolkit:
      env:
      - name: CONTAINERD_SOCKET
        value: /run/k3s/containerd/containerd.sock</screen>
    <warning>
      <para>
        The &nvidia; operator restarts &containerd; with a hangup call which
        restarts &rke2a;.
      </para>
    </warning>
    <para>
      After one minute approximately, you can make the following checks to
      verify that everything works as expected.
    </para>
    <procedure>
      <step>
        <para>
          Assuming the drivers and <filename>libnvidia-ml.so</filename> library
          are installed, check if the operator detects them correctly.
        </para>
<screen>&prompt.user;kubectl get node <replaceable>NODENAME</replaceable> \
  -o jsonpath='{.metadata.labels}' | grep "nvidia.com/gpu.deploy.driver"</screen>
        <para>
          You should see the value <literal>pre-installed</literal>. If you see
          <literal>true</literal>, the drivers are not correctly installed. If
          the <xref linkend="nvidia-operator-host-requirements"/> are correct,
          it is possible that you forgot to reboot the node after installing all
          packages.
        </para>
        <para>
          You can also check other driver labels:
        </para>
<screen>&prompt.user;kubectl get node <replaceable>NODENAME</replaceable> \
  -o jsonpath='{.metadata.labels}' | jq | grep "nvidia.com"</screen>
        <para>
          You should see labels specifying driver and GPU , for example,
          <literal>nvidia.com/gpu.machine</literal> or
          <literal>nvidia.com/cuda.driver.major</literal>.
        </para>
      </step>
      <step>
        <para>
          Check if the GPU was added by
          <literal>nvidia-device-plugin-daemonset</literal> as an allocatable
          resource in the node.
        </para>
<screen>&prompt.user;kubectl get node <replaceable>NODENAME</replaceable> \
  -o jsonpath='{.status.allocatable}' | jq</screen>
        <para>
          You should see <literal>"nvidia.com/gpu":</literal> followed by the
          number of GPUs in the node.
        </para>
      </step>
      <step>
        <para>
          Check that the container runtime binary was installed by the operator
          (in particular, by the
          <literal>nvidia-container-toolkit-daemonset</literal>):
        </para>
<screen>&prompt.user;ls /usr/local/nvidia/toolkit/nvidia-container-runtime</screen>
      </step>
      <step>
        <para>
          Verify if &containerd; configuration was updated to include the
          &nvidia; container runtime.
        </para>
<screen>&prompt.user;grep nvidia /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
      </step>
      <step>
        <para>
          Run a pod to verify that the GPU resource can successfully be
          scheduled on a pod and the pod can detect it.
        </para>
<screen>apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: compute,utility</screen>
      </step>
    </procedure>
    <note>
      <title>Version gate</title>
      <para>
        Available as of October 2024 releases: v1.28.15+rke2r1, v1.29.10+rke2r1,
        v1.30.6+rke2r1, v1.31.2+rke2r1.
      </para>
    </note>
    <para>
      &rke2a; will now use <envar>PATH</envar> to find alternative container
      runtimes, in addition to checking the default paths used by the container
      runtime packages. To use this feature, you must modify the &rke2a;
      service's <envar>PATH</envar> environment variable to add the directories
      containing the container runtime binaries.
    </para>
    <para>
      It is recommended that you modify one of this two environment files:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <filename>/etc/default/rke2-server</filename> # or rke2-agent
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>/etc/sysconfig/rke2-server</filename> # or rke2-agent
        </para>
      </listitem>
    </itemizedlist>
    <para>
      This example adds the <envar>PATH</envar> in
      <filename>/etc/default/rke2-server</filename>:
    </para>
<screen>&prompt.user;echo PATH=$PATH &gt;&gt; /etc/default/rke2-server</screen>
    <warning>
      <para>
        <envar>PATH</envar> changes should be done with care to avoid placing
        untrusted binaries in the path of services that run as root.
      </para>
    </warning>
  </section>
</topic>
