<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<!-- based on https://github.com/rancher/rancher-product-docs/blob/main/versions/latest/modules/en/pages/installation-and-upgrade/install-rancher.adoc -->
<topic xml:id="rancher-register-clusters"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>&ranchera;: registering existing clusters</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        In this section, you will learn how to register existing &rke2a;
        clusters in &ranchermanager; (&ranchera;).
      </para>
    </abstract>
  </info>
  <para>
    The cluster registration feature replaced the feature for importing
    clusters.
  </para>
  <para>
    The control that &ranchera; has to manage a registered cluster depends on
    the type of cluster. For details, see
    <xref linkend="rancher-register-management-capabilities"/>.
  </para>
  <section xml:id="rancher-register-prerequisites">
    <title>Prerequisites</title>
    <section xml:id="rancher-register-node-roles">
      <title>&kube; node roles</title>
      <para>
        Registered &rkea; &kube; clusters must have all three node roles:
        <literal>etcd</literal>, <literal>controlplane</literal> and
        <literal>worker</literal>. A cluster with only
        <literal>controlplane</literal> components cannot be registered in
        &ranchera;.
      </para>
      <para>
        For more information on &rkea; node roles, see the
        <link xlink:href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-requirements/production-checklist#cluster-architecture">best
        practices</link>.
      </para>
    </section>
    <section xml:id="rancher-register-permissions">
      <title>Permissions</title>
      <para>
        To register a cluster in &ranchera;, you must have
        <literal>cluster-admin</literal> privileges within that cluster. If you
        do not, grant these privileges to your user by running:
      </para>
<screen>&prompt.user;kubectl create clusterrolebinding cluster-admin-binding \
  --clusterrole cluster-admin \
  --user <replaceable>USER_ACCOUNT</replaceable></screen>
    </section>
  </section>
  <section xml:id="rancher-register-cluster">
    <title>Registering a cluster</title>
    <procedure>
      <step>
        <para>
          Click <guimenu>☰</guimenu> &gt; <guimenu>Cluster
          Management</guimenu>.
        </para>
      </step>
      <step>
        <para>
          On the <guimenu>Clusters</guimenu> page, click <guimenu>Import
          Existing</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Choose the type of cluster.
        </para>
      </step>
      <step>
        <para>
          Use <guimenu>Member Roles</guimenu> to configure user authorization
          for the cluster. Click <guimenu>Add Member</guimenu> to add users who
          can access the cluster. Use the <guimenu>Role</guimenu> drop-down list
          to set permissions for each user.
        </para>
      </step>
      <step>
        <para>
          If you are importing a generic &kube; cluster in &ranchera;, perform
          the following steps for setup:
        </para>
        <substeps>
          <step>
            <para>
              Click <guimenu>Agent Environment Variables</guimenu> under
              <guimenu>Cluster Options</guimenu> to set environment variables
              for the
              <link xlink:href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/advanced-options/about-rancher-agents">&ranchera;
              cluster agent</link>. The environment variables can be set using
              key-value pairs. If the &ranchera; agent requires the use of a
              proxy to communicate with the &ranchera; server,
              <envar>HTTP_PROXY</envar>, <envar>HTTP_PROXY</envar>,
              <envar>HTTPS_PROXY</envar> and <envar>NO_PROXY</envar> environment
              variables can be set using agent environment variables.
            </para>
          </step>
          <step>
            <para>
              Enable <guimenu>Project Network Isolation</guimenu> to ensure the
              cluster supports &kube; <literal>NetworkPolicy</literal>
              resources. Users can select the <guimenu>Project Network
              Isolation</guimenu> option under the <guimenu>Advanced
              Options</guimenu> drop-down list to do so.
            </para>
          </step>
          <step>
            <para>
              <xref linkend="rancher-register-configure-version-management"
              xrefstyle="template: Configure the version management feature for imported &rke2a; and K3s clusters."/>
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          Click <guimenu>Create</guimenu>.
        </para>
      </step>
      <step>
        <para>
          The requirements for <literal>cluster-admin</literal> privileges are
          shown (see <xref linkend="rancher-register-prerequisites"/>),
          including an example command to fulfill them.
        </para>
      </step>
      <step>
        <para>
          Copy the <command>kubectl</command> command to your clipboard and run
          it on a node where <filename>kubeconfig</filename> is configured to
          point to the cluster you want to import. If you are unsure it is
          configured correctly, run <command>kubectl get nodes</command> to
          verify before running the command shown in &ranchera;.
        </para>
      </step>
      <step>
        <para>
          If you are using self-signed certificates, you will receive the
          message <literal>certificate signed by unknown authority</literal>. To
          work around this validation, copy the command starting with
          <command>curl</command> displayed in &ranchera; to your clipboard.
          Then run the command on a node where <filename>kubeconfig</filename>
          is configured to point to the cluster you want to import.
        </para>
      </step>
      <step>
        <para>
          After you finish running the command(s) on your node, click
          <guimenu>Done</guimenu>.
        </para>
      </step>
    </procedure>
    <important>
      <para>
        The <envar>NO_PROXY</envar> environment variable is not standardized,
        and the accepted format of the value can differ between applications.
        When configuring the <envar>NO_PROXY</envar> variable for &ranchera;,
        the value must adhere to the format expected by Golang.
      </para>
      <para>
        Specifically, the value should be a comma-delimited string that contains
        only IP addresses, CIDR notation, domain names or special DNS labels
        (such as <literal>*</literal>). For a full description of the expected
        value format, refer to the
        <link xlink:href="https://pkg.go.dev/golang.org/x/net/http/httpproxy#Config">upstream
        Golang documentation</link>.
      </para>
    </important>
    <note>
      <title>Expected results</title>
      <itemizedlist>
        <listitem>
          <para>
            Your cluster is registered and assigned a state of
            <literal>Pending</literal>. &ranchera; is deploying resources to
            manage your cluster.
          </para>
        </listitem>
        <listitem>
          <para>
            You can access your cluster after its state is updated to
            <literal>Active</literal>.
          </para>
        </listitem>
        <listitem>
          <para>
            <literal>Active</literal> clusters are assigned two projects:
            <literal>Default</literal> (containing the namespace
            <literal>default</literal>) and <literal>System</literal>
            (containing the namespaces <literal>cattle-system</literal>,
            <literal>ingress-nginx</literal>, <literal>kube-public</literal> and
            <literal>kube-system</literal>, if present).
          </para>
        </listitem>
      </itemizedlist>
    </note>
    <note>
      <para>
        You cannot re-register a cluster that is currently active in a
        &ranchera; setup.
      </para>
    </note>
  </section>
  <section xml:id="rancher-register-management-capabilities">
    <title>Management capabilities for registered clusters</title>
    <para>
      The control that &ranchera; has to manage a registered cluster depends on
      the type of cluster.
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <xref linkend="rancher-register-features-all" xrefstyle="select: title"/>
        </para>
      </listitem>
      <listitem>
        <para>
          <xref linkend="rancher-register-features-rke2-k3s" xrefstyle="select: title"/>
        </para>
      </listitem>
    </itemizedlist>
    <section xml:id="rancher-register-features-all">
      <title>Features for all registered clusters</title>
      <para>
        After registering a cluster, the cluster owner can:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/authentication-permissions-and-global-configuration/manage-role-based-access-control-rbac/cluster-and-project-roles">Manage
            cluster access</link> through role-based access control
          </para>
        </listitem>
        <listitem>
          <para>
            Enable
            <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/monitoring-alerting-and-logging/rancher-ui-monitoring">monitoring,
            alerts and notifiers</link>
          </para>
        </listitem>
        <listitem>
          <para>
            Enable
            <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/monitoring-alerting-and-logging/logging">logging</link>
          </para>
        </listitem>
        <listitem>
          <para>
            Enable
            <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/istio-setup-guide">Istio</link>
          </para>
        </listitem>
        <listitem>
          <para>
            Manage projects and workloads
          </para>
        </listitem>
      </itemizedlist>
    </section>
    <section xml:id="rancher-register-features-rke2-k3s">
      <title>Additional features for registered &rke2a; and &k3s; clusters</title>
      <para>
        <link xlink:href="https://documentation.suse.com/cloudnative/k3s/latest/en/introduction.html">&k3s;</link>
        is a lightweight, fully compliant &kube; distribution for edge
        installations.
      </para>
      <para>
        <link xlink:href="https://documentation.suse.com/cloudnative/rke2/latest/en/introduction.html">&rke2a;</link>
        is &ranchera;'s next-generation &kube; distribution for data center and
        cloud installations.
      </para>
      <para>
        When an &rke2a; or &k3s; cluster is registered in &ranchera;, &ranchera;
        will recognize it. The &ranchera; UI will expose features available to
        <xref linkend="rancher-register-features-all"/>, along with the
        following options for editing and upgrading the cluster:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            Enable or disable
            <xref linkend="rancher-register-configure-version-management"
            xrefstyle="template: version management"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/back-up-restore-and-disaster-recovery/back-up-rancher-launched-kubernetes-clusters">Upgrade
            the &kube; version</link> when version management is enabled.
          </para>
        </listitem>
        <listitem>
          <para>
            Configure the
            <xref linkend="rancher-register-configure-upgrades"
            xrefstyle="template: upgrade strategy"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            View a read-only version of the cluster’s configuration arguments
            and environment variables used to launch each node.
          </para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
  <section xml:id="rancher-register-configure-version-management">
    <title>Configuring version management for &rke2a; and &k3s; clusters</title>
    <warning>
      <para>
        When version management is enabled for an imported cluster, upgrading it
        outside of &ranchera; may lead to unexpected consequences.
      </para>
    </warning>
    <para>
      The version management feature for imported &rke2a; and &k3s; clusters can
      be configured using one of the following options:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <emphasis role="bold">Global default</emphasis> (default): Inherits
          behavior from the global
          <literal>imported-cluster-version-management</literal> setting.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="bold">True</emphasis>: Enables version management,
          allowing users to control the &kube; version and upgrade strategy of
          the cluster through &ranchera;.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="bold">False</emphasis>: Disables version management,
          enabling users to manage the cluster’s &kube; version independently,
          outside of &ranchera;.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      You can define the default behavior for newly created clusters or existing
      ones set to <quote>Global default</quote> by modifying the
      <literal>imported-cluster-version-management</literal> setting.
    </para>
    <para>
      Changes to the global
      <literal>imported-cluster-version-management</literal> setting take effect
      during the cluster’s next reconciliation cycle.
    </para>
    <note>
      <para>
        If version management is enabled for a cluster, &ranchera; will deploy
        the <literal>system-upgrade-controller</literal> app, along with the
        associated plans and other required &kube; resources, to the cluster. If
        version management is disabled, &ranchera; will remove these components
        from the cluster.
      </para>
    </note>
  </section>
  <section xml:id="rancher-register-configure-upgrades">
    <title>Configuring &rke2a; and &k3s; cluster upgrades</title>
    <tip>
      <para>
        It is a &kube; best practice to back up the cluster before upgrading.
        When upgrading a high-availability &k3s; cluster with an external
        database, back up the database in whichever way is recommended by the
        relational database provider.
      </para>
    </tip>
    <para>
      The <emphasis role="bold">concurrency</emphasis> is the maximum number of
      nodes that are permitted to be unavailable during an upgrade. If the
      number of unavailable nodes is larger than the
      <emphasis>concurrency</emphasis>, the upgrade will fail. If an upgrade
      fails, you may need to repair or remove failed nodes before the upgrade
      can succeed.
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <emphasis role="bold">Control plane concurrency:</emphasis> the
          maximum number of server nodes to upgrade at a single time; also the
          maximum unavailable server nodes
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis role="bold">Worker concurrency:</emphasis> the maximum
          number of worker nodes to upgrade at the same time; also the maximum
          unavailable worker nodes
        </para>
      </listitem>
    </itemizedlist>
    <para>
      In the &rke2a; and &k3s; documentation, control plane nodes are called
      server nodes. These nodes run the &kube; master, which maintains the
      desired state of the cluster. By default, these control plane nodes can
      have workloads scheduled to them by default.
    </para>
    <para>
      Also in the &rke2a; and &k3s; documentation, nodes with the worker role
      are called agent nodes. Any workloads or pods that are deployed in the
      cluster can be scheduled to these nodes by default.
    </para>
  </section>
  <section xml:id="rancher-register-debug-troubleshooting">
    <title>Debug logging and troubleshooting for registered &rke2a; and &k3s; clusters</title>
    <para>
      Nodes are upgraded by the system upgrade controller running in the
      downstream cluster. Based on the cluster configuration, &ranchera; deploys
      two
      <link xlink:href="https://github.com/rancher/system-upgrade-controller#example-upgrade-plan">plans</link>
      to upgrade nodes: one for control plane nodes and one for workers. The
      system upgrade controller follows the plans and upgrades the nodes.
    </para>
    <para>
      To enable debug logging on the system upgrade controller deployment, edit
      the
      <link xlink:href="https://github.com/rancher/system-upgrade-controller/blob/50a4c8975543d75f1d76a8290001d87dc298bdb4/manifests/system-upgrade-controller.yaml#L32">configmap</link>
      to set the debug environment variable to true. Then restart the
      <systemitem class="daemon">system-upgrade-controller</systemitem> pod.
    </para>
    <para>
      Logs created by the
      <systemitem class="daemon">system-upgrade-controller</systemitem> can be
      viewed by running this command:
    </para>
<screen>&prompt.user;kubectl logs -n cattle-system system-upgrade-controller</screen>
    <para>
      The current status of the plans can be viewed with this command:
    </para>
<screen>&prompt.user;kubectl get plans -A -o yaml</screen>
    <tip>
      <para>
        If the cluster becomes stuck during upgrading, restart the
        <systemitem class="daemon">system-upgrade-controller</systemitem>.
      </para>
    </tip>
    <para>
      To prevent issues when upgrading, the
      <link xlink:href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">&kube;
      upgrade best practices</link> should be followed.
    </para>
  </section>
  <section xml:id="rancher-register-ace-support">
    <title>Authorized cluster endpoint support for &rke2a; and &k3s; clusters</title>
    <para>
      &ranchera; supports Authorized Cluster Endpoints (ACE) for registered
      &rke2a; and &k3s; clusters. This support includes manual steps you will
      perform on the downstream cluster to enable the ACE. For additional
      information on the authorized cluster endpoint, refer to
      <link xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/access-clusters/authorized-cluster-endpoint">How
      the Authorized Cluster Endpoint Works</link>.
    </para>
    <note>
      <title>Notes</title>
      <itemizedlist>
        <listitem>
          <para>
            These steps only need to be performed on the control plane nodes of
            the downstream cluster. You must configure each control plane node
            individually.
          </para>
        </listitem>
        <listitem>
          <para>
            The following steps will work on both &rke2a; and &k3s; clusters
            registered in v2.6.x as well as those registered (or imported) from
            a previous version of &ranchera; with an upgrade to v2.6.x.
          </para>
        </listitem>
        <listitem>
          <para>
            These steps will alter the configuration of the downstream &rke2a;
            and &k3s; clusters and deploy the
            <filename>kube-api-authn-webhook</filename>. If a future
            implementation of the ACE requires an update to the
            <filename>kube-api-authn-webhook</filename>, then this would also
            have to be done manually. For more information on this webhook, see
            <link
            xlink:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/access-clusters/authorized-cluster-endpoint#about-the-kube-api-auth-authentication-webhook">Authentication
            webhook documentation</link>.
          </para>
        </listitem>
      </itemizedlist>
    </note>
    <procedure>
      <title>Manual steps to be taken on the control plane of each downstream cluster to enable ACE</title>
      <step>
        <para>
          Create a file at
          <filename>/var/lib/rancher/{rke2,k3s}/kube-api-authn-webhook.yaml</filename>
          with the following contents:
        </para>
<screen>apiVersion: v1
kind: Config
clusters:
- name: Default
  cluster:
    insecure-skip-tls-verify: true
    server: http://127.0.0.1:6440/v1/authenticate
users:
- name: Default
  user:
    insecure-skip-tls-verify: true
current-context: webhook
contexts:
- name: webhook
  context:
    user: Default
    cluster: Default</screen>
      </step>
      <step>
        <para>
          Add the following to the configuration file (or create one if it does
          not exist). Note that the default location is
          <filename>/etc/rancher/{rke2,k3s}/config.yaml</filename>:
        </para>
<screen>kube-apiserver-arg:
  - authentication-token-webhook-config-file=/var/lib/rancher/{rke2,k3s}/kube-api-authn-webhook.yaml</screen>
      </step>
      <step>
        <para>
          Run the following commands:
        </para>
<screen>&prompt.sudo;systemctl stop {rke2,k3s}-server
&prompt.sudo;systemctl start {rke2,k3s}-server</screen>
      </step>
      <step>
        <para>
          Finally, you <emphasis role="bold">must</emphasis> go back to the
          &ranchera; UI and edit the imported cluster there to complete the ACE
          enablement. Click on <guimenu>⋮</guimenu> &gt; <guimenu>Edit
          Config</guimenu>, then click the <guimenu>Networking</guimenu> tab
          under <guimenu>Cluster Configuration</guimenu>. Finally, click the
          <guimenu>Enabled</guimenu> button for <guimenu>Authorized
          Endpoint</guimenu>. Once the ACE is enabled, you then have the option
          of entering a fully qualified domain name (FQDN) and certificate
          information.
        </para>
      </step>
    </procedure>
    <note>
      <para>
        The <guimenu>FQDN</guimenu> field is optional, and if one is entered, it
        should point to the downstream cluster. Certificate information is only
        needed if there is a load balancer in front of the downstream cluster
        that is using an untrusted certificate. If you have a valid certificate,
        then nothing needs to be added to the <guimenu>CA Certificates</guimenu>
        field.
      </para>
    </note>
  </section>
  <section xml:id="rancher-register-annotating-clusters">
    <title>Annotating registered clusters</title>
    <para>
      For all types of registered &kube; clusters except for &rke2a; and &k3s;
      &kube; clusters, &ranchera; does not have any information about how the
      cluster is provisioned or configured.
    </para>
    <para>
      Therefore, when &ranchera; registers a cluster, it assumes that several
      capabilities are disabled by default. &ranchera; assumes this to avoid
      exposing UI options to the user even when the capabilities are not enabled
      in the registered cluster.
    </para>
    <para>
      However, if the cluster has a certain capability, a user of that cluster
      might still want to select the capability for the cluster in the
      &ranchera; UI. To do that, the user will need to manually indicate to
      &ranchera; that certain capabilities are enabled for the cluster.
    </para>
    <para>
      By annotating a registered cluster, it is possible to indicate to
      &ranchera; that a cluster was given additional capabilities outside of
      &ranchera;.
    </para>
    <para>
      The following annotation indicates &ingress; capabilities. Note that the
      values of non-primitive objects need to be JSON-encoded, with quotations
      escaped.
    </para>
<screen>"capabilities.cattle.io/ingressCapabilities": "[
  {
    "customDefaultBackend":true,
    "ingressProvider":"asdf"
  }
]"</screen>
    <para>
      These capabilities can be annotated for the cluster:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>ingressCapabilities</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>loadBalancerCapabilities</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>nodePoolScalingSupported</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>nodePortRange</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>taintSupport</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      All the capabilities and their type definitions can be viewed in the
      &ranchera; API view, at
      <filename><replaceable>RANCHER_SERVER_URL</replaceable>/v3/schemas/capabilities</filename>.
    </para>
    <para>
      To annotate a registered cluster,
    </para>
    <procedure>
      <step>
        <para>
          Click <guimenu>☰</guimenu> &gt; <guimenu>Cluster
          Management</guimenu>.
        </para>
      </step>
      <step>
        <para>
          On the <guimenu>Clusters</guimenu> page, go to the custom cluster you
          want to annotate and click <guimenu>⋮</guimenu> &gt; <guimenu>Edit
          Config</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Expand the <guimenu>Labels &amp; Annotations</guimenu> section.
        </para>
      </step>
      <step>
        <para>
          Click <guimenu>Add Annotation</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Add an annotation to the cluster with the format
          <literal>capabilities/&lt;capability&gt;: &lt;value&gt;</literal>
          where <literal>value</literal> is the cluster capability that will be
          overridden by the annotation. In this scenario, &ranchera; is not
          aware of any capabilities of the cluster until you add the annotation.
        </para>
      </step>
      <step>
        <para>
          Click <guimenu>Save</guimenu>.
        </para>
      </step>
    </procedure>
    <tip>
      <para>
        The annotation does not give the capabilities to the cluster, but it
        does indicate to &ranchera; that the cluster has those capabilities.
      </para>
    </tip>
  </section>
</topic>
