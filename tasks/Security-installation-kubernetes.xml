<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<!-- based on https://documentation.suse.com/cloudnative/security/5.4/en/kubernetes.html -->
<topic xml:id="suse-security-installation-kubernetes"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Installing &ssecurity; using &kube;</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        You can use &kube; to deploy separate manager, controller and enforcer
        containers and make sure that all new nodes have an enforcer deployed.
        &ssecurity; requires and supports &kube; network plug-ins such as
        flannel, weave and calico.
      </para>
    </abstract>
  </info>
  <para>
    The sample file will deploy one manager and 3 controllers. It will deploy an
    enforcer on every node as a daemonset. By default, the sample below will
    deploy to the Master node as well.
  </para>
  <para>
    Refer to <xref linkend="suse-security-installation-kubernetes-node-labels"/>
    for specifying dedicated manager or controller nodes using node labels.
  </para>
  <note>
    <para>
      It is not recommended to deploy (scale) more than one manager behind a
      load balancer due to potential session state issues. If you plan to use a
      PersistentVolume claim to store the backup of &ssecurity; configuration
      files, please see the general Backup/Persistent Data section in the
      <link xlink:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/neuvector/production-deployment-considerations#backups-and-persistent-data">Deploying
      &ssecurity;</link> overview.
    </para>
  </note>
  <para>
    If your deployment supports an integrated load balancer, change type
    <literal>NodePort</literal> to <literal>LoadBalancer</literal> for the
    console in the YAML file below.
  </para>
  <para>
    &ssecurity; supports &helm;-based deployment with a &helm; chart at
    <link xlink:href="https://github.com/neuvector/neuvector-helm">https://github.com/neuvector/neuvector-helm</link>.
  </para>
  <para>
    There is a separate section for OpenShift instructions, and EE on &kube; has
    some special steps described in the &docker; section.
  </para>
  <section xml:id="suse-security-installation-kubernetes-images">
    <title>&ssecurity; images on &docker; Hub</title>
    <para>
      The images are on the &ssecurity; &docker; Hub registry. Use the
      appropriate version tag for the manager, controller and enforcer, and
      leave the version as <quote>latest</quote> for scanner and updater. For
      example:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>neuvector/manager:5.4.3</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>neuvector/controller:5.4.3</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>neuvector/enforcer:5.4.3</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>neuvector/scanner:latest</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>neuvector/updater:latest</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Be sure to update the image references in the appropriate YAML files.
    </para>
    <para>
      If deploying with the current &ssecurity; &helm; chart (v1.8.9+), the
      following changes should be made to <filename>values.yml</filename>:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Update the registry to <literal>docker.io</literal>.
        </para>
      </listitem>
      <listitem>
        <para>
          Update image names and tags to the current version on &docker; Hub, as
          shown above.
        </para>
      </listitem>
      <listitem>
        <para>
          Leave <literal>imagePullSecrets</literal> empty.
        </para>
      </listitem>
    </itemizedlist>
    <note>
      <para>
        If deploying from the &ranchermanager; 2.6.5+ &ssecurity; chart, images
        are pulled automatically from the &ranchera; Registry mirrored image
        repo, and deployed into the <literal>cattle-neuvector-system</literal>
        namespace.
      </para>
    </note>
  </section>
  <section xml:id="suse-security-installation-kubernetes-deploy">
    <title>Deploy &ssecurity;</title>
    <procedure>
      <step>
        <para>
          Create the &ssecurity; namespace and the required service accounts:
        </para>
<screen>&prompt.user;kubectl create namespace neuvector
&prompt.user;kubectl create sa controller -n neuvector
&prompt.user;kubectl create sa enforcer -n neuvector
&prompt.user;kubectl create sa basic -n neuvector
&prompt.user;kubectl create sa updater -n neuvector
&prompt.user;kubectl create sa scanner -n neuvector
&prompt.user;kubectl create sa registry-adapter -n neuvector
&prompt.user;kubectl create sa cert-upgrader -n neuvector</screen>
      </step>
      <step performance="optional">
        <para>
          Create the &ssecurity; Pod Security Admission (PSA) or Pod Security
          Policy (PSP). If you have enabled Pod Security Admission (aka Pod
          Security Standards) in &kube; 1.25+, or Pod Security Policies (prior
          to 1.25) in your &kube; cluster, add the following for &ssecurity;
          (for example, <filename>nv_psp.yaml</filename>).
        </para>
        <note>
          <itemizedlist>
            <listitem>
              <para>
                PSP is deprecated in &kube; 1.21 and will be removed in 1.25.
              </para>
            </listitem>
            <listitem>
              <para>
                The Manager and Scanner pods run without a UID. If your PSP has
                a rule <literal>Run As User: Rule: MustRunAsNonRoot</literal>
                then add the following into the sample YAML below with the
                appropriate value for <replaceable>###</replaceable>:
              </para>
            </listitem>
          </itemizedlist>
        </note>
<screen>securityContext:
    runAsUser: ###</screen>
        <para>
          For PSA in &kube; 1.25+, label the &ssecurity; namespace with the
          privileged profile for deploying on a PSA-enabled cluster.
        </para>
<screen>&prompt.user;kubectl label namespace neuvector \
  "pod-security.kubernetes.io/enforce=privileged"</screen>
      </step>
      <step>
        <para>
          Create the custom resources (CRD) for &ssecurity; rules. For &kube;
          1.19+:
        </para>
        <note>
          <para>
            If you are upgrading to version <literal>5.4.6</literal> using YAML,
            you must deploy the <filename>responserules-crd-k8s.yaml</filename>
            file. If you are using &helm; charts, this step is handled
            automatically, and no action is required.
          </para>
        </note>
<screen>&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/waf-crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/dlp-crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/com-crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/vul-crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/admission-crd-k8s-1.19.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s.yaml
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/5.4.3_group-definition-k8s
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/responserules-crd-k8s.yaml</screen>
      </step>
      <step>
        <para>
          Add read permission to access the &k8s; API.
        </para>
        <important>
          <para>
            The standard &ssecurity; 5.2+ deployment uses least-privileged
            service accounts instead of the default. See below if upgrading from
            a version prior to 5.3.
          </para>
        </important>
        <warning>
          <para>
            If you are upgrading to 5.3.0+, run the following commands based on
            your current version:
          </para>
        </warning>
        <para>
          Version 5.2.0:
        </para>
<screen>&prompt.user;kubectl delete clusterrole neuvector-binding-nvsecurityrules \
  neuvector-binding-nvadmissioncontrolsecurityrules \
  neuvector-binding-nvdlpsecurityrules \
  neuvector-binding-nvwafsecurityrules</screen>
        <para>
          Versions prior to 5.2.0:
        </para>
<screen>&prompt.user;kubectl delete clusterrolebinding \
  neuvector-binding-app neuvector-binding-rbac \
  neuvector-binding-admission \
  neuvector-binding-customresourcedefinition \
  neuvector-binding-nvsecurityrules \
  neuvector-binding-view \
  neuvector-binding-nvwafsecurityrules \
  neuvector-binding-nvadmissioncontrolsecurityrules \
  neuvector-binding-nvdlpsecurityrules
&prompt.user;kubectl delete rolebinding neuvector-admin -n neuvector</screen>
        <para>
          Apply the read permissions via the following <command>create
          clusterrole</command> commands:
        </para>
<screen>&prompt.user;kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
&prompt.user;kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io
&prompt.user;kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
&prompt.user;kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions
&prompt.user;kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules
&prompt.user;kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules
&prompt.user;kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules
&prompt.user;kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller
&prompt.user;kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector
&prompt.user;kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector
&prompt.user;kubectl create role neuvector-binding-secret --verb=get,list,watch --resource=secrets -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller --serviceaccount=neuvector:enforcer --serviceaccount=neuvector:scanner --serviceaccount=neuvector:registry-adapter -n neuvector
&prompt.user;kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller
&prompt.user;kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller
&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-roles-k8s.yaml
&prompt.user;kubectl create role neuvector-binding-lease --verb=create,get,update --resource=leases -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-cert-upgrader --role=neuvector-binding-cert-upgrader --serviceaccount=neuvector:cert-upgrader -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-job-creation --role=neuvector-binding-job-creation --serviceaccount=neuvector:controller -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-lease --role=neuvector-binding-lease --serviceaccount=neuvector:controller --serviceaccount=neuvector:cert-upgrader -n neuvector
&prompt.user;kubectl create clusterrole neuvector-binding-nvgroupdefinitions --verb=list,get,delete --resource=nvgroupdefinitions
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvgroupdefinitions --clusterrole=neuvector-binding-nvgroupdefinitions --serviceaccount=neuvector:controller
&prompt.user;kubectl create role neuvector-binding-secret-controller --verb=create,patch,update --resource=secrets -n neuvector
&prompt.user;kubectl create rolebinding neuvector-binding-secret-controller --role=neuvector-binding-secret-controller --serviceaccount=neuvector:controller --serviceaccount=neuvector:default -n neuvector
&prompt.user;kubectl create clusterrole neuvector-binding-nvresponserulesecurityrules --verb=get,list,delete --resource=nvresponserulesecurityrules
&prompt.user;kubectl create clusterrolebinding neuvector-binding-nvresponserulesecurityrules --clusterrole=neuvector-binding-nvresponserulesecurityrules --serviceaccount=neuvector:controller</screen>
      </step>
      <step>
        <para>
          Run the following commands to check if the neuvector/controller and
          neuvector/updater service accounts are added successfully.
        </para>
<screen>&prompt.user;kubectl get ClusterRoleBinding \
  neuvector-binding-app neuvector-binding-rbac \
  neuvector-binding-admission \
  neuvector-binding-customresourcedefinition \
  neuvector-binding-nvsecurityrules \
  neuvector-binding-view \
  neuvector-binding-nvwafsecurityrules \
  neuvector-binding-nvadmissioncontrolsecurityrules \
  neuvector-binding-nvdlpsecurityrules \
  neuvector-binding-nvgroupdefinitions \
  neuvector-binding-nvresponserulesecurityrules -o wide</screen>
        <para>
          Sample output:
        </para>
<screen>NAME                                                ROLE                                                            AGE   USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-app                               ClusterRole/neuvector-binding-app                               66d                    neuvector/controller
neuvector-binding-rbac                              ClusterRole/neuvector-binding-rbac                              66d                    neuvector/controller
neuvector-binding-admission                         ClusterRole/neuvector-binding-admission                         66d                    neuvector/controller
neuvector-binding-customresourcedefinition          ClusterRole/neuvector-binding-customresourcedefinition          66d                    neuvector/controller
neuvector-binding-nvsecurityrules                   ClusterRole/neuvector-binding-nvsecurityrules                   66d                    neuvector/controller
neuvector-binding-view                              ClusterRole/view                                                66d                    neuvector/controller
neuvector-binding-nvwafsecurityrules                ClusterRole/neuvector-binding-nvwafsecurityrules                66d                    neuvector/controller
neuvector-binding-nvadmissioncontrolsecurityrules   ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules   66d                    neuvector/controller
neuvector-binding-nvdlpsecurityrules                ClusterRole/neuvector-binding-nvdlpsecurityrules                66d                    neuvector/controller
neuvector-binding-nvgroupdefinitions                ClusterRole/neuvector-binding-nvgroupdefinitions                66d                    neuvector/controller</screen>
        <para>
          And this command:
        </para>
<screen>&prompt.user;kubectl get RoleBinding neuvector-binding-scanner \
  neuvector-binding-cert-upgrader \
  neuvector-binding-job-creation \
  neuvector-binding-lease \
  neuvector-binding-secret -n neuvector -o wide</screen>
        <para>
          Sample output:
        </para>
<screen>NAME                              ROLE                                   AGE    USERS   GROUPS   SERVICEACCOUNTS
neuvector-binding-scanner         Role/neuvector-binding-scanner         8m8s                    neuvector/controller, neuvector/updater
neuvector-binding-cert-upgrader   Role/neuvector-binding-cert-upgrader   8m8s                    neuvector/cert-upgrader
neuvector-binding-job-creation    Role/neuvector-binding-job-creation    8m8s                    neuvector/controller
neuvector-binding-lease           Role/neuvector-binding-lease           8m8s                    neuvector/controller, neuvector/cert-upgrader
neuvector-binding-secret          Role/neuvector-binding-secret          8m8s                    neuvector/controller, neuvector/enforcer, neuvector/scanner, neuvector/registry-adapter</screen>
      </step>
      <step>
        <para>
          (Optional) Create the Federation Master and/or Remote Multi-Cluster
          Management Services. If you plan to use the multi-cluster management
          functions in &ssecurity;, one cluster must have the Federation Master
          service deployed, and each remote cluster must have the Federation
          Worker service. For flexibility, you may choose to deploy both Master
          and Worker services on each cluster so any cluster can be a master or
          remote.
        </para>
<screen>apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-master
  namespace: neuvector
spec:
  ports:
  - port: 11443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-controller-fed-worker
  namespace: neuvector
spec:
  ports:
  - port: 10443
    name: fed
    protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod</screen>
        <para>
          Then create the appropriate service(s):
        </para>
<screen>&prompt.user;kubectl create -f nv_master_worker.yaml</screen>
      </step>
      <step>
        <para>
          Create the primary &ssecurity; services and pods using the preset
          version commands or modify the sample YAML below. The preset version
          invokes a LoadBalancer for the &ssecurity; Console. If using the
          sample YAML file below, replace the image names and
          <replaceable>VERSION</replaceable> tags for the manager, controller
          and enforcer image references in the YAML file. Also, make any other
          modifications required for your deployment environment (such as
          LoadBalancer/NodePort/Ingress for manager access). The YAML below
          needs to be changed for internal certificate changes if deployed from
          v5.4.2 or above. Refer to this
          <xref linkend="suse-security-installation-kubernetes-yaml-5-4-2"/>.
        </para>
<screen>&prompt.user;kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.4.0/neuvector-k8s.yaml</screen>
        <para>
          Or, if modifying any of the above YAML or samples from below:
        </para>
<screen>&prompt.user;kubectl create -f neuvector.yaml</screen>
        <para>
          Now you should be able to connect to the &ssecurity; console and log
          in with admin:admin, for example:
          <literal>+https://<replaceable>PUBLIC-IP</replaceable>:8443+</literal>
        </para>
      </step>
    </procedure>
    <note>
      <para>
        The <literal>nodeport</literal> service specified in the
        <filename>neuvector.yaml</filename> file will open a random port on all
        &k8s; nodes for the &ssecurity; Management Web console port.
        Alternatively, you can use a LoadBalancer or &ingress;, using a public
        IP and default port 8443. For <literal>nodeport</literal>, be sure to
        open access through firewall rules for that port, if needed. To see
        which port is open on the host nodes, please run the following commands:
      </para>
<screen>&prompt.user;kubectl get svc -n neuvector</screen>
      <para>
        And you will see something like:
      </para>
<screen>NAME                          CLUSTER-IP      EXTERNAL-IP   PORT(S)                                          AGE
neuvector-service-webui     10.100.195.99     &lt;nodes>       8443:30257/TCP                                   15m</screen>
    </note>
    <para>
      <emphasis role="bold">PKS Change</emphasis>
    </para>
    <note>
      <para>
        PKS is field-tested and requires enabling privileged containers to the
        plan/tile, and changing the YAML hostPath as follows for All-in-One,
        Controller and Enforcer:
      </para>
<screen>hostPath:
  path: /var/vcap/sys/run/docker/docker.sock</screen>
    </note>
    <para>
      <emphasis role="bold">Master node taints and tolerations</emphasis>
    </para>
    <para>
      All taint info must match to schedule Enforcers on nodes. To check the
      taint info on a node (such as Master):
    </para>
<screen>&prompt.user;kubectl get node taintnodename -o yaml</screen>
    <para>
      Sample output:
    </para>
<screen>spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  # there may be an extra info for taint as below
  - effect: NoSchedule
    key: mykey
    value: myvalue</screen>
    <para>
      If there are additional taints as above, add these to the sample YAML
      tolerations section:
    </para>
<screen>spec:
  template:
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        # if there is an extra info for taints as above, please add it here.
        # This is required to match all the taint info defined on the taint
        # node. Otherwise, the Enforcer won't deploy on the taint node
        - effect: NoSchedule
          key: mykey
          value: myvalue</screen>
  </section>
  <section xml:id="suse-security-installation-kubernetes-node-labels">
    <title>Using node labels for manager and controller nodes</title>
    <para>
      To control which nodes the Manager and Controller are deployed on, label
      each node. Replace <replaceable>NODE_NAME</replaceable> with the
      appropriate node name ('<command>kubectl get nodes</command>'). Note: By
      default, &kube; will not schedule pods on the master node.
    </para>
<screen>&prompt.user;kubectl label nodes <replaceable>NODE_NAME</replaceable> nvcontroller=true</screen>
    <para>
      Then add a nodeSelector to the YAML file for the Manager and Controller
      deployment sections. For example:
    </para>
<screen>          - mountPath: /host/cgroup
              name: cgroup-vol
              readOnly: true
      nodeSelector:
        nvcontroller: "true"
      restartPolicy: Always</screen>
    <para>
      To prevent the enforcer from being deployed on a controller node, if it is
      a dedicated management node (without application containers to be
      monitored), add a nodeAffinity to the Enforcer YAML section. For example:
    </para>
<screen>  app: neuvector-enforcer-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: nvcontroller
                  operator: NotIn
                  values: ["true"]
      imagePullSecrets:</screen>
  </section>
  <section xml:id="suse-security-installation-kubernetes-rolling-updates">
    <title>Rolling updates</title>
    <para>
      Orchestration tools such as &kube;, Red Hat OpenShift, and &ranchera;
      support rolling updates with configurable policies. You can use this
      feature to update the &ssecurity; containers. The most important thing
      will be to ensure that there is at least one Controller (or All-in-One)
      running so that policies, logs and connection data are not lost. Make sure
      that there is a minimum of 120 seconds between container updates so that a
      new leader can be elected and the data synchronized between controllers.
    </para>
    <para>
      The provided sample deployment YAMLs already configure the rolling update
      policy. If you are updating via the &ssecurity; &helm; chart, please pull
      the latest chart to properly configure new features such as admission
      control, and delete the old cluster role and cluster role binding for
      &ssecurity;. If you are updating via &kube;, you can manually update to a
      new version with the sample commands below.
    </para>
    <section xml:id="suse-security-installation-kubernetes-sample-rolling-update">
      <title>Sample &kube; rolling update</title>
      <para>
        For upgrades that just need to update to a new image version, you can
        use this simple approach.
      </para>
      <para>
        If your Deployment or DaemonSet is already running, you can change the
        YAML file to the new version, then apply the update:
      </para>
<screen>&prompt.user;kubectl apply -f <replaceable>YAML_FILE</replaceable></screen>
      <para>
        This will update to a new version of &ssecurity; from the command line.
      </para>
      <para>
        For the controller as a Deployment (also do the same for the manager):
      </para>
<screen>&prompt.user;kubectl set image deployment/neuvector-controller-pod \
  neuvector-controller-pod=neuvector/controller:<replaceable>VERSION</replaceable> -n neuvector</screen>
      <para>
        For any container as a DaemonSet:
      </para>
<screen>&prompt.user;kubectl set image -n neuvector \
  ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:<replaceable>VERSION</replaceable></screen>
      <para>
        To check the status of the rolling update:
      </para>
<screen>&prompt.user;kubectl rollout status -n neuvector ds/neuvector-enforcer-pod
&prompt.user;kubectl rollout status -n neuvector deployment/neuvector-controller-pod</screen>
      <para>
        To roll back the update:
      </para>
<screen>&prompt.user;kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod
&prompt.user;kubectl rollout undo -n neuvector deployment/neuvector-controller-pod</screen>
    </section>
  </section>
  <section xml:id="suse-security-installation-kubernetes-expose-rest-api">
    <title>Expose REST API in &kube;</title>
    <para>
      To expose the REST API for access from outside of the &kube; cluster, here
      is a sample YAML file:
    </para>
<screen>apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-rest
  namespace: neuvector
spec:
  ports:
    - port: 10443
      name: controller
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-controller-pod</screen>
    <para>
      Please see the Automation section for more info on the REST API.
    </para>
  </section>
  <section xml:id="suse-security-installation-kubernetes-non-privileged">
    <title>&kube; deployment in non-privileged mode</title>
    <para>
      The following instructions can be used to deploy &ssecurity; without using
      privileged mode containers. The controller is already in non-privileged
      mode and enforcer deployment should be changed, which is shown in the
      excerpted snippets below.
    </para>
    <para>
      Enforcer:
    </para>
<screen>spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
        # this line is required to be added if k8s version is pre-v1.19
        # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      containers:
          securityContext:
            # the following two lines are required for k8s v1.19+. 
            # Comment out both lines if version is pre-1.19.
            # Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK</screen>
  </section>
  <section xml:id="suse-security-installation-kubernetes-yaml-5-4-2">
    <title>&kube; deployment YAML for v5.4.2 onwards</title>
    <para>
      The following sample YAML is for versions 5.4.2 and onwards where we need
      to mount the internal certificates on Controller, Enforcer and Scanner
      pods since we do not support hardcoded certificates anymore. Create the
      internal-certificate secret from the given link before deploying:
      <link xlink:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/neuvector/custom-certs#replacing-internal-certificates">Replacing
      Internal Certificates</link>.
    </para>
<screen>apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            privileged: true
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
          volumeMounts:
            - mountPath: /etc/neuvector/certs/internal/cert.key
              name: internal-cert
              readOnly: true
              subPath: tls.key
            - mountPath: /etc/neuvector/certs/internal/cert.pem
              name: internal-cert
              readOnly: true
              subPath: tls.crt
            - mountPath: /etc/neuvector/certs/internal/ca.cert
              name: internal-cert
              readOnly: true
              subPath: ca.crt
      restartPolicy: Always
      volumes:
        - name: internal-cert
          secret:
            defaultMode: 420
            secretName: internal-cert
---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - /bin/sh
            - -c
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never</screen>
    <para>
      The following sample is a complete deployment reference (&kube; 1.19+).
    </para>
<screen>apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-crd-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 30443
    protocol: TCP
    name: crd-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-admission-webhook
  namespace: neuvector
spec:
  ports:
  - port: 443
    targetPort: 20443
    protocol: TCP
    name: admission-webhook
  type: ClusterIP
  selector:
    app: neuvector-controller-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-service-webui
  namespace: neuvector
spec:
  ports:
    - port: 8443
      name: manager
      protocol: TCP
  type: LoadBalancer
  selector:
    app: neuvector-manager-pod

---

apiVersion: v1
kind: Service
metadata:
  name: neuvector-svc-controller
  namespace: neuvector
spec:
  ports:
  - port: 18300
    protocol: "TCP"
    name: "cluster-tcp-18300"
  - port: 18301
    protocol: "TCP"
    name: "cluster-tcp-18301"
  - port: 18301
    protocol: "UDP"
    name: "cluster-udp-18301"
  clusterIP: None
  selector:
    app: neuvector-controller-pod

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-manager-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-manager-pod
  replicas: 1
  template:
    metadata:
      labels:
        app: neuvector-manager-pod
    spec:
      serviceAccountName: basic
      serviceAccount: basic
      containers:
        - name: neuvector-manager-pod
          image: neuvector/manager:5.4.3
          env:
            - name: CTRL_SERVER_IP
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-controller-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-controller-pod
  minReadySeconds: 60
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 3
  template:
    metadata:
      labels:
        app: neuvector-controller-pod
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - neuvector-controller-pod
              topologyKey: "kubernetes.io/hostname"
      serviceAccountName: controller
      serviceAccount: controller
      containers:
        - name: neuvector-controller-pod
          image: neuvector/controller:5.4.3
          securityContext:
            runAsUser: 0
          readinessProbe:
            exec:
              command:
              - cat
              - /tmp/ready
            initialDelaySeconds: 5
            periodSeconds: 5
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /etc/config
              name: config-volume
              readOnly: true
      terminationGracePeriodSeconds: 300
      restartPolicy: Always
      volumes:
        - name: config-volume
          projected:
            sources:
              - configMap:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-init
                  optional: true
              - secret:
                  name: neuvector-secret
                  optional: true

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: neuvector-enforcer-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-enforcer-pod
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: neuvector-enforcer-pod
      annotations:
        container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined
      # Add the following for pre-v1.19
      # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined
    spec:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      hostPID: true
      serviceAccountName: enforcer
      serviceAccount: enforcer
      containers:
        - name: neuvector-enforcer-pod
          image: neuvector/enforcer:5.4.3
          securityContext:
            # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show
            seccompProfile:
              type: Unconfined
            capabilities:
              add:
              - SYS_ADMIN
              - NET_ADMIN
              - SYS_PTRACE
              - IPC_LOCK
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
            - name: CLUSTER_ADVERTISED_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CLUSTER_BIND_ADDR
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - mountPath: /lib/modules
              name: modules-vol
              readOnly: true
            - mountPath: /var/nv_debug
              name: nv-debug
              readOnly: false
      terminationGracePeriodSeconds: 1200
      restartPolicy: Always
      volumes:
        - name: modules-vol
          hostPath:
            path: /lib/modules
        - name: nv-debug
          hostPath:
            path: /var/nv_debug

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: neuvector-scanner-pod
  namespace: neuvector
spec:
  selector:
    matchLabels:
      app: neuvector-scanner-pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 2
  template:
    metadata:
      labels:
        app: neuvector-scanner-pod
    spec:
      serviceAccountName: scanner
      serviceAccount: scanner
      containers:
        - name: neuvector-scanner-pod
          image: neuvector/scanner:latest
          imagePullPolicy: Always
          env:
            - name: CLUSTER_JOIN_ADDR
              value: neuvector-svc-controller.neuvector
      restartPolicy: Always

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: neuvector-updater-pod
  namespace: neuvector
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: neuvector-updater-pod
        spec:
          serviceAccountName: updater
          serviceAccount: updater
          containers:
          - name: neuvector-updater-pod
            image: neuvector/updater:latest
            imagePullPolicy: Always
            command:
            - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H "Authorization:Bearer $TOKEN" -H "Content-Type:application/strategic-merge-patch+json" -d '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt":"'`date +%Y-%m-%dT%H:%M:%S%z`'"}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod'
          restartPolicy: Never</screen>
  </section>
  <section xml:id="suse-security-installation-kubernetes-pks-change">
    <title>PKS change</title>
    <note>
      <para>
        PKS is field-tested and requires enabling privileged containers to the
        plan/tile, and changing the YAML hostPath as follows for All-in-One and
        Enforcer:
      </para>
<screen>      hostPath:
            path: /var/vcap/sys/run/docker/docker.sock</screen>
    </note>
  </section>
</topic>
