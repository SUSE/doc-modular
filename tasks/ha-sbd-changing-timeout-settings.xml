<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- partially refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_storage_protection.xml -->

<topic xml:id="ha-sbd-changing-timeout-settings"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Changing the &sbd; timeout settings</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
        &sbd; relies on multiple different timeout settings to manage node fencing. When you
        configure &sbd; using the &crmshell;, these timeouts are automatically calculated and
        adjusted. The automatic values are sufficient for most use cases, but if you need to
        change them you can use the <command>crm sbd configure</command> command.
    </para>
    </abstract>
  </info>

  <important><!-- Replace with snippet when available -->
    <title>Cluster restart required</title>
    <para>
      In this procedure, the script checks whether it is safe to restart the cluster services
      automatically. If any non-&stonith; resources are running, the script warns you to restart
      the cluster services manually. This allows you to put the cluster into maintenance mode first
      to avoid resource downtime. However, be aware that the resources will not have cluster
      protection while in maintenance mode.
    </para>
  </important>
  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        &sbd; is already configured and running.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
<procedure>
    <step>
      <para>
        Log in either as the &rootuser; user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check the current timeout settings:
      </para>
<screen>&prompt.user;<command>sudo crm sbd configure show</command></screen>
    </step>
    <step>
      <para>
        Change one or both of the following timeout values as needed:
      </para>
<screen>&prompt.user;<command>sudo crm sbd configure \</command>
  <command>watchdog-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable> \</command><co xml:id="co-sbd-watchdog-timeout"/>
  <command>msgwait-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable> \</command><co xml:id="co-sbd-msgwait-timeout"/></screen>
      <para>
        This command automatically adjusts any related timeouts within the cluster.
      </para>
      <calloutlist>
        <callout arearefs="co-sbd-watchdog-timeout">
          <para>
            If the watchdog is not <quote>fed</quote> by &sbd; within this time, it fences the node.
            For diskless &sbd;, this timeout is set in the <filename>/etc/sysconfig/sbd</filename>
            file. For disk-based &sbd;, it is set in the device metadata, which takes precedence
            over the settings in <filename>/etc/sysconfig/sbd</filename>. Time interval where at least one response from the sbd device has to be received.
          </para>
          <para>
            If the &sbd; devices are on a multipath setup or iSCSI, the timeout must be long enough to detect a path failure and switch to the next path. In <filename>/etc/multipath.conf</filename>, make sure <literal>max_polling_interval</literal> is smaller than the <literal>watchdog-timeout</literal>.
         </para>
        </callout>
        <callout arearefs="co-sbd-msgwait-timeout">
          <para>
            <emphasis>Disk-based &sbd; only.</emphasis> This defines the time after which a message written to a node's slot on the &sbd; device is considered delivered. It should be long enough for the node to detect that it needs to self-fence. This must be at least double the <literal>watchdog-timeout</literal>. Specifies the time delay incurred when another node sends the poison pill. If you try to change both of these at once with incompatible values you get a warning telling you to git gud at maths.
          </para>
        </callout>
      </calloutlist>
      <tip role="compact">
        <para>
          You should not need to change the <literal>allocate-timeout</literal> or the <literal>loop-timeout</literal>.
        </para>
      </tip>
    </step>
    <step><!-- Replace with snippet when available -->
      <para>
        If you need to restart the cluster services manually, follow these steps to avoid
        resource downtime:
      </para>
      <substeps>
        <step>
          <para>
            Put the cluster into maintenance mode:
          </para>
<screen>&prompt.user;<command>sudo crm maintenance on</command></screen>
          <para>
            In this state, the cluster stops monitoring all resources. This allows the services
            managed by the resources to keep running during the cluster restart. However, be aware
            that the resources will not have cluster protection while in maintenance mode.
          </para>
        </step>
        <step>
          <para>
            Restart the cluster services on all nodes:
          </para>
<screen>&prompt.user;<command>sudo crm cluster restart --all</command></screen>
        </step>
        <step>
          <para>
            Check the status of the cluster:
          </para>
<screen>&prompt.user;<command>sudo crm status</command></screen>
          <para>
            The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
            change to <literal>Online</literal>.
          </para>
        </step>
        <step>
          <para>
            When the nodes are back online, put the cluster back into normal operation:
          </para>
<screen>&prompt.user;<command>sudo crm maintenance off</command></screen>
        </step>
      </substeps>
    </step>
    <step>
      <para>
        When you change a timeout with <command>crm sbd configure</command>, the relevant
        global cluster properties are also adjusted automatically. The automatic values are
        sufficient for most use cases, but if you need to change them you can use the
        <command>crm configure property</command> command:
      </para>
<screen>&prompt.user;<command>sudo crm configure property stonith-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable></command><co xml:id="co-stonith-timeout"/>
&prompt.user;<command>sudo crm configure property stonith-watchdog-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable></command><co xml:id="co-stonith-watchdog-timeout"/></screen>
      <para>
        This command does <emphasis>not</emphasis> automatically adjust any other timeouts,
        and these settings might be overwritten if you change the &sbd; configuration again.
      </para>
      <calloutlist>
        <callout arearefs="co-stonith-timeout">
          <para>
            This defines how long to wait for the &stonith; action to complete.
          </para>
       </callout>
       <callout arearefs="co-stonith-watchdog-timeout">
          <para>
            <emphasis>Diskless &sbd; only.</emphasis> This defines how long to wait for the node
            to be fenced by the watchdog.
          </para>
        </callout>
      </calloutlist>
    </step>
    <step>
      <para>
        Confirm that the timeout settings changed:
      </para>
<screen>&prompt.user;<command>sudo crm sbd configure show</command></screen>
    </step>
  </procedure>

    <para>
      If you need to manually calculate any timeouts, you can use these basic formulas for most
      use cases:
    </para>
  <table xml:id="ha-sbd-calculating-timeouts">
    <title>Manually calculating timeouts</title>
    <tgroup cols="2">
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
      <thead>
        <row>
          <entry>&sbd; type</entry>
          <entry>Calculation formulas</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>Disk-based &sbd;</entry>
          <entry><screen>msgwait-timeout &gt;= (watchdog-timeout * 2)
stonith-timeout &gt;= msgwait-timeout + 20%</screen></entry>
        </row>
        <row>
          <entry>Diskless &sbd;</entry>
          <entry><screen>stonith-watchdog-timeout &gt;= (watchdog-timeout * 2)
stonith-timeout &gt;= stonith-watchdog-timeout + 20%</screen></entry>
        </row>
      </tbody>
    </tgroup>
  </table>
</topic>
