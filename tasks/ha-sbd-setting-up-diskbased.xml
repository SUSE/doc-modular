<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_storage_protection.xml -->

<topic xml:id="ha-sbd-setting-up-diskbased"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Setting up disk-based &sbd;</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
        TODO
      </para>
    </abstract>
  </info>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        An existing &ha; cluster is already running.
      </para>
    </listitem>
    <listitem>
      <para>
        The cluster does <emphasis>not</emphasis> already have either type of &sbd; configured.
        <!--To change diskless &sbd; to disk-based &sbd;, see (an article that doesn't exist yet).-->
      </para>
    </listitem>
    <listitem>
      <para>
        Shared storage is configured and accessible on all nodes.
      </para>
    </listitem>
    <listitem>
      <para>
        The path to the shared storage device is consistent across all nodes. Use stable device
        names such as <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>.
      </para>
    </listitem>
    <listitem>
      <para>
        The shared storage <emphasis>must not</emphasis> use host-based RAID, LVM, DRBD, or Cluster MD.
      </para>
    </listitem>
    <listitem>
      <para>
        All nodes have a watchdog device, and the correct watchdog kernel module is loaded.
      </para>
    </listitem>
  </itemizedlist>
  <warning>
    <title>Overwriting existing data</title>
    <para>
      Make sure the device or devices you want to use for &sbd; do not hold any important data. When you execute the <command>sbd create</command> command, roughly the first megabyte of the specified block devices is overwritten without further requests or backup.
    </para>
  </warning>

  <para>
    Perform this procedure on only one cluster node:
  </para>
  <procedure><!--Copied from changing-sbd-config, needs to be rearranged a bit-->
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen>&prompt.root;<command>crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources, so the services managed
        by the resources can continue to run even if the cluster services stop.
      </para>
    </step>
    <step>
      <para>
        Configure the new device:
      </para>
<screen>&prompt.root;<command>crm -F cluster init sbd -s /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <para>
        The <option>-F</option> option allows &crmsh; to reconfigure &sbd; even if the &sbd; service is
        already running.
      </para>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen>&prompt.root;<command>crm status</command></screen>
      <para>
        Initially, the nodes have a status of <literal>UNCLEAN (offline)</literal>,
        but after a short time they change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the &sbd; configuration. First, check the device metadata:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> dump</command>
</screen>
      <para>
        Then check that all nodes in the cluster are assigned to a slot in the device:
      </para>
<screen>&prompt.root;<command>sbd -d /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> list</command></screen>
    </step>
    <step>
      <para>
        Check the status of the &sbd; service:
      </para>
<screen>&prompt.root;<command>systemctl status sbd</command></screen>
      <para>
        If you changed diskless &sbd; to disk-based &sbd;, check that the following section
        includes a device ID:
      </para>
<screen>CGroup: /system/.slice/sbd.service
        |&mdash;23314 "sbd: inquisitor"
        |&mdash;23315 "sbd: watcher: /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable> - slot: 0 - uuid: <replaceable>DEVICE_UUID</replaceable>"
        |&mdash;23316 "sbd: watcher: Pacemaker"
        |&mdash;23317 "sbd: watcher: Cluster"</screen>
    </step>
    <step>
      <para>
        When the nodes are back online, move the cluster out of maintenance mode and back
        into normal operation:
      </para>
<screen>&prompt.root;<command>crm maintenance off</command></screen>
    </step>
  </procedure>
</topic>
