<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>

<!-- refers to legacy doc: https://github.com/SUSE/doc-sleha/blob/main/xml/ha_storage_protection.xml -->

<topic xml:id="ha-sbd-setting-up-diskbased"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Setting up disk-based &sbd;</title>
    <meta name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract>
      <para>
        To set up disk-based &sbd; in a running cluster, use the &sbd; stage of the cluster setup
        script. This script handles all the required setup tasks for &sbd; and calculates appropriate
        timeout settings for the cluster. You can configure up to three &sbd; devices.
      </para>
    </abstract>
  </info>

  <para>
    This procedure assumes the cluster does not already have either type of &sbd; configured. If the
    &sbd; service is already running, see <citetitle>Changing &sbd; Configuration</citetitle> instead.
    <!-- Add link when available -->
  </para>
  <important>
    <title>Cluster restart might be required</title>
    <para>
      The &sbd; setup script checks whether a cluster restart is required and whether it is
      safe to do so automatically. If any non-&stonith; resources are running, the script does
      <emphasis>not</emphasis> automatically restart the cluster services. Instead, it warns you to
      restart the cluster services manually. This allows you to put the cluster into maintenance
      mode first to avoid resource downtime.
    </para>
  </important>
  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        An existing &ha; cluster is already running.
      </para>
    </listitem>
    <listitem>
      <para>
        The &sbd; service is not running.
      </para>
    </listitem>
    <listitem>
      <para>
        Shared storage is configured and accessible on all nodes.
      </para>
    </listitem>
    <listitem>
      <para>
        The path to the shared storage device is consistent across all nodes. Use stable device
        names such as <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>.
      </para>
    </listitem>
    <listitem>
      <para>
        The shared storage must <emphasis>not</emphasis> use host-based RAID, LVM, DRBD, or Cluster MD.
      </para>
    </listitem>
    <listitem>
      <para>
        All nodes have a watchdog device, and the correct watchdog kernel module is loaded.
      </para>
    </listitem>
  </itemizedlist>
  <warning>
    <title>Overwriting existing data</title>
    <para>
      Make sure any device you want to use for &sbd; does not hold any important data. Configuring
      a device for use with &sbd; overwrites the existing data.
    </para>
  </warning>

  <para>
    Perform this procedure on only one cluster node:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the &rootuser; user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Run the &sbd; stage of the cluster setup script, using the option <option>--sbd-device</option>
        (or <option>-s</option>) to specify the shared storage device:
      </para>
<screen>&prompt.user;<command>sudo crm cluster init sbd --sbd-device /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <itemizedlist>
        <title>Additional options</title>
        <listitem>
          <para>
            You can use <option>--sbd-device</option> (or <option>-s</option>) multiple times to
            configure up to three &sbd; devices.
          </para>
        </listitem>
        <listitem>
          <para>
            If multiple watchdogs are available, you can use the option <option>--watchdog</option>
            (or <option>-w</option>) to choose which watchdog to use. Specify either the device name
            (for example, <literal>/dev/watchdog1</literal>) or the driver name (for example,
            <literal>iTCO_wdt</literal>).
          </para>
        </listitem>
      </itemizedlist>
      <para>
        The script initializes &sbd; on the shared storage device, creates a &stonith; cluster
        resource, and updates the &sbd; configuration file and timeout settings. The script also
        checks whether a cluster restart is required and whether it is safe to do so automatically.
        If any non-&stonith; resources are running, the script warns you to restart the cluster
        services manually.
      </para>
    </step>
    <step>
      <para>
        If you need to restart the cluster services manually, follow these steps to avoid
        resource downtime:
      </para>
      <substeps>
      <step>
        <para>
          Put the cluster into maintenance mode:
        </para>
<screen>&prompt.user;<command>sudo crm maintenance on</command></screen>
        <para>
          In this state, the cluster stops monitoring all resources. This allows the services
          managed by the resources to keep running during the cluster restart.
        </para>
      </step>
      <step>
        <para>
          Restart the cluster services on all nodes:
        </para>
<screen>&prompt.user;<command>sudo crm cluster restart --all</command></screen>
      </step>
      <step>
        <para>
          Check the status of the cluster:
        </para>
<screen>&prompt.user;<command>sudo crm status</command></screen>
        <para>
          The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
          change to <literal>Online</literal>.
        </para>
      </step>
      <step>
        <para>
          When the nodes are back online, put the cluster back into normal operation:
        </para>
<screen>&prompt.user;<command>sudo crm maintenance off</command></screen>
      </step>
    </substeps>
    </step>
    <step>
      <para>
        Check the &sbd; configuration:
      </para>
<screen>&prompt.user;<command>sudo crm sbd configure show</command>
</screen>
      <para>
        The output of this command shows the &sbd; device's metadata, the enabled settings in the <filename>/etc/sysconfig/sbd</filename> file, and the &sbd;-related cluster settings.
      </para>
    </step>
  </procedure>
</topic>
