<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ollama-installing"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Installing &ollama;</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        &ollama; is a tool for running and managing language models locally on
        your computer. It offers a simple interface to download, run and
        interact with models without relying on cloud resources.
      </para>
    </abstract>
  </info>
  <tip os="suseai">
    <para>
      When installing &suseai;, &ollama; is installed by the &owui; installation
      by default. If you decide to install &ollama; separately, disable its
      installation during the installation of &owui; as outlined in
      <xref
      linkend="owui-ollama-deploy-separate"/>.
    </para>
  </tip>
  <section xml:id="ollama-installing-app-details">
    <title>Details about the &ollama; application</title>
    <para>
      Before deploying &ollama;, it is important to know more about the
      supported configurations and documentation. The following command provides
      the corresponding details:
    </para>
<screen>helm show values oci://dp.apps.rancher.io/charts/ollama</screen>
    <para>
      Alternatively, you can also refer to the &ollama; &helm; chart page on the
      &sappco; site at
      <link xlink:href="https://apps.rancher.io/applications/ollama"/>. It
      contains the available versions and a link to pull the &ollama; container
      image.
    </para>
  </section>
  <section xml:id="ollama-installing-procedure">
    <title>&ollama; installation procedure</title>
    <procedure>
      <xi:include href="../snippets/ai-library-requirement.xml"/>
      <step>
        <para>
          Create the <filename>ollama_custom_overrides.yaml</filename> file to
          override the values of the parent &helm; chart. Refer to
          <xref linkend="ollama-helmchart"/> for more details.
        </para>
      </step>
      <step>
        <para>
          Install the &ollama; &helm; chart using the
          <filename>ollama-custom-overrides.yaml</filename> override file.
        </para>
<screen condition="deployment_standard">&prompt.user;<command>helm upgrade \
  --install ollama oci://dp.apps.rancher.io/charts/ollama \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  -f ollama_custom_overrides.yaml</command></screen>
<screen condition="deployment_airgapped">&prompt.user;<command>helm upgrade \
  --install ollama charts/ollama-<replaceable>X.Y.Z</replaceable>.tgz \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  -f ollama_custom_overrides.yaml</command></screen>
        <important condition="deployment_airgapped">
          <title>Downloading AI models</title>
          <para>
            &ollama; normally needs to have an active internet connection to
            download AI models. In air-gap environment, you must download the
            models manually and copy them to your local &ollama; instance, for
            example:
          </para>
<screen>kubectl cp
  <replaceable>PATH_TO_LOCALLY_DOWNLOADED_MODELS</replaceable>/blobs/* \
  <replaceable>OLLAMA_POD_NAME</replaceable>:~/.ollama/models/blobs/</screen>
        </important>
        <tip>
          <title>&huggingface; models</title>
          <para>
            Models downloaded from &huggingface; need to be converted before
            they can be used by &ollama;. Refer to
            <link xlink:href="https://github.com/ollama/ollama/blob/main/docs/import.md"/>
            for more details.
          </para>
        </tip>
      </step>
    </procedure>
  </section>
  <section xml:id="ollama-uninstalling">
    <title>Uninstalling &ollama;</title>
    <para>
      To uninstall &ollama;, run the following command:
    </para>
<screen>&prompt.user;<command>helm uninstall ollama -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command></screen>
  </section>
  <section xml:id="ollama-upgrading">
    <title>Upgrading &ollama;</title>
    <para>
      You can upgrade &ollama; to a specific version by running the following
      command:
    </para>
<screen>&prompt.user;<command>helm upgrade ollama oci://dp.apps.rancher.io/charts/ollama \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  --version <replaceable>OLLAMA_VERSION_NUMBER</replaceable> -f <replaceable>ollama_custom_overrides.yaml</replaceable></command></screen>
    <para>
      If you omit the <option>--version</option> option, &ollama; gets upgraded
      to the latest available version.
    </para>
    <section xml:id="ollama-upgrading-0-to-1">
      <title>Upgrading from version 0.x.x to 1.x.x</title>
      <para>
        The version 1.x.x introduces the ability to load models in memory at
        startup. To reflect this, change <option>ollama.models</option> to
        <option>ollama.models.pull</option> in the &ollama; &helm; chart to
        avoid errors before upgrading, for example:
      </para>
      <example>
        <title>&ollama; &helm; chart version 0.x.x</title>
<screen>
[...]
ollama:
  models:
    - "gemma:2b"
    - "llama3.1"</screen>
      </example>
      <example>
        <title>&ollama; &helm; chart version 1.x.x</title>
<screen>
[...]
ollama:
  models:
    pull:
      - "gemma:2b"
      - "llama3.1"</screen>
      </example>
      <para>
        Without this change you may experience the following error when trying
        to upgrade from 0.x.x to 1.x.x.
      </para>
<screen>coalesce.go:286: warning: cannot overwrite table with non table for
ollama.ollama.models (map[pull:[] run:[]])
Error: UPGRADE FAILED: template: ollama/templates/deployment.yaml:145:27:
executing "ollama/templates/deployment.yaml" at &lt;.Values.ollama.models.pull&gt;:
can't evaluate field pull in type interface {}</screen>
    </section>
  </section>
</topic>
