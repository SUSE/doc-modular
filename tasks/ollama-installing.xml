<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="ollama-installing"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Installing &ollama;</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        &ollama; is a tool for running and managing language models locally on
        your computer. It offers a simple interface to download, run and
        interact with models without relying on cloud resources.
      </para>
    </abstract>
  </info>
  <tip os="suseai">
    <para>
      When installing &suseai;, &ollama; is installed by the &owui; installation
      by default. If you decide to install &ollama; separately, disable its
      installation during the installation of &owui;.
    </para>
  </tip>
  <section xml:id="ollama-installing-app-details">
    <title>Details about the &ollama;; application</title>
    <para>
      Before deploying &ollama;, it is important to know more about the
      supported configurations and documentation. The following command provides
      the corresponding details:
    </para>
<screen>helm show values oci://dp.apps.rancher.io/charts/ollama</screen>
    <para>
      Alternatively, you can also refer to the &ollama; &helm; chart page on the
      &rancherappco; site at
      <link xlink:href="https://apps.rancher.io/applications/ollama"/>. It
      contains available versions and the link to pull the &ollama; container
      image.
    </para>
  </section>
  <section xml:id="ollama-installing-procedure">
    <title>&ollama; installation procedure</title>
    <procedure>
      <itemizedlist>
        <title>Requirements</title>
        <para>
          To install &ollama;, you need to have the following:
        </para>
        <listitem>
          <para>
            The <command>helm</command> command properly installed.
          </para>
        </listitem>
        <listitem>
          <para>
            A running &kube; cluster, such as &k3s;.
          </para>
        </listitem>
      </itemizedlist>
      <step>
        <para>
          Visit the &rancherappco;, sign in, and get the access token as
          described in
          <link
        xlink:href="https://docs.apps.rancher.io/get-started/authentication/"/>.
        </para>
      </step>
      <step performance="optional">
        <para>
          Create a &kube; namespace if it does not already exist. The steps in
          this procedure assume that all containers are deployed into the same
          namespace referred to as <replaceable>SUSE_AI_NAMESPACE</replaceable>.
          Replace its name to match your preferences.
        </para>
<screen>&prompt.user;<command>kubectl create namespace <replaceable>SUSE_AI_NAMESPACE</replaceable></command></screen>
      </step>
      <step>
        <para>
          Create the &rancherappco; secret.
        </para>
<screen>&prompt.user;<command>kubectl create secret docker-registry application-collection \
  --docker-server=dp.apps.rancher.io \
  --docker-username=<replaceable>APPCO_USERNAME</replaceable> \
  --docker-password=<replaceable>APPCO_USER_TOKEN</replaceable> \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command></screen>
      </step>
      <step>
        <para>
          Log in to the &helm; registry.
        </para>
<screen>&prompt.user;<command>helm registry login dp.apps.rancher.io/charts \
  -u <replaceable>APPCO_USERNAME</replaceable> \
  -p <replaceable>APPCO_USER_TOKEN</replaceable></command></screen>
      </step>
      <step>
        <para>
          Create the <filename>ollama_custom_overrides.yaml</filename> file to
          override the values of the parent &helm; chart.
        </para>
        <important>
          <title>GPU section</title>
          <para>
            &ollama; can run optimized for &nvidia; GPUs if the following
            conditions are fulfilled:
          </para>
          <itemizedlist>
            <listitem>
              <para>
                The &nvidia; driver and &nvoperator; are installed as described
                in
                <link xlink:href="https://documentation.suse.com/suse-ai/1.0/html/NVIDIA-GPU-driver-on-SL-Micro/index.html"/>.
              </para>
            </listitem>
            <listitem>
              <para>
                The workloads are set to run on &nvidia; enabled nodes as
                described in
                <link xlink:href="https://documentation.suse.com/suse-ai/1.0/html/AI-deployment-intro/index.html#ai-gpu-nodes-assigning"/>.
              </para>
            </listitem>
          </itemizedlist>
          <para>
            If you do not want to use the &nvidia; GPU, remove the
            <literal>gpu</literal> section from the
            <filename>ollama_custom_overrides.yaml</filename> below.
          </para>
<screen>global:
  imagePullSecrets:
  - application-collection
ingress:
  enabled: false
defaultModel: "gemma:2b"
ollama:
  models:
    - "gemma:2b"
    - "llama3.1"
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1</screen>
        </important>
      </step>
      <step>
        <para>
          Install the &ollama;; &helm; chart using the
          <filename>ollama-custom-overrides.yaml</filename> override file.
        </para>
<screen>&prompt.user;<command>helm upgrade --install ollama oci://dp.apps.rancher.io/charts/ollama \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  --version 0.54.0 -f <replaceable>ollama_custom_overrides.yaml</replaceable></command></screen>
      </step>
    </procedure>
  </section>
  <section xml:id="ollama-uninstalling">
    <title>Uninstalling &ollama;</title>
    <para>
      To uninstall &ollama;, run the following command:
    </para>
<screen>&prompt.user;<command>helm uninstall ollama -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command></screen>
  </section>
</topic>
