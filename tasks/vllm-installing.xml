<?xml version="1.0" encoding="UTF-8"?>
<!-- This file originates from the project https://github.com/openSUSE/doc-kit -->
<!-- This file can be edited downstream. -->
<!DOCTYPE topic
[
  <!ENTITY % entities SYSTEM "../common/generic-entities.ent">
    %entities;
]>
<topic xml:id="vllm-installing"
 role="task" xml:lang="en"
 xmlns="http://docbook.org/ns/docbook" version="5.2"
 xmlns:its="http://www.w3.org/2005/11/its"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:trans="http://docbook.org/ns/transclusion">
  <info>
    <title>Installing &vllm;</title>
    <meta name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract>
      <para>
        &vllm; is an open-source high-performance inference and serving engine
        for large language models (LLMs). It is designed to maximize throughput
        and reduce latency by using an efficient memory management system that
        handles dynamic batching and streaming outputs. In short, &vllm; makes
        running LLMs cheaper and faster in production.
      </para>
    </abstract>
<!-- TODO AI: udate apps in #ai-intro-how-works-->
  </info>
  <para>
    Deploying &vllm; on &k8s; is a scalable and efficient way to serve machine
    learning models. This guide walks you through deploying &vllm; using the its
    &helm; chart which is part of &ailibrary;. The &helm; chart deploys the full
    &vllm; production stack and enables you to run optimized LLM inference
    workloads on &nvidia; GPU in your &k8s; cluster. It consists of the
    following components:
  </para>
  <itemizedlist>
    <listitem>
      <para>
        <emphasis role="bold">Serving Engine</emphasis> runs the model
        inference.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">Router</emphasis> handles OpenAI-compatible API
        requests.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">&lmcache;</emphasis> (optional) improves caching
        efficiency.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">CacheServer </emphasis> (optional) is a
        distributed KV cache back-end.
      </para>
    </listitem>
  </itemizedlist>
  <section xml:id="vllm-installing-app-details">
    <title>Details about the &vllm; application</title>
    <para>
      Before deploying &vllm;, it is important to know more about the supported
      configurations and documentation. The following command provides the
      corresponding details:
    </para>
<screen>helm show values oci://dp.apps.rancher.io/charts/vllm</screen>
    <para>
      Alternatively, you can also refer to the &vllm; &helm; chart page on the
      &sappco; site at
      <link xlink:href="https://apps.rancher.io/applications/vllm"/>. It
      contains &vllm; dependencies, available versions and the link to pull the
      &vllm; container image.
    </para>
  </section>
  <section xml:id="vllm-installing-procedure">
    <title>&vllm; installation procedure</title>
    <procedure>
      <xi:include href="../snippets/ai-library-requirement.xml"/>
      <warning>
        <title>&nvidia; GPUs required</title>
        <para>
          &nvidia; GPUs must be available in your &k8s; cluster to successfully
          deploy and run &vllm;.
        </para>
      </warning>
      <important>
        <title>Limitation</title>
        <para>
          The current release of &productname; &vllm; does not support Ray and
          LoraController.
        </para>
      </important>
      <step>
        <para>
          Create the <filename>vllm_custom_overrides.yaml</filename> file to
          override the default values of the &helm; chart. The following minimal
          example includes a model that is publicly accessible.
        </para>
<screen>global:
  imagePullSecrets:
  - application-collection
servingEngineSpec:
  modelSpec:
  - name: "phi3-mini-4k"
    registry: "dp.apps.rancher.io"
    repository: "containers/vllm-openai"
    tag: "0.9.1"
    imagePullPolicy: "IfNotPresent"
    modelURL: "microsoft/Phi-3-mini-4k-instruct"
    replicaCount: 1
    requestCPU: 6
    requestMemory: "16Gi"
    requestGPU: 1
</screen>
      </step>
      <step>
        <para>
          Install the &vllm; &helm; chart using the
          <filename>vllm_custom_overrides.yaml</filename> override file.
        </para>
<screen>&prompt.user;<command>helm upgrade --install \
  vllm oci://dp.apps.rancher.io/charts/vllm \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  -f <replaceable>vllm_custom_overrides.yaml</replaceable></command></screen>
      </step>
    </procedure>
  </section>
  <section xml:id="vllm-upgrading">
    <title>Upgrading &vllm;</title>
    <para>
      The &vllm; chart receives application updates and updates of the &helm;
      chart templates. New versions may include changes that require manual
      steps. These steps are listed in the corresponding
      <filename>README</filename> file. All &vllm; dependencies are updated
      automatically during &vllm; upgrade.
    </para>
    <para>
      To upgrade &vllm;, identify the new version number and run the following
      command below:
    </para>
<screen>&prompt.user;<command>helm upgrade --install \
  vllm oci://dp.apps.rancher.io/charts/vllm \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  --version <replaceable>VERSION_NUMBER</replaceable> \
  -f <replaceable>vllm_custom_overrides.yaml</replaceable></command></screen>
    <tip>
      <para>
        If you omit the <option>--version</option> option, &vllm; gets upgraded
        to the latest available version.
      </para>
    </tip>
    <note>
      <title>Rolling update</title>
      <para>
        The <command>helm upgrade</command> command performs a rolling update on
        Deployments or StatefulSets with the following conditions:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            The <emphasis>old</emphasis> pod stays running until the
            <emphasis>new</emphasis> pod passes readiness checks.
          </para>
        </listitem>
        <listitem>
          <para>
            If the cluster is already at GPU capacity, the new pod cannot start
            because there is no GPU left to schedule it. This requires to patch
            the deployment using the <emphasis>Recreate</emphasis> update
            strategy. The following commands identify the &vllm; deployment name
            and patch its deployment.
          </para>
<screen>&prompt.user;<command>kubectl get deployments -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command>
&prompt.user;<command>kubectl patch deployment <replaceable>VLLM_DEPLOYMENT_NAME</replaceable> \
  -n <replaceable>SUSE_AI_NAMESPACE</replaceable> \
  -p '{"spec": {"strategy": {"type": "Recreate", "rollingUpdate": null}}}'</command></screen>
        </listitem>
      </itemizedlist>
    </note>
  </section>
  <section xml:id="vllm-uninstalling">
    <title>Uninstalling &vllm;</title>
    <para>
      To uninstall &vllm;, run the following command:
    </para>
<screen>&prompt.user;<command>helm uninstall vllm -n <replaceable>SUSE_AI_NAMESPACE</replaceable></command></screen>
  </section>
</topic>
