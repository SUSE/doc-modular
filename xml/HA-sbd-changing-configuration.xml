<?xml version="1.0"?>
<article xmlns="http://docbook.org/ns/docbook" version="5.2" xml:id="ha-sbd-changing-configuration" xml:lang="en">
  <info>
      <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">Changing the Configuration of SBD</title>
      <revhistory xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xml:id="rh-ha-sbd-changing-configuration">
        <revision><date>2025-11-04</date>
          <revdescription>
            <para>
              Initial version
            </para>
          </revdescription>
        </revision>
      </revhistory>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="architecture">
        <phrase>AMD64/Intel&#xA0;64</phrase>
        <phrase>POWER</phrase>
        <phrase>IBM&#xA0;Z</phrase>
      </meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="productname" its:translate="no">
        <productname version="16.0" os="sleha"><phrase><phrase os="sleha">SUSE Linux Enterprise High Availability</phrase></phrase></productname>
      </meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="title" its:translate="yes">Changing the Configuration of SBD</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="description" its:translate="yes">How to change a High Availability cluster's SBD configuration when the SBD service is already running</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="social-descr" its:translate="yes">Change the configuration of a running SBD setup</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="task" its:translate="no">
        <phrase>Administration</phrase>
        <phrase>Clustering</phrase>
        <phrase>Configuration</phrase>
        <phrase>High Availability</phrase>
      </meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" name="series" its:translate="no">Products &amp; Solutions</meta>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:component>Documentation</dm:component>
          <dm:product>
            <dm:product>SUSE Linux Enterprise High Availability 16.0</dm:product>
          </dm:product>
          <dm:assignee>tahlia.richardson@suse.com@suse.com</dm:assignee>
       </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
      <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude">
      <variablelist>
        <varlistentry>
          <term>WHAT?</term>
          <listitem>
            <para>
              How to change a High Availability cluster's SBD configuration when the SBD service is
              already running.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>WHY?</term>
          <listitem>
            <para>
              You might need to change the cluster's SBD configuration for various reasons,
              such as increasing resilience, using custom settings instead of the defaults,
              or switching to a different fencing mechanism.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>EFFORT</term>
            <listitem>
              <para>
                Each task in this article only takes a few minutes and does not require any
                downtime for cluster resources.
              </para>
            </listitem>
        </varlistentry>
        
        <varlistentry>
          <term>REQUIREMENTS</term>
            <listitem>
              <itemizedlist>
                <listitem>
                  <para>
                    An existing SUSE Linux Enterprise High Availability cluster
                  </para>
                </listitem>
                <listitem>
                  <para>
                    SBD already configured and running
                  </para>
                </listitem>
                <listitem>
                  <para>
                    A hardware watchdog device on all cluster nodes
                  </para>
                </listitem>
                <listitem>
                  <para>
                    Shared storage accessible from all nodes (if using disk-based SBD)
                  </para>
                </listitem>
              </itemizedlist>
            </listitem>
          </varlistentry>
        </variablelist>
      </abstract>
    </info>
  <section role="concept" xml:lang="en" version="5.2" xml:id="ha-sbd-what-is"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">What is SBD?</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        SBD (STONITH Block Device) provides a node fencing mechanism without using an external power-off
        device. The software component (the SBD daemon) works together with a watchdog device to
        ensure that misbehaving nodes are fenced. SBD can be used in disk-based mode with shared
        block storage, or in diskless mode using only the watchdog.
      </para>
      <para>
        Disk-based SBD uses shared block storage to exchange fencing messages between the nodes.
        It can be used with one to three devices. One device is appropriate for simple cluster setups,
        but two or three devices are recommended for more complex setups or critical workloads.
      </para>
      <para>
        Diskless SBD fences nodes by using only the watchdog, without relying on a shared storage
        device. A node is fenced if it loses quorum, if any monitored daemon is lost and cannot be
        recovered, or if Pacemaker determines that the node requires fencing.
      </para>
    </abstract>
  </info>
  
  <section xml:id="ha-sbd-what-is-components">
    <title>Components</title>
    <variablelist>
      <varlistentry>
        <term>SBD daemon</term>
        <listitem>
          <para>
            The SBD daemon starts on each node before the rest of the cluster stack and stops
            in the reverse order. This ensures that cluster resources are never active without
            SBD supervision.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>SBD device (disk-based SBD)</term>
        <listitem>
          <para>
            A small logical unit (or a small partition on a logical unit) is formatted for use with
            SBD. A message layout is created on the device with slots for up to 255 nodes.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Messages (disk-based SBD)</term>
        <listitem>
          <para>
            The message layout on the SBD device is used to send fencing messages to nodes.
            The SBD daemon on each node monitors the message slot and immediately complies with
            any requests. To avoid becoming disconnected from fencing messages, the SBD daemon
            also fences the node if it loses its connection to the SBD device.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Watchdog</term>
        <listitem>
          <para version="5.2">
  SBD needs a watchdog on each node to ensure that misbehaving nodes are really stopped.
  SBD <quote>feeds</quote> the watchdog by regularly writing a service pulse to it. If SBD
  stops feeding the watchdog, the hardware enforces a system restart. This protects against
  failures of the SBD process itself, such as becoming stuck on an I/O error.
</para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="ha-sbd-limitations-recommendations">
    <title>Limitations and recommendations</title>
    <variablelist>
      <varlistentry>
        <term>Disk-based SBD</term>
        <listitem>
          <itemizedlist>
            <listitem>
              <para>
                The shared storage can be Fibre Channel (FC), Fibre Channel over Ethernet (FCoE),
                or iSCSI.
              </para>
            </listitem>
            <listitem>
              <para>
                The shared storage must <emphasis>not</emphasis> use host-based RAID, LVM,
                Cluster MD, or DRBD.
              </para>
            </listitem>
            <listitem>
              <para>
                Using storage-based RAID and multipathing is recommended for increased reliability.
              </para>
            </listitem>
            <listitem>
              <para>
                If a shared storage device has different <filename>/dev/sdX</filename> names on
                different nodes, SBD communication will fail. To avoid this, always use stable
                device names, such as <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>.
              </para>
            </listitem>
            <listitem>
              <para>
                An SBD device can be shared between different clusters, up to a limit of 255 nodes.
              </para>
            </listitem>
            <listitem>
              <para>
                When using more than one SBD device, all devices must have the same configuration.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Diskless SBD</term>
        <listitem>
          <itemizedlist>
            <listitem>
              <para>
                Diskless SBD cannot handle a split-brain scenario for a two-node cluster. This
                configuration should only be used for clusters with more than two nodes, or in
                combination with QDevice to help handle split-brain scenarios.
              </para>
            </listitem>
          </itemizedlist>
        </listitem>
      </varlistentry>
    </variablelist>

  </section>
  <section xml:id="ha-sbd-what-is-more-info">
    <title>For more information</title>
    <para>
      For more information, see the man page <literal>sbd</literal> or run the
      <command>crm sbd help</command> command.
    </para>
  </section>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-changing-timeout-settings"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Changing the SBD timeout settings</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        SBD relies on multiple different timeout settings to manage node fencing. When you
        configure SBD using the CRM Shell, these timeouts are automatically calculated and
        adjusted. The automatic values are sufficient for most use cases, but if you need to
        change them, you can use the <command>crm sbd configure</command> command.
    </para>
    </abstract>
  </info>
  

  <important version="5.2">
  <title>Cluster restart required</title>
  <para>
    In this procedure, the script checks whether it is safe to restart the cluster services
    automatically. If any non-<literal>stonith</literal> resources are running, the script warns
    you to restart the cluster services manually. This allows you to put the cluster into
    maintenance mode first to avoid resource downtime. However, be aware that the resources will
    not have cluster protection while in maintenance mode.
  </para>
</important>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        SBD is already configured and running.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
<procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check the current timeout settings:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show</command></screen>
    </step>
    <step>
      <para>
        Change one or both of the following timeout values as needed:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure \</command>
  <command>watchdog-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable> \</command><co xml:id="co-sbd-watchdog-timeout"/>
  <command>msgwait-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable> \</command><co xml:id="co-sbd-msgwait-timeout"/></screen>
      <para>
        If you change one timeout, the other timeout is automatically adjusted so that the
        <literal>msgwait-timeout</literal> is twice the <literal>watchdog-timeout</literal>. You
        only need to change both timeouts manually if you want the <literal>msgwait-timeout</literal>
        to be <emphasis>more</emphasis> than double the <literal>watchdog-timeout</literal>.
        If you try to make the <literal>msgwait-timeout</literal> <emphasis>less</emphasis> than
        double the <literal>watchdog-timeout</literal>, the command fails with a warning.
      </para>
      <calloutlist>
        <callout arearefs="co-sbd-watchdog-timeout">
          <para>
            The <literal>watchdog-timeout</literal> defines how long the watchdog waits for a
            response from SBD before fencing the node. Diskless SBD reads this timeout from
            <filename>/etc/sysconfig/sbd</filename>, but disk-based SBD reads it from the device
            metadata, which takes precedence over the settings in <filename>/etc/sysconfig/sbd</filename>.
          </para>
          <para>
            For disk-based SBD on a multipath setup, this timeout must be longer than the
            <literal>max_polling_interval</literal> in <filename>/etc/multipath.conf</filename>,
            to allow enough time to detect a path failure and switch to the next path.
         </para>
        </callout>
        <callout arearefs="co-sbd-msgwait-timeout">
          <para>
            <emphasis>Only used for disk-based SBD.</emphasis>
            After the <literal>msgwait-timeout</literal> is reached, SBD assumes that a message
            written to the node's slot on the SBD device was delivered successfully. The timeout
            must be long enough for the node to detect that it needs to self-fence.
          </para>
          <para>
            When you increase this timeout, the script also automatically adjusts the
            <literal>SBD_DELAY_START</literal> setting. This helps to avoid a situation where
            a node reboots too quickly and rejoins the cluster before the fencing action is
            considered complete, which can cause a split-brain scenario.
          </para>
        </callout>
      </calloutlist>
      <tip role="compact">
        <para>
          You should not need to change the <literal>allocate-timeout</literal> or the
          <literal>loop-timeout</literal>.
        </para>
      </tip>
      <para>
        The script automatically adjusts any other related timeouts in the cluster and displays
        the new values. The script also checks whether it is safe to restart the cluster services
        automatically. If any non-<literal>stonith</literal> resources are running, the script
        warns you to restart the cluster services manually.
      </para>
    </step>
    <step version="5.2">
  <para>
    If you need to restart the cluster services manually, follow these steps to avoid
    resource downtime:
  </para>
  <substeps>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step>
      <para>
        Restart the cluster services on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
  </substeps>
</step>
    <step>
      <para>
        When you change a timeout with <command>crm sbd configure</command>, the global
        fencing timeouts are also adjusted automatically. The automatic values are
        sufficient for most use cases, but if you need to change them, you can use the
        <command>crm configure property</command> command:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm configure property stonith-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable></command><co xml:id="co-stonith-timeout"/>
<prompt>&gt; </prompt><command>sudo crm configure property stonith-watchdog-timeout=<replaceable>INTEGER_IN_SECONDS</replaceable></command><co xml:id="co-stonith-watchdog-timeout"/></screen>
      <para>
        This command does <emphasis>not</emphasis> automatically adjust any other timeouts,
        and these settings might be overwritten if you change the SBD configuration again.
      </para>
      <calloutlist>
        <callout arearefs="co-stonith-timeout">
          <para>
            The <literal>stonith-timeout</literal> defines how long to wait for the fencing
            action to complete.
          </para>
       </callout>
       <callout arearefs="co-stonith-watchdog-timeout">
          <para>
            <emphasis>Only used for diskless SBD.</emphasis>
            The <literal>stonith-watchdog-timeout</literal> defines how long to wait for the
            watchdog to fence the node.
          </para>
        </callout>
      </calloutlist>
    </step>
    <step>
      <para>
        Confirm that the timeout settings changed:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show</command></screen>
    </step>
  </procedure>

  <para>
    If you need to manually calculate any timeouts, you can use these basic formulas for most
    use cases:
  </para>
  <variablelist xml:id="ha-sbd-calculating-timeouts">
    <varlistentry>
      <term>Disk-based SBD</term>
      <listitem>
        <para>
          <literal>msgwait-timeout &gt;= (watchdog-timeout * 2)</literal>
        </para>
        <para>
          <literal>stonith-timeout &gt;= msgwait-timeout + 20%</literal>
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Diskless SBD</term>
      <listitem>
        <para>
          <literal>stonith-watchdog-timeout &gt;= (watchdog-timeout * 2)</literal>
        </para>
        <para>
          <literal>stonith-timeout &gt;= stonith-watchdog-timeout + 20%</literal>
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    For more information, run the <command>crm help TimeoutFormulas</command> command.
  </para>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-changing-watchdog-device"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Changing the SBD watchdog device</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to change the watchdog device that SBD uses. This can be useful if
        your system has multiple hardware watchdogs available or if you need to switch from the
        software watchdog to a hardware watchdog.
      </para>
    </abstract>
  </info>
  

  <important version="5.2">
  <title>Cluster restart required</title>
  <para>
    In this procedure, the script checks whether it is safe to restart the cluster services
    automatically. If any non-<literal>stonith</literal> resources are running, the script warns
    you to restart the cluster services manually. This allows you to put the cluster into
    maintenance mode first to avoid resource downtime. However, be aware that the resources will
    not have cluster protection while in maintenance mode.
  </para>
</important>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        SBD is already configured and running.
      </para>
    </listitem>
    <listitem>
      <para>
        All nodes have the new watchdog device available.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check the available watchdog devices:
      </para>
<screen><prompt>&gt; </prompt><command>sudo sbd query-watchdog</command></screen>
      <para>
        The output shows which watchdog is being used by SBD and which other watchdogs are available.
      </para>
    </step>
    <step>
      <para>
        Change the SBD watchdog device, specifying either the device name (for example,
        <filename>/dev/watchdog1</filename>) or the driver name (for example,
        <systemitem>iTCO_wdt</systemitem>):
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure watchdog-device=<replaceable>WATCHDOG</replaceable></command></screen>
      <para>
        The script updates the SBD configuration file with the new watchdog device and
        checks whether it is safe to restart the cluster services automatically. If any
        non-<literal>stonith</literal> resources are running, the script warns you to restart
        the cluster services manually.
      </para>
    </step>
    <step version="5.2">
  <para>
    If you need to restart the cluster services manually, follow these steps to avoid
    resource downtime:
  </para>
  <substeps>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step>
      <para>
        Restart the cluster services on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
  </substeps>
</step>
    <step>
      <para>
        Check the watchdog devices again:
      </para>
<screen><prompt>&gt; </prompt><command>sudo sbd query-watchdog</command></screen>
      <para>
        The output should now show the new watchdog device being used by SBD.
      </para>
    </step>
    <step>
      <para>
        Check that the correct watchdog is configured on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command></screen>
      <para>
        In the section <literal>Watchdog info</literal>, the watchdog device and driver should be
        the same on every node.
      </para>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-changing-diskless-to-diskbased"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Changing diskless SBD to disk-based SBD</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to change diskless SBD to disk-based SBD.
      </para>
    </abstract>
  </info>
  

  <important>
    <title>Cluster restart required</title>
    <para>
      In this procedure, the setup script automatically puts the cluster into maintenance mode
      and restarts the cluster services. In maintenance mode, the cluster stops monitoring all
      resources. This allows the services managed by the resources to keep running even during
      the cluster restart. However, be aware that the resources will not have cluster protection
      while in maintenance mode.
    </para>
  </important>
  <warning>
    <title>Overwriting existing data</title>
    <para>
      Make sure any device you want to use for SBD does not hold any important data. Configuring
      a device for use with SBD overwrites the existing data.
    </para>
  </warning>
  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        Diskless SBD is configured and running.
      </para>
    </listitem>
    <listitem>
      <para>
        All nodes can access shared storage.
      </para>
    </listitem>
    <listitem>
      <para>
        The path to the shared storage device is consistent across all nodes. Use stable device
        names such as <filename>/dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></filename>.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command>
# Type of SBD:
Diskless SBD configured</screen>
    </step>
    <step>
      <para>
        Configure disk-based SBD. Use <option>--force</option> (or <option>-F</option>) to allow
        you to reconfigure SBD even when it is already running, and <option>--sbd-device</option>
        (or <option>-s</option>) to specify the shared storage device:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm --force cluster init sbd --sbd-device /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <tip role="compact">
        <para>
          You can use <option>--sbd-device</option> (or <option>-s</option>) multiple times to
          configure up to three SBD devices.
        </para>
      </tip>
      <para>
        The script initializes SBD on the shared storage device, creates a
        <literal>fence_sbd</literal> cluster resource, and updates the SBD configuration
        file and timeout settings. The script also puts the cluster into maintenance mode,
        restarts the cluster services, then puts the cluster back into normal operation.
      </para>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes should be <literal>Online</literal> and the resources <literal>Started</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command>
# Type of SBD:
Disk-based SBD configured</screen>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-changing-diskbased-to-diskless"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Changing disk-based SBD to diskless SBD</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to change disk-based SBD to diskless SBD.
      </para>
      <para>
        Diskless SBD cannot handle a split-brain scenario for a two-node cluster. This
        configuration should only be used for clusters with more than two nodes, or in
        combination with QDevice to help handle split-brain scenarios.
      </para>
    </abstract>
  </info>
  

  <important>
    <title>Cluster restart required</title>
    <para>
      In this procedure, the setup script automatically puts the cluster into maintenance mode
      and restarts the cluster services. In maintenance mode, the cluster stops monitoring all
      resources. This allows the services managed by the resources to keep running even during
      the cluster restart. However, be aware that the resources will not have cluster protection
      while in maintenance mode.
    </para>
  </important>
  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        Disk-based SBD is configured and running.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command>
# Type of SBD:
Disk-based SBD configured</screen>
    </step>
    <step>
      <para>
        Configure diskless SBD. Use <option>--force</option> (or <option>-F</option>) to allow
        you to reconfigure SBD even when it is already running, and <option>--enable-sbd</option>
        (or <option>-S</option>) to specify that no device is needed:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm --force cluster init sbd --enable-sbd</command></screen>
      <para>
        The script stops and removes the <literal>fence_sbd</literal> cluster resource, then
        updates the SBD configuration file and timeout settings. The script also puts the
        cluster into maintenance mode, restarts the cluster services, then puts the cluster
        back into normal operation.
      </para>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes should be <literal>Online</literal> and the resources <literal>Started</literal>.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command>
# Type of SBD:
Diskless SBD configured</screen>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-adding-another-device"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Adding another SBD device</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to add more SBD devices to a cluster that already has disk-based
        SBD configured. The cluster can have up to three SBD devices.
      </para>
    </abstract>
  </info>
  

  <important version="5.2">
  <title>Cluster restart required</title>
  <para>
    In this procedure, the script checks whether it is safe to restart the cluster services
    automatically. If any non-<literal>stonith</literal> resources are running, the script warns
    you to restart the cluster services manually. This allows you to put the cluster into
    maintenance mode first to avoid resource downtime. However, be aware that the resources will
    not have cluster protection while in maintenance mode.
  </para>
</important>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        Disk-based SBD is already configured and running with at least one device.
      </para>
    </listitem>
    <listitem>
      <para>
        An additional shared storage device is accessible from all cluster nodes.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check which device or devices are already configured for use with SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output shows one or more device IDs in the <literal>SBD_DEVICE</literal> line.
      </para>
    </step>
    <step>
      <para>
        Add a new device to the existing SBD configuration:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device add /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <para>
        The script initializes SBD on the new device, updates the SBD configuration file,
        and checks whether it is safe to restart the cluster services automatically. If any
        non-<literal>stonith</literal> resources are running, the script warns you to restart
        the cluster services manually.
      </para>
    </step>
    <step version="5.2">
  <para>
    If you need to restart the cluster services manually, follow these steps to avoid
    resource downtime:
  </para>
  <substeps>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step>
      <para>
        Restart the cluster services on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
  </substeps>
</step>
    <step>
      <para>
        Check the SBD configuration again:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output should now show more devices.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD to make sure all the nodes can see the new device:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command></screen>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-replacing-existing-device-with-new-device"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Replacing an existing SBD device with a new device</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        If you need to replace an SBD device, you can use <command>crm sbd device add</command>
        to add the new device and <command>crm sbd device remove</command> to remove the old device.
        If the cluster has two SBD devices, you can run these commands in any order. However, if
        the cluster has one or three SBD devices, you must run these commands in a specific order:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            One device: <command>crm sbd device remove</command> cannot remove the only device,
            so you must add the new device before you can remove the old device.
          </para>
        </listitem>
        <listitem>
          <para>
            Three devices: <command>crm sbd device add</command> cannot add a fourth device,
            so you must remove the old device before you can add the new device.
          </para>
        </listitem>
      </itemizedlist>
    </abstract>
  </info>
  

  <important>
    <title>Cluster restart required</title>
    <para>
      In this procedure, the cluster services must be restarted twice: once after adding the new
      device <emphasis>and</emphasis> once after removing the old device. We recommend putting the
      cluster into maintenance mode first to avoid resource downtime. However, be aware that the
      resources will not have cluster protection while in maintenance mode.
    </para>
  </important>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        Disk-based SBD is already configured and running with at least one device.
      </para>
    </listitem>
    <listitem>
      <para>
        An additional shared storage device is accessible from all cluster nodes.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step xml:id="ha-sbd-replacing-check-sbd-devices">
      <para>
        Check how many devices are already configured for use with SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output shows one or more device IDs in the <literal>SBD_DEVICE</literal> line.
        The number of devices determines the order of the next steps.
      </para>
    </step>
    <step>
      <para>
        Add or remove a device, depending on the number of devices shown in
        <xref linkend="ha-sbd-replacing-check-sbd-devices"/>:
      </para>
      <variablelist>
        <varlistentry>
          <term>One device:</term>
          <listitem>
            <para>
              Add the new device to the existing SBD configuration:
            </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device add /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
            <para>
              The script restarts the cluster services automatically.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Two or three devices:</term>
          <listitem>
            <para>
              Remove the old device from the SBD configuration:
            </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device remove /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
            <para>
              The script warns you to restart the cluster services manually.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </step>
    <step>
      <para>
        If you need to restart the cluster services manually, run the following command:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        Add or remove a device, depending on the number of devices shown in
        <xref linkend="ha-sbd-replacing-check-sbd-devices"/>:
      </para>
      <variablelist>
        <varlistentry>
          <term>One device:</term>
          <listitem>
            <para>
              Remove the old device from the SBD configuration:
            </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device remove /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
            <para>
              The script warns you to restart the cluster services manually.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Two or three devices:</term>
          <listitem>
            <para>
              Add the new device to the existing SBD configuration:
            </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device add /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
            <para>
              The script restarts the cluster services automatically.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </step>
    <step>
      <para>
        If you need to restart the cluster services manually, run the following command:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
    <step>
      <para>
        Check the SBD configuration again:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output should now show the new device in the <literal>SBD_DEVICE</literal> line.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD to make sure the correct device is listed:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command></screen>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-removing-device"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Removing an SBD device</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to remove an SBD device from a cluster with multiple SBD devices
        configured. You cannot use this method if there is only one SBD device configured.
      </para>
    </abstract>
  </info>
  

  <important version="5.2">
  <title>Cluster restart required</title>
  <para>
    In this procedure, the script checks whether it is safe to restart the cluster services
    automatically. If any non-<literal>stonith</literal> resources are running, the script warns
    you to restart the cluster services manually. This allows you to put the cluster into
    maintenance mode first to avoid resource downtime. However, be aware that the resources will
    not have cluster protection while in maintenance mode.
  </para>
</important>

  <itemizedlist>
    <title>Requirements</title>
    <listitem>
      <para>
        Disk-based SBD is configured with more than one device.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Check which devices are already configured for use with SBD:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output shows multiple device IDs in the <literal>SBD_DEVICE</literal> line.
      </para>
    </step>
    <step>
      <para>
        Remove a device from the SBD configuration:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd device remove /dev/disk/by-id/<replaceable>DEVICE_ID</replaceable></command></screen>
      <para>
        The script removes the device, updates the SBD configuration file, and checks whether it
        is safe to restart the cluster services automatically. If any non-<literal>stonith</literal>
        resources are running, the script warns you to restart the cluster services manually.
      </para>
    </step>
    <step version="5.2">
  <para>
    If you need to restart the cluster services manually, follow these steps to avoid
    resource downtime:
  </para>
  <substeps>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step>
      <para>
        Restart the cluster services on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
  </substeps>
</step>
    <step>
      <para>
        Check the SBD configuration again:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd configure show sysconfig</command></screen>
      <para>
        The output should now show fewer devices.
      </para>
    </step>
    <step>
      <para>
        Check the status of SBD to make sure the device was removed from all the nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command></screen>
    </step>
  </procedure>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="ha-sbd-removing-all-configuration"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Removing all SBD configuration</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Use this procedure to remove all SBD-related configuration from the cluster. You might
        need to do this if you want to switch from SBD to a physical fencing device. Keep in
        mind that to be supported, all SUSE Linux Enterprise High Availability clusters <emphasis>must</emphasis> have either
        SBD or a physical fencing device configured.
      </para>
    </abstract>
  </info>
  

  <important version="5.2">
  <title>Cluster restart required</title>
  <para>
    In this procedure, the script checks whether it is safe to restart the cluster services
    automatically. If any non-<literal>stonith</literal> resources are running, the script warns
    you to restart the cluster services manually. This allows you to put the cluster into
    maintenance mode first to avoid resource downtime. However, be aware that the resources will
    not have cluster protection while in maintenance mode.
  </para>
</important>

  <para>
    Perform this procedure on only one node in the cluster:
  </para>
  <procedure>
    <step>
      <para>
        Log in either as the <systemitem class="username">root</systemitem> user or as a user with <command>sudo</command> privileges.
      </para>
    </step>
    <step>
      <para>
        Remove the SBD configuration from the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd purge</command></screen>
      <para>
        The script stops the SBD service on all nodes, moves the SBD configuration file
        to a backup file, and adjusts any SBD-related cluster properties. The script also
        checks whether it is safe to restart the cluster services automatically. If any
        non-<literal>stonith</literal> resources are running, the script warns you to restart
        the cluster services manually.
      </para>
    </step>
    <step version="5.2">
  <para>
    If you need to restart the cluster services manually, follow these steps to avoid
    resource downtime:
  </para>
  <substeps>
    <step>
      <para>
        Put the cluster into maintenance mode:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance on</command></screen>
      <para>
        In this state, the cluster stops monitoring all resources. This allows the services
        managed by the resources to keep running while the cluster restarts. However, be aware
        that the resources will not have cluster protection while in maintenance mode.
      </para>
    </step>
    <step>
      <para>
        Restart the cluster services on all nodes:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm cluster restart --all</command></screen>
    </step>
    <step>
      <para>
        Check the status of the cluster:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm status</command></screen>
      <para>
        The nodes will have the status <literal>UNCLEAN (offline)</literal>, but will soon
        change to <literal>Online</literal>.
      </para>
    </step>
    <step>
      <para>
        When the nodes are back online, put the cluster back into normal operation:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm maintenance off</command></screen>
    </step>
  </substeps>
</step>
    <step>
      <para>
        Confirm that the SBD configuration is gone:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm sbd status</command>
ERROR: SBD configuration file /etc/sysconfig/sbd not found</screen>
    </step>
    <step>
      <para>
        Check the cluster configuration:
      </para>
<screen><prompt>&gt; </prompt><command>sudo crm configure show</command></screen>
      <para>
        The output should show <literal>stonith-enabled=false</literal> and no other SBD-related
        properties.
      </para>
    </step>
  </procedure>
</section>
  <section version="5.2" xml:id="legal-disclaimer"><info>
    <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Legal Notice</title>
  </info>
  
  <para> Copyright&#xA9; 2006&#x2013;<?dbtimestamp format="Y"?>
 SUSE LLC and contributors.
  All rights reserved. </para>
  <para>
    Permission is granted to copy, distribute and/or modify this document under the terms of the
    GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the Invariant
    Section being this copyright notice and license. A copy of the license version 1.2 is included
    in the section entitled <quote>GNU Free Documentation License</quote>.
  </para>
  <para>
    For SUSE trademarks, see <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.suse.com/company/legal/"/>. All other
    third-party trademarks are the property of their respective owners. Trademark symbols (&#xAE;, &#x2122;
    etc.) denote trademarks of SUSE and its affiliates. Asterisks (*) denote third-party
    trademarks.
  </para>
  <para>
    All information found in this book has been compiled with utmost attention to detail. However,
    this does not guarantee complete accuracy.  Neither SUSE LLC, its affiliates, the authors, nor
    the translators shall be held liable for possible errors or the consequences thereof.
  </para>
</section>
  <appendix xmlns:its="http://www.w3.org/2005/11/its" version="5.2" role="legal" its:translate="no" xml:id="doc-gfdl-license"><info>
   <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">GNU Free Documentation License</title>
 </info>

 

 <para>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </para>

 <bridgehead renderas="sect4">
    0. PREAMBLE
  </bridgehead>

 <para>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </para>

 <para>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </para>

 <para>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </para>

 <bridgehead renderas="sect4">
    1. APPLICABILITY AND DEFINITIONS
  </bridgehead>

 <para>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </para>

 <para>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </para>

 <para>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </para>

 <para>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </para>

 <para>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </para>

 <para>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </para>

 <para>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </para>

 <para>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </para>

 <para>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </para>

 <para>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </para>

 <bridgehead renderas="sect4">
    2. VERBATIM COPYING
  </bridgehead>

 <para>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </para>

 <para>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </para>

 <bridgehead renderas="sect4">
    3. COPYING IN QUANTITY
  </bridgehead>

 <para>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </para>

 <para>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </para>

 <para>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </para>

 <para>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </para>

 <bridgehead renderas="sect4">
    4. MODIFICATIONS
  </bridgehead>

 <para>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </para>

 <orderedlist numeration="upperalpha" spacing="normal">
  <listitem>
   <para>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </para>
  </listitem>
  <listitem>
   <para>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </para>
  </listitem>
  <listitem>
   <para>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve all the copyright notices of the Document.
   </para>
  </listitem>
  <listitem>
   <para>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </para>
  </listitem>
  <listitem>
   <para>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </para>
  </listitem>
  <listitem>
   <para>
    Include an unaltered copy of this License.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </para>
  </listitem>
  <listitem>
   <para>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </para>
  </listitem>
  <listitem>
   <para>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </para>
  </listitem>
  <listitem>
   <para>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve any Warranty Disclaimers.
   </para>
  </listitem>
 </orderedlist>

 <para>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </para>

 <para>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </para>

 <para>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </para>

 <para>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </para>

 <bridgehead renderas="sect4">
    5. COMBINING DOCUMENTS
  </bridgehead>

 <para>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </para>

 <para>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </para>

 <para>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </para>

 <bridgehead renderas="sect4">
    6. COLLECTIONS OF DOCUMENTS
  </bridgehead>

 <para>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </para>

 <para>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </para>

 <bridgehead renderas="sect4">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </bridgehead>

 <para>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </para>

 <para>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </para>

 <bridgehead renderas="sect4">
    8. TRANSLATION
  </bridgehead>

 <para>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </para>

 <para>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </para>

 <bridgehead renderas="sect4">
    9. TERMINATION
  </bridgehead>

 <para>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </para>

 <bridgehead renderas="sect4">
    10. FUTURE REVISIONS OF THIS LICENSE
  </bridgehead>

 <para>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.gnu.org/copyleft/"/>.
 </para>

 <para>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </para>

 <bridgehead renderas="sect4">
    ADDENDUM: How to use this License for your documents
  </bridgehead>

<screen>Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled &#x201C;GNU
Free Documentation License&#x201D;.</screen>

 <para>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the &#x201C;with...Texts.&#x201D; line with this:
 </para>

<screen>with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</screen>

 <para>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </para>

 <para>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
</para>
</appendix>
  <glossary role="reference" xml:lang="en" version="5.2" xml:id="ha-glossary"><info>
   <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">HA glossary</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tahlia.richardson@suse.com" its:translate="no"/>
  </info>
  
  <glossentry><glossterm>active/active, active/passive</glossterm>
    <glossdef>
    <para>
      How resources run on the nodes. Active/passive means that resources only run on the active
      node, but can move to the passive node if the active node fails. Active/active means that all
      nodes are active at once, and resources can run on (and move to) any node in the cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-arbitrator"><glossterm>arbitrator</glossterm>
    <glossdef>
    <para>
      An <emphasis>arbitrator</emphasis> is a machine running outside the cluster to provide an
      additional instance for cluster calculations. For example, <xref linkend="gloss-qnetd"/>
      provides a vote to help <xref linkend="gloss-qdevice"/> participate in
      <xref linkend="gloss-quorum"/> decisions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>CIB (cluster information base)</glossterm>
    <glossdef>
    <para>
      An XML representation of the whole cluster configuration and status (cluster options,
      nodes, resources, constraints and the relationships to each other). The CIB manager
      (<systemitem>pacemaker-based</systemitem>) keeps the CIB synchronized across the cluster
      and handles requests to modify it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-clone"><glossterm>clone</glossterm>
    <glossdef>
    <para>
      A <emphasis>clone</emphasis> is an identical copy of an existing node, used to make
      deploying multiple nodes simpler.
    </para>
    <para>
      In the context of a cluster <xref linkend="gloss-resource"/>, a clone is a resource that can
      be active on multiple nodes. Any resource can be cloned if its resource agent supports it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>cluster</glossterm>
    <glossdef>
    <para>
      A <emphasis>high-availability</emphasis> cluster is a group of servers (physical or virtual)
      designed primarily to secure the highest possible availability of data, applications and services.
      Not to be confused with a <emphasis>high-performance</emphasis> cluster, which shares the
      application load to achieve faster results.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Cluster logical volume manager (Cluster LVM)</glossterm>
    <glossdef>
    <para>
      The term <emphasis>Cluster LVM</emphasis> indicates that LVM is being used in a cluster environment.
      This requires configuration adjustments to protect the LVM metadata on shared storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-clus-part"><glossterm>cluster partition</glossterm>
    <glossdef>
    <para>
      A cluster partition occurs when communication fails between one or more nodes and the rest of
      the cluster. The nodes are split into partitions but are still active. They can only
      communicate with nodes in the same partition and are unaware of the separated nodes. This
      is known as a <xref linkend="gloss-splitbrain"/> scenario.
    </para>
    </glossdef>
  </glossentry>
  <glossentry>
    <glossterm>cluster stack</glossterm>
    <glossdef>
    <para>
      The ensemble of software technologies and components that make up a cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-col-con">
    <glossterm>colocation constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that specifies which resources can or cannot
      run together on a node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>concurrency violation</glossterm>
    <glossdef>
    <para>
      A resource that should be running on only one node in the cluster is running on several nodes.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-corosync"><glossterm>Corosync</glossterm>
    <glossdef>
    <para>
      Corosync provides reliable messaging, membership and quorum information about the cluster.
      This is handled by the Corosync Cluster Engine, a group communication system.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-crm"><glossterm>CRM (cluster resource manager)</glossterm>
    <glossdef>
    <para>
      The management entity responsible for coordinating all non-local interactions in a High Availability cluster.
      SUSE Linux Enterprise High Availability uses <xref linkend="gloss-pacemaker"/> as the CRM. It interacts with several components:
      local executors on its own node and on the other nodes, non-local CRMs, administrative
      commands, the fencing functionality, and the membership layer.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm><systemitem>crmsh</systemitem> (CRM Shell)</glossterm>
    <glossdef>
    <para>
      The command-line utility <emphasis><systemitem>crmsh</systemitem></emphasis> manages the cluster, nodes and resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Csync2</glossterm>
    <glossdef>
    <para>
      A synchronization tool for replicating configuration files across all nodes in the cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-dc"><glossterm>DC (designated coordinator)</glossterm>
    <glossdef>
    <para>
      The <systemitem>pacemaker-controld</systemitem> daemon is the cluster controller, which
      coordinates all actions. This daemon has an instance on each cluster node, but only one
      instance is elected to act as the DC. The DC is elected when the cluster services start,
      or if the current DC fails or leaves the cluster. The DC decides whether a cluster-wide
      change must be performed, such as fencing a node or moving resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>disaster</glossterm>
    <glossdef>
    <para>
      An unexpected interruption of critical infrastructure caused by nature, humans, hardware
      failure, or software bugs.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-dis-rec"><glossterm>disaster recovery</glossterm>
    <glossdef>
    <para>
      The process by which a function is restored to the normal, steady state after a disaster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Disaster Recovery Plan</glossterm>
    <glossdef>
    <para>
      A strategy to recover from a disaster with the minimum impact on IT infrastructure.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>DLM (Distributed Lock Manager)</glossterm>
    <glossdef>
    <para>
      DLM coordinates accesses to shared resources in a cluster, for example, managing file
      locking in clustered file systems to increase performance and availability.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-drbd"><glossterm>DRBD</glossterm>
    <glossdef>
    <para>
      <trademark class="registered">DRBD</trademark> is a block device designed for building
      High Availability clusters. It replicates data on a primary device to secondary devices in a way that
      ensures all copies of the data remain identical.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>existing cluster</glossterm>
    <glossdef>
    <para>
      The term <emphasis>existing cluster</emphasis> is used to refer to any cluster that consists
      of at least one node. An existing cluster has a basic <xref linkend="gloss-corosync"/>
      configuration that defines the communication channels, but does not necessarily have resource
      configuration yet.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="glo-failover"><glossterm>failover</glossterm>
    <glossdef>
    <para>
      Occurs when a resource or node fails on one machine and the affected resources move to
      another node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>failover domain</glossterm>
    <glossdef>
    <para>
      A named subset of cluster nodes that are eligible to run a resource if a node fails.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-fencing"><glossterm>fencing</glossterm>
    <glossdef>
    <para>
      Prevents access to a shared resource by isolated or failing cluster members. There are two
      classes of fencing: <emphasis>resource-level</emphasis> fencing and <emphasis>node-level</emphasis>
      fencing. Resource-level fencing ensures exclusive access to a resource. Node-level fencing
      prevents a failed node from accessing shared resources and prevents resources from running on
      a node with an uncertain status. This is usually done by resetting or powering off the node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>GFS2</glossterm>
    <glossdef>
    <para>
      Global File System 2 (GFS2) is a shared disk file system for Linux computer clusters.
      GFS2 allows all nodes to have direct concurrent access to the same shared block storage.
      GFS2 has no disconnected operating mode, and no client or server roles. All nodes in a GFS2
      cluster function as peers. GFS2 supports up to 32 cluster nodes. Using GFS2 in a cluster
      requires hardware to allow access to the shared storage, and a lock manager to control access
      to the storage.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>group</glossterm>
    <glossdef>
    <para>
      Resource groups contain multiple resources that need to be located together, started
      sequentially and stopped in the reverse order.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>Hawk (HA Web Konsole)</glossterm>
    <glossdef>
    <para>
      A user-friendly Web-based interface for monitoring and administering a High Availability cluster from
      Linux or non-Linux machines. Hawk can be accessed from any machine that can connect to the
      cluster nodes, using a graphical Web browser.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-heuristics"><glossterm>heuristics</glossterm>
    <glossdef>
    <para>
      <xref linkend="gloss-qdevice"/> supports using a set of commands (<emphasis>heuristics</emphasis>)
      that run locally on start-up of cluster services, cluster membership change, successful
      connection to the <xref linkend="gloss-qnetd"/> server, or optionally at regular times. The
      result is used in calculations to determine which partition should have <xref linkend="gloss-quorum"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>knet (kronosnet)</glossterm>
    <glossdef>
    <para>
      A network abstraction layer supporting redundancy, security, fault tolerance, and fast
      fail-over of network links. In SUSE Linux Enterprise High Availability 16, <emphasis>knet</emphasis> is the default transport
      protocol for the <xref linkend="gloss-corosync"/> communication channels.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>local cluster</glossterm>
    <glossdef>
    <para>
      A single cluster in one location (for example, all nodes are located in one data center).
      Network latency is minimal. Storage is typically accessed synchronously by all nodes.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>local executor</glossterm>
    <glossdef>
    <para>
      The local executor is located between <xref linkend="gloss-pacemaker"/> and the resources
      on each node. Through the <systemitem class="daemon">pacemaker-execd</systemitem> daemon,
      Pacemaker can start, stop and monitor resources.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>location</glossterm>
    <glossdef>
    <para>
      In the context of a whole cluster, <emphasis>location</emphasis> can refer to the physical
      location of nodes (for example, all nodes might be located in the same data center). In the
      context of a <xref linkend="gloss-loc-con"/>, <emphasis>location</emphasis> refers to the
      nodes on which a resource can or cannot run.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-loc-con"><glossterm>location constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that defines the nodes on which a resource can
      or cannot run.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>meta attributes (resource options)</glossterm>
    <glossdef>
    <para>
      Parameters that tell the <xref linkend="gloss-crm"/> how to treat a specific
      <xref linkend="gloss-resource"/>. For example, you might define a resource's priority
      or target role.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>metro cluster</glossterm>
    <glossdef>
    <para>
      A single cluster that can stretch over multiple buildings or data centers, with all sites
      connected by Fibre Channel. Network latency is usually low. Storage is frequently replicated
      using mirroring or synchronous replication.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>network device bonding</glossterm>
    <glossdef>
    <para>
      Network device bonding combines two or more network interfaces into a single bonded device to
      increase bandwidth and/or provide redundancy. When using <xref linkend="gloss-corosync"/>, the
      bonded device is not managed by the cluster software. Therefore, the bonded device must be
      configured on every cluster node that might need to access it.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>node</glossterm>
    <glossdef>
    <para>
      Any server (physical or virtual) that is a member of a cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-ord-con"><glossterm>order constraint</glossterm>
    <glossdef>
    <para>
      A type of <xref linkend="gloss-resource-con"/> that defines the sequence of actions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-pacemaker"><glossterm>Pacemaker</glossterm>
    <glossdef>
    <para>
      Pacemaker is the <xref linkend="gloss-crm"/> in SUSE Linux Enterprise High Availability, or the <quote>brain</quote>
      that reacts to events occurring in the cluster. Events might be nodes that join or
     leave the cluster, failure of resources, or scheduled activities such as maintenance,
     for example. The <systemitem>pacemakerd</systemitem> daemon launches and monitors
     all other related daemons.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>parameters (instance attributes)</glossterm>
    <glossdef>
    <para>
      Parameters determine which instance of a service the <xref linkend="gloss-resource"/> controls.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>scheduler</glossterm>
    <glossdef>
    <para>
      The scheduler is implemented as <systemitem class="daemon">pacemaker-schedulerd</systemitem>.
      When a cluster transition is needed, <systemitem class="daemon">pacemaker-schedulerd</systemitem>
      calculates the expected next state of the cluster and determines what actions need to be
      scheduled to achieve the next state.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>primitive</glossterm>
    <glossdef>
    <para>
      A primitive resource is the most basic type of cluster resource.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-prom-clone"><glossterm>promotable clone</glossterm>
    <glossdef>
    <para>
      Promotable clones are a special type of <xref linkend="gloss-clone"/> resource that can be
      promoted. Active instances of these resources are divided into two states: promoted and
      unpromoted (also known as <quote>active and passive</quote> or <quote>primary and
      secondary</quote>).
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qdevice"><glossterm>QDevice</glossterm>
    <glossdef>
    <para>
      QDevice and <xref linkend="gloss-qnetd"/> participate in <xref linkend="gloss-quorum"/>
      decisions. The <systemitem>corosync-qdevice</systemitem> daemon runs on each cluster node and
      communicates with QNetd to provide a configurable number of votes, allowing a cluster to
      sustain more node failures than the standard quorum rules allow.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-qnetd"><glossterm>QNetd</glossterm>
    <glossdef>
    <para>
      QNetd is an <xref linkend="gloss-arbitrator"/> that runs outside the cluster.
      The <systemitem>corosync-qnetd</systemitem> daemon provides a vote to the
      <systemitem>corosync-qdevice</systemitem> daemon on each node to help it participate
      in quorum decisions.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-quorum"><glossterm>quorum</glossterm>
    <glossdef>
    <para>
      A <xref linkend="gloss-clus-part"/> is defined to have quorum (be <emphasis>quorate</emphasis>)
      if it has the majority of nodes (or <quote>votes</quote>). Quorum distinguishes exactly one
      partition. This is part of the algorithm to prevent several disconnected partitions or nodes
      (<quote>split brain</quote>) from proceeding and causing data and service corruption.
      Quorum is a prerequisite for fencing, which then ensures that quorum is unique.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>RA (resource agent)</glossterm>
    <glossdef>
    <para>
      A script acting as a proxy to manage a <xref linkend="gloss-resource"/> (for example, to
      start, stop or monitor a resource). SUSE Linux Enterprise High Availability supports different kinds of resource agents.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>ReaR (Relax and Recover)</glossterm>
    <glossdef>
    <para>
      An administrator tool set for creating <xref linkend="gloss-dis-rec"/> images.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource"><glossterm>resource</glossterm>
    <glossdef>
    <para>
      Any type of service or application that is known to <xref linkend="gloss-pacemaker"/>, for
      example, an IP address, a file system, or a database. The term <emphasis>resource</emphasis>
      is also used for <xref linkend="gloss-drbd"/>, where it names a set of block devices that use
      a common connection for replication.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-resource-con"><glossterm>resource constraint</glossterm>
    <glossdef>
    <para>
      Resource constraints specify which cluster nodes resources can run on, what order resources
      load in, and what other resources a specific resource is dependent on.
    </para>
    <para>
      See also <xref linkend="gloss-col-con"/>, <xref linkend="gloss-loc-con"/> and
      <xref linkend="gloss-ord-con"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource set</glossterm>
    <glossdef>
    <para>
      As an alternative format for defining location, colocation or order constraints, you can use
      <emphasis>resource sets</emphasis>, where primitives are grouped together in one set. When
      creating a constraint, you can specify multiple resources for the constraint to apply to.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>resource template</glossterm>
    <glossdef>
    <para>
      To help create many resources with similar configurations, you can define a resource template.
      After being defined, it can be referenced in primitives or in certain types of constraints.
      If a template is referenced in a primitive, the primitive inherits all operations, instance
      attributes (parameters), meta attributes and utilization attributes defined in the template.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-sbd"><glossterm>SBD (STONITH Block Device)</glossterm>
    <glossdef>
    <para>
      SBD provides a node <xref linkend="gloss-fencing"/> mechanism through the exchange of
      messages via shared block storage. Alternatively, it can be used in diskless mode. In either
      case, it needs a hardware or software <xref linkend="gloss-watchdog"/> on each node to ensure
      that misbehaving nodes are really stopped.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-splitbrain"><glossterm>split brain</glossterm>
    <glossdef>
    <para>
      A scenario in which the cluster nodes are divided into two or more groups that do not know
      about each other (either through a software or hardware failure). <xref linkend="gloss-stonith"/>
      prevents a split-brain scenario from badly affecting the entire cluster. Also known as a
      <emphasis>partitioned cluster</emphasis> scenario.
    </para>
    <para>
      The term <emphasis>split brain</emphasis> is also used in <xref linkend="gloss-drbd"/> but
      means that the nodes contain different data.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>SPOF (single point of failure)</glossterm>
    <glossdef>
    <para>
      Any component of a cluster that, if it fails, triggers the failure of the entire cluster.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-stonith"><glossterm>STONITH</glossterm>
    <glossdef>
    <para>
      Another term for the
      <xref linkend="gloss-fencing"/> mechanism that shuts down a misbehaving node to prevent it
      from causing trouble in a cluster. In a <xref linkend="gloss-pacemaker"/> cluster, node
      fencing is managed by the fencing subsystem <systemitem>pacemaker-fenced</systemitem>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>switchover</glossterm>
    <glossdef>
    <para>
      The planned moving of resources to other nodes in a cluster. See also <xref linkend="glo-failover"/>.
    </para>
    </glossdef>
  </glossentry>
  <glossentry><glossterm>utilization</glossterm>
    <glossdef>
    <para>
      Tells the CRM what capacity a certain <xref linkend="gloss-resource"/> requires from a node.
    </para>
    </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-watchdog"><glossterm>watchdog</glossterm>
    <glossdef>
    <para>
      <xref linkend="gloss-sbd"/> needs a watchdog on each node to ensure that misbehaving nodes are
      really stopped. SBD <quote>feeds</quote> the watchdog by regularly writing a service pulse
      to it. If SBD stops feeding the watchdog, the hardware enforces a system restart. This
      protects against failures of the SBD process itself, such as becoming stuck on an I/O error.
    </para>
    </glossdef>
  </glossentry>
</glossary>
</article>
