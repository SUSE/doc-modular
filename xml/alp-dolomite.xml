<?xml version="1.0"?>
<book xmlns="http://docbook.org/ns/docbook" xml:base="alp-dolomite.xml" version="5.0" xml:lang="en">
  <info>
      <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">The <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> Guide</title>
      <revhistory xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <title>Changelog</title>
        <revision><date>2023-06-13</date>
          <revdescription>
            <para>
              Added an article about autostarting Podman containers.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-06-08</date>
          <revdescription>
            <para>
              Added a section on directories excluded from snapshots.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-06-05</date>
          <revdescription>
            <para>
              Added an article about running the Keylime remote attestation
              workload.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-04-28</date>
          <revdescription>
            <para>
              Added an article about running the Kea DHCP server workload.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-04-12</date>
          <revdescription>
            <para>
              Added an article about automated installation using the
              Agama installer.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-04-05</date>
          <revdescription>
            <para>
              Added Ansible examples.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-04-04</date>
          <revdescription>
            <para>
              Added IBM Z deployment.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-03-30</date>
          <revdescription>
            <para>
              Added an article about running the Ansible workload.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-03-28</date>
          <revdescription>
            <para>
              Updated the Agama installer section to include product
              selection and extended iSCSI configuration.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-03-28</date>
          <revdescription>
            <para>
              Added instructions to deploy the NeuVector workload.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-03-23</date>
          <revdescription>
            <para>
              Added instructions to deploy secure virtual machines.
            </para>
          </revdescription>
        </revision>
        <revision><date>2023-03-21</date>
          <revdescription>
            <para>
              Added an article about deploying customized virtual machines
              using the <command>virt-scenario</command> tool.
            </para>
          </revdescription>
        </revision>
      </revhistory>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="updated" content="2023-05-03" its:translate="no"/>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="bugtracker" its:translate="no"><phrase role="url">https://bugzilla.suse.com/enter_bug.cgi</phrase><phrase role="component">Documentation</phrase><phrase role="product"><phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase><phrase role="assignee">tbazant@suse.com</phrase>
      </meta>
      
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="architecture" content="x86;aarch64" its:translate="no"/>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="productname" its:translate="no"><productname version="0.1"><phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase></productname>
      </meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="title" its:translate="no"><phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase></meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="description" its:translate="yes">A minimal OS that runs
      containerized workloads.</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="social-descr" its:translate="yes">A minimal immutable
      self-healing OS</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="category" content="systems-management" its:translate="no"/>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:component>Documentation</dm:component>
          <dm:product>Dolomite</dm:product>
          <dm:assignee>tbazant@suse.com</dm:assignee>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
      <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <para>
          This guide introduces the <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>
          (<phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>)—its deployment, system management and
          software installation as well as running of containerized workloads.
          To enhance this <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> documentation, find its sources at
          <link xlink:href="https://github.com/SUSE/doc-modular/edit/main/xml/"/>.
        </para>
        <variablelist>
          <varlistentry>
            <term>TOPIC</term>
            <listitem>
              <para>
                <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is a minimal operating system that allows
                customers to run their workloads in a containerized or
                virtualized form. Typically, <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is deployed on
                bare-metal servers and runs data center-oriented workloads.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>INTENTION</term>
            <listitem>
              <para>
                This guide introduces an overview of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>. It
                describes steps required to deploy and administer
                <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> as well as install and manage SUSE
                workloads.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>EFFORT</term>
            <listitem>
              <para>
                Although <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is a complex platform, its basic
                deployment takes less than 30 minutes. More advanced topics,
                such as using the <command>transactional-update</command> command, or installing and
                configuring containerized SUSE workloads, take considerably
                more time.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>REQUIREMENTS</term>
            <listitem>
              <para>
                To understand the concepts and perform tasks described in this
                guide, you need to have good knowledge and practice with the
                SUSE Linux operating system.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </abstract>
    </info>
  <chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">General description</title>
  </info>
  
  <section xml:id="what-is-alp">
    <title>What is <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>?</title>
    <para>
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is a minimal operating system that allows customers to
      run their workloads in a containerized or virtualized form. Typically,
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is deployed on bare-metal servers and runs data
      center-oriented workloads. <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is best described by the following
      characteristics:
    </para>
    <variablelist>
      <varlistentry>
        <term>Minimalistic</term>
        <listitem>
          <para>
            <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> provides the bare minimum to run workloads and
            services as containers or virtual machines. Its default software
            collection is carefully preselected to include only packages
            required to support its mission.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Secure</term>
        <listitem>
          <para>
            <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is an immutable operating system with a
            read-only file system. Changes to the file system are performed in
            <emphasis>transactions</emphasis> and can easily be rolled back.
            Moreover, the SELinux access control makes it a truly secure
            platform.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Container-ready</term>
        <listitem>
          <para>
            In <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>, containerized workloads replace traditional
            applications. They contain all software dependencies required to
            run a specific application or tool. <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> uses
            Podman as the default container engine. Podman makes it easy to
            find, run, build, share and deploy containerized workloads.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
</chapter>
  <chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp-deployment"><info>
        <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Deployment</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <section xml:id="alp-pre-deployment-considerations">
    <title>Considerations before deployment</title>
    <para>
      This section introduces tips and suggestions that need to be considered
      before or during the deployment.
    </para>
    <section xml:id="alp-pre-deployment-considerations-root-ssh-login">
      <title><systemitem class="username">root</systemitem> SSH login</title>
      <para>
        By default, <systemitem class="username">root</systemitem> SSH login in <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is permitted
        only by using the SSH key. <systemitem class="username">root</systemitem> SSH login with password is
        prohibited. To enable <systemitem class="username">root</systemitem> SSH login with password, you have
        several options:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            <emphasis role="bold">Create an unprivileged user.</emphasis>
            Depending on the installation method, you can either create a new
            user in the Agama installer, or use
            Combustion/Ignition tools. Refer to
            <xref linkend="task-deploy-alp-installer"/> and
            <xref linkend="concept-configure-ignition"/> for more
            details.
          </para>
          <tip>
            <para>
              Creating an unprivileged user during system installation is
              useful for logging in to the Cockpit Web interface. Find more
              details in
              <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/alp/dolomite/html/cockpit-alp-dolomite/index.html"/>.
            </para>
          </tip>
        </listitem>
        <listitem>
          <para>
            <emphasis role="bold">Install the
            <package>openssh-server-config-rootlogin</package>
            package.</emphasis> Use Combustion/Ignition tools to install
            the package during the deployment process.
          </para>
          <tip>
            <para>
              See <xref linkend="alp-post-deploy-enable-root-ssh-login"/> for
              details on installing
              <package>openssh-server-config-rootlogin</package> manually after
              the system is deployed.
            </para>
          </tip>
        </listitem>
      </itemizedlist>
    </section>
  </section>
  <section xml:id="alp-deployment-methods">
    <title>Deployment methods</title>
    <para>
      <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> is distributed either as a disk image of the
      Agama installer, or as a pre-built raw disk image.
    </para>
    <section xml:id="alp-deployment-installer">
      <title>The Agama installer</title>
      <para>
        While the Agama installer handles both bare-metal and
        virtualized/cloud deployments, it is a preferred method for bare-metal
        deployments. <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment using the Agama
        installer is similar to a traditional operating system setup. After
        booting the Agama installer image, the installer uses a
        graphical user-friendly interface to walk you through the system
        configuration and deployment.
      </para>
      <note>
        <para>
          The installer live images do not contain <package>linuxrc</package>,
          <package>wicked</package> and
          <package>installation-images</package> packages. Therefore, the
          device activation and configuration need adjusting, and there is no
          interactive menu for configuring certain parameters of the
          installation.
        </para>
      </note>
    </section>
    <section xml:id="alp-deployment-raw-image">
      <title>Raw disk image</title>
      <para>
        This method handles both bare-metal and virtualized/cloud deployment.
        It is different from the installer-based deployment in that you do not
        boot an installer but the actual <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> image itself. On
        first boot, you can configure basic system options using an
        <emphasis>ncurses</emphasis> user interface. Using a raw disk image,
        you can fine-tune the deployment setup with Combustion and Ignition
        tools.
      </para>
    </section>
  </section>
  <section xml:id="requirements-deploy-alp-raw-image">
    <title>Hardware requirements</title>
    <para>
      The minimum supported hardware requirements for deploying
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> follow:
    </para>
    <variablelist>
      <varlistentry>
        <term>BIOS</term>
        <listitem>
          <para>
            Installing <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is supported on hosts with UEFI BIOS
            only. Hosts with the legacy BIOS are not supported.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>CPU</term>
        <listitem>
          <para>
            AMD64/Intel 64, AArch64 and IBM Z CPU architectures are supported.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Maximum number of CPUs</term>
        <listitem>
          <para>
            The maximum number of CPUs supported by software design is 8192.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Memory</term>
        <listitem>
          <para>
            <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> requires at least 1 GB RAM. Bear in mind
            that this is a minimal value for the operating system, the actual
            memory size depends on the workload.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Hard disk</term>
        <listitem>
          <para>
            The minimum hard disk space is 12 GB, while the recommended
            value is 20 GB of hard disk space. Adjust the value according
            to the workloads of your containers.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-prepare-alp-vm"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Preparing the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> virtual machine</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to configure a new virtual machine suitable
        for the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment by using the Virtual Machine Manager.
      </para>
    </abstract>
  </info>
  
  <section xml:id="requirements-prepare-alp-vm">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          A VM Host Server with KVM hypervisor.
        </para>
      </listitem>
      <listitem>
        <para>
          Download either the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> raw disk or the
          Agama installer image from <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://susealp.io/downloads/"/> on the
          VM Host Server where you intend to run virtualized <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>.
        </para>
        <note>
          <para>
            For the raw disk image deployment, there are two types of images,
            depending on whether you intend to run <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on an
            encrypted disk or an unencrypted disk.
          </para>
        </note>
      </listitem>
    </itemizedlist>
    <important>
      <title>Encrypted image does not expand to the full disk capacity</title>
      <para>
        As of now, the encrypted raw disk image does not expand to the full
        disk capacity automatically. As a workaround, the following steps are
        required:
      </para>
      <procedure>
        <step>
          <para>
            Use the <command>qemu-img</command> command to increase the disk
            image to the desired size.
          </para>
        </step>
        <step>
          <para>
            Set up the virtual machine and boot it. When the JeOS Firstboot
            wizard asks you which method to use for encryption, select
            <guimenu>passphrase</guimenu>.
          </para>
        </step>
        <step>
          <para>
            When the system is ready, use the <command>parted</command> command
            to resize the partition where the LUKS device resides (for example,
            partition number 3) to the desired size.
          </para>
        </step>
        <step>
          <para>
            Run the <command>cryptsetup resize luks</command> command. When
            asked, enter the passphrase to resize the encrypted device.
          </para>
        </step>
        <step>
          <para>
            Run the <command>transactional-update shell</command> command to
            open a read-write shell in the current disk snapshot. Then resize
            the Btrfs file system to the desired size, for example:
          </para>
<screen><prompt role="root"># </prompt>btrfs fi resize max /</screen>
        </step>
        <step>
          <para>
            Leave the shell with <command>exit</command> and reboot the system
            with <command>reboot</command>.
          </para>
        </step>
      </procedure>
    </important>
  </section>
  <section xml:id="requirements-prepare-alp-vm-">
    <title>Configuring the virtual machine for <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment</title>
    <procedure>
      <step>
        <para>
          Start Virtual Machine Manager and select
          <menuchoice><guimenu>File</guimenu><guimenu>New Virtual
          Machine</guimenu></menuchoice>.
        </para>
        <substeps>
          <step>
            <para>
              For deployment using the Agama installer, select <guimenu>Local
              install media</guimenu>.
            </para>
          </step>
          <step>
            <para>
              For the raw disk deployment, select <guimenu>Import existing disk
              image</guimenu>.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          Confirm with <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the path to the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> disk image that you
          previously downloaded and the type of linux OS you are deploying, for
          example, <literal>Generic Linux 2020</literal>. Confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the amount of memory and number of processors that you want
          to assign to the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> virtual machine and confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          For deployment using the Agama installer, enable storage for the
          virtual machine and specify the size of the disk image.
        </para>
      </step>
      <step>
        <para>
          Specify the name for the virtual machine and the network to be used.
        </para>
      </step>
      <step>
        <para>
          If you are deploying an encrypted <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> image, perform
          these additional steps:
        </para>
        <substeps>
          <step>
            <para>
              Enable <guimenu>Customize configuration before install</guimenu>
              and confirm with <guimenu>Finish</guimenu>.
            </para>
          </step>
          <step>
            <para>
              Click <guimenu>Overview</guimenu> from the left menu and change
              the boot method from BIOS to UEFI for secure boot. Confirm with
              <guimenu>Apply</guimenu>.
            </para>
            <figure>
              <title>Set UEFI firmware for the encrypted <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> image</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Set UEFI firmware for the encrypted <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> image</phrase>
                </textobject>
              </mediaobject>
            </figure>
          </step>
          <step>
            <para>
              Add a Trusted Platform Module (TPM) device. Click <guimenu>Add
              Hardware</guimenu>, select <guimenu>TPM</guimenu> from the left
              menu, and select the <guimenu>Emulated</guimenu> type.
            </para>
            <figure>
              <title>Add an emulated TPM device</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Add an emulated
                TPM device</phrase>
                </textobject>
              </mediaobject>
            </figure>
            <para>
              Confirm with <guimenu>Finish</guimenu> and start the
              <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment by clicking <guimenu>Begin
              Installation</guimenu> from the top menu.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <substeps>
          <step>
            <para>
              For the raw disk image deployment, to deploy <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>
              with only minimal setup options, confirm with
              <guimenu>Finish</guimenu>. The disk image will be booted and
              JeOS Firstboot will take care of the deployment. Refer to
              <xref linkend="deploy-alp-jeos-firstboot"/> for next
              steps.
            </para>
            <tip>
              <para>
                You can detail the deployment setup by using the Ignition or
                Combustion tools. For more details, refer to
                <xref linkend="concept-configure-ignition"/> and
                <xref linkend="concept-configure-combustion"/>.
              </para>
            </tip>
          </step>
          <step>
            <para>
              To continue the deployment by using the Agama installer, confirm
              with <guimenu>finish</guimenu> and continue with
              <xref linkend="task-deploy-alp-installer"/>.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp-installer"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Interactive deployment using the Agama installer</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to deploy <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> interactively
        using the Agama installer. For an automated deployment, refer
        to <xref linkend="alp-installer-automated-deployment"/>.
      </para>
    </abstract></info>
  
  <procedure>
    <step>
      <para>
        Download the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> Agama installer image from
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://susealp.io/downloads/"/>.
      </para>
    </step>
    <step>
      <para>
        If you are deploying <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> as a VM Guest, you need to
        first prepare the virtual machine. To do this, follow the steps in
        <xref linkend="task-prepare-alp-vm"/>.
      </para>
    </step>
    <step>
      <para>
        Boot the Agama installer image and select
        <guimenu>agama-live</guimenu> from the boot menu. A screen with the
        main installation menu is displayed.
      </para>
      <figure>
        <title>Agama installer main menu</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-dolomite-installer-welcome.png" width="90%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-dolomite-installer-welcome.png" width="90%"/>
          </imageobject>
          <textobject role="description"><phrase>The Agama installer main menu</phrase>
          </textobject>
        </mediaobject>
      </figure>
    </step>
    <step>
      <para>
        Click <guimenu>Localization</guimenu> and select your preferred
        language from the drop-down list.
      </para>
    </step>
    <step>
      <para>
        Configure <guimenu>Network</guimenu> settings by selecting
        <guimenu>Edit</guimenu> from the menu on the right side of the default
        wired connection. You can, for example, change the networking mode to
        <guimenu>Manual</guimenu>, add IP addresses and related prefixes or
        netmasks, or add gateway and DNS servers.
      </para>
      <figure>
        <title>Configuring the network</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-installer-network-wired.png" width="50%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-installer-network-wired.png" width="50%"/>
          </imageobject>
          <textobject role="description"><phrase>Configuring the network</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        By clicking <guimenu>Connect to a Wi-Fi network</guimenu> you can
        utilize your local wireless network.
      </para>
    </step>
    <step>
      <para>
        Configure <guimenu>Storage</guimenu>.
      </para>
      <figure>
        <title>Configuring storage</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-installer-storage.png" width="90%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-installer-storage.png" width="90%"/>
          </imageobject>
          <textobject role="description"><phrase>Configuring <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> storage</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        Select the device where <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> should be installed.
        Optionally, enable LVM or disk encryption and enter an encryption
        password.
      </para>
      <tip>
        <para>
          If you enable disk encryption, you may be asked for a decryption
          password on each reboot. Because the GRUB 2 boot loader does not
          enable switching keyboard layouts, select a password made of
          alphanumeric characters and be aware of national keyboard layout
          differences. For extended post-deployment information about disk
          encryption, refer to
          <xref linkend="alp-post-deploy-full-disk-encryption"/>.
        </para>
      </tip>
      <para>
        By default, the partitioning scheme includes one root file system. To
        change its size, click the corresponding three dots on the right side
        and select <guimenu>Edit</guimenu>.
      </para>
      <figure>
        <title>Editing file system properties</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-agama-change-fs.png" width="50%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-agama-change-fs.png" width="50%"/>
          </imageobject>
          <textobject role="description"><phrase>Editing file system properties</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        To create additional file systems, click <guimenu>Actions</guimenu> on
        the <guimenu>Settings</guimenu> page and select <guimenu>Add file
        system</guimenu>.
      </para>
      <para>
        To configure iSCSI targets or DASD disks (for IBM Z) for the
        installation, click the down arrow on the right of the top bar and
        select <guimenu>iSCSI</guimenu> or <guimenu>DASD</guimenu>.
      </para>
      <figure>
        <title>Discover iSCSI targets</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-installer-iscsi-targets.png" width="75%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-installer-iscsi-targets.png" width="75%"/>
          </imageobject>
          <textobject role="description"><phrase>Discover iSCSI targets</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        Click <guimenu>Discover</guimenu> to add new iSCSI targets.
      </para>
      <figure>
        <title>Adding a new iSCSI target</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-installer-add-iscsi-target.png" width="50%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-installer-add-iscsi-target.png" width="50%"/>
          </imageobject>
          <textobject role="description"><phrase>Adding a new iSCSI target</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <figure>
        <title>DASD storage contextual menu (IBM Z)</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-installer-zseries-dasd.png" width="75%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-installer-zseries-dasd.png" width="75%"/>
          </imageobject>
          <textobject role="description"><phrase>DASD storage contextual menu (IBM Z)</phrase>
          </textobject>
        </mediaobject>
      </figure>
    </step>
    <step>
      <para>
        In the <guimenu>Users</guimenu> section, specify a <systemitem class="username">root</systemitem> password,
        upload a <guimenu>Root SSH public key</guimenu>, or create an
        additional user account and optionally enable auto login for it.
      </para>
      <figure>
        <title>Adding new users</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-agama-new-user.png" width="50%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-agama-new-user.png" width="50%"/>
          </imageobject>
          <textobject role="description"><phrase>User management</phrase>
          </textobject>
        </mediaobject>
      </figure>
    </step>
    <step>
      <para>
        To begin the installation, click <guimenu>Install</guimenu> and confirm
        with <guimenu>Continue</guimenu>.
      </para>
      <para>
        After the installation is finished, click <guimenu>Reboot</guimenu> and
        select <guimenu><phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></guimenu> from the boot menu after
        reboot.
      </para>
    </step>
  </procedure>
</section><section role="concept" xml:lang="en" version="5.2" xml:id="alp-installer-automated-deployment"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Automated deployment using the Agama installer</title>
  </info>
  
  <section xml:id="concept-what-is-alp-installer-automated-deployment">
    <title>What is an automated deployment?</title>
    <para>
      As an addition to the interactive deployment described in
      <xref linkend="task-deploy-alp-installer"/>, the Agama
      installer supports an unattended automated deployment.
    </para>
  </section>
  <section xml:id="concept-alp-installer-automated-deployment-how-it-works">
    <title>How does an automated deployment work?</title>
    <para>
      The Agama installer supports the following types of unattended
      deployment:
    </para>
    <variablelist>
      <varlistentry>
        <term>Profile-based deployment</term>
        <listitem>
          <para>
            This type of deployment uses a customized file called
            <emphasis>profile</emphasis> that includes a description of the
            system to install.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Script-based deployment</term>
        <listitem>
          <para>
            This type of deployment uses a plain shell script that enables
            custom pre-deployment workflows.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The actual automated deployment is started by passing the following
      parameter on the kernel command line during the Agama installer
      boot process:
    </para>
<screen>agama.auto=<replaceable>PROFILE_OR_SCRIPT_URL</replaceable></screen>
    <tip>
      <para>
        When booting from the Agama installer live media ISO, you
        need to edit and modify the GRUB 2 boot loader command line and specify
        the path to the profile configuration file. When booting via PXE, the
        file can be reached via HTTP, for example:
      </para>
<screen>agama.auto=http://example.net/profile1.jsonnet</screen>
    </tip>
    <important>
      <title>Use correct file suffix</title>
      <para>
        Using the correct suffix of the file name is important:
      </para>
      <variablelist>
        <varlistentry>
          <term>.jsonnet</term>
          <listitem>
            <para>
              Enables dynamic content through Jsonnet.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>.json</term>
          <listitem>
            <para>
              Assumes that the profile is just a JSON file with no dynamic
              content.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>.sh</term>
          <listitem>
            <para>
              Is interpreted as a shell script.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </important>
  </section>
  <section xml:id="concept-alp-installer-automated-deployment-benefits">
    <title>Benefits of an automated deployment</title>
    <itemizedlist>
      <listitem>
        <para>
          You can prepare the deployment setup in advance and modify it easily
          for future deployments.
        </para>
      </listitem>
      <listitem>
        <para>
          By running an unattended deployment, you can save the time that you
          would normally spend in the interactive deployment process.
        </para>
      </listitem>
      <listitem>
        <para>
          After fine-tuning the deployment profile, you can deploy multiple
          hosts at the same time to meet datacenter requirements.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section role="concept" xml:lang="en" version="5.2" xml:id="alp-installer-profile-based-automated-deployment"><info>
            <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Profile-based installation</title>
            <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
              <para>
                Select the profile-based installation if you prefer fine-tuning
                installation scenarios, using dynamic modifications, or
                validating the installation profile.
              </para>
            </abstract>
          <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <section xml:id="alp-installer-automated-deployment-profile">
    <title>Deployment profile</title>
    <para>
      A <emphasis>profile</emphasis> defines which options to use during the
      deployment process. For example, which product to install, localization
      settings, or partitioning layout. Profiles are written in
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://jsonnet.org/">Jsonnet</link>. Jsonnet is a
      superset of JSON that provides additional features, such as creating
      dynamic and more concise profiles.
    </para>
    <example>
      <title>Example profile</title>
<screen>
{
  "localization": {
    "language": "en_US"
  },
  "software": {
    "product": "ALP-Bedrock"
  },
  "storage": {
    "devices": [
      {
        "name": "/dev/sda"
      }
    ]
  },
  "user": {
    "fullName": "Jane Doe",
    "password": "123456",
    "userName": "jane.doe"
  }
}
</screen>
    </example>
  </section>
  <section xml:id="alp-installer-automated-deployment-dynamic-profiles">
    <title>Dynamic profiles</title>
    <para>
      You can adapt the deployment profile at runtime depending on the system
      where the automated deployment is running. The Agama installer injects
      the hardware information into the profile to be processed using Jsonnet.
    </para>
    <para>
      The following example profile is adapted to install <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on
      the biggest disk discovered. The hardware information provided by the
      <command>lshw</command> command is available as a
      <literal>hw.libsonnet</literal> JSON object.
    </para>
    <important>
      <para>
        Only the storage information is injected for now. You can inspect the
        available data by installing the <package>lshw</package> package and
        running the following command:
      </para>
<screen><command>lshw -json -class disk</command></screen>
    </important>
<screen>
local agama = import 'hw.libsonnet';
local findBiggestDisk(disks) =
  local sizedDisks = std.filter(function(d) std.objectHas(d, 'size'), disks);
  local sorted = std.sort(sizedDisks, function(x) x.size);
  sorted[0].logicalname;

{
  software: {
    product: 'ALP-Bedrock',
  },
  root: {
    password: 'nots3cr3t',
  },
  localization: {
    language: 'en_US',
  },
  storage: {
    devices: [
      {
        name: findBiggestDisk(agama.disks),
      },
    ],
  },
}
</screen>
  </section>
  <section xml:id="alp-installer-automated-deployment-validating-profiles">
    <title>Evaluating and validating profiles</title>
    <para>
      The Agama installer includes a command-line interface available from the
      <package>agama-cli</package> package. It handles multiple tasks, such as
      downloading, validating and evaluating profiles. For example, the
      following command checks the result of running the previous profile:
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>agama profile evaluate example-profile.jsonnet</command></screen>
    <para>
      To validate a JSON profile, run the following command:
    </para>
<screen><prompt>&gt; </prompt><command>agama profile validate my-profile.json</command></screen>
    <important>
      <para>
        You can only validate JSON profiles. Jsonnet profiles must be evaluated
        first.
      </para>
    </important>
  </section>
  <section xml:id="alp-installer-automated-deployment-generating-profiles">
    <title>Generating profiles</title>
    <para>
      Although writing deployment profiles manually in JSON format is easy, the
      Agama installer can export the current setup as a profile:
    </para>
    <procedure>
      <step>
        <para>
          Boot the Agama image.
        </para>
      </step>
      <step>
        <para>
          Configure all deployment options in the Agama user
          interface as described in
          <xref linkend="task-deploy-alp-installer"/>.
        </para>
      </step>
      <step>
        <para>
          Open the Agama built-in terminal and enter the following
          command to dump the current deployment profile:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>agama config show</command></screen>
      </step>
    </procedure>
  </section>
<section role="reference" xml:lang="en" version="5.2" xml:id="reference-alp-installer-profile-based-automated-deployment-supported-options"><info>
              <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Supported options</title>
              <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
                <para>
                  The following options are supported when creating
                  installation profiles:
                </para>
              </abstract>
            <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <tip>
    <para>
      Although profiled-based configuration files use JSON syntax, this
      reference uses <replaceable>PARENT.CHILD</replaceable> expressions for
      convenience purposes. Therefore,
    </para>
<screen>localization.language</screen>
    <para>
      translates to
    </para>
<screen>
"localization": {
  "language": 
}
</screen>
    <para>
      Refer to <xref linkend="alp-installer-automated-deployment-profile"/> for the
      description and syntax of the profile-based configuration files.
    </para>
  </tip>
  <variablelist>
    <varlistentry>
      <term>software</term>
      <listitem>
        <para>
          Specifies which software or product to install.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>software.product</term>
      <listitem>
        <para>
          Mandatory product identifier. For example, ALP-Dolomite.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>localization</term>
      <listitem>
        <para>
          Localization settings
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>localization.language</term>
      <listitem>
        <para>
          System language ID. For example, <literal>en_US</literal>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>storage</term>
      <listitem>
        <para>
          Storage settings
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>storage.devices</term>
      <listitem>
        <para>
          Array of devices where <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> will be installed. For
          example, <literal>["/dev/sda"]</literal>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>user</term>
      <listitem>
        <para>
          First user account settings
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>user.fullName</term>
      <listitem>
        <para>
          Full user name
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>user.userName</term>
      <listitem>
        <para>
          User login name
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>user.password</term>
      <listitem>
        <para>
          User password
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>root</term>
      <listitem>
        <para>
          Authentication of the <systemitem class="username">root</systemitem>
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>root.password</term>
      <listitem>
        <para>
          <systemitem class="username">root</systemitem> password
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>root.sshPublicKey</term>
      <listitem>
        <para>
          <systemitem class="username">root</systemitem> SSH public key
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section></section><section role="concept" xml:lang="en" version="5.2" xml:id="alp-installer-script-based-automated-deployment"><info>
            <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Script-based installation</title>
            <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
              <para>
                Select the script-based installation to obtain full control of
                the installation process.
              </para>
            </abstract>
          <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    The script that you specify at the boot command line
  </para>
<screen>agama.auto=<replaceable>PROFILE_OR_SCRIPT_URL</replaceable></screen>
  <para>
    is a Linux shell script and you can include any commands available on the
    deployment media. To specify <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment option, use the
    <command>/usr/bin/agama</command> command.
  </para>
  <para>
    The following is a minimal working example to install <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>:
  </para>
<screen>
set -ex

/usr/bin/agama config set software.product=ALP-Dolomite
/usr/bin/agama config set user.userName=<replaceable>EXAMPLE_USER</replaceable> user.password=<replaceable>PASSWORD</replaceable>
/usr/bin/agama install
</screen>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp-zseries"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Deploying <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on an IBM Z host</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to deploy <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on an IBM Z
        host using the Agama installer.
      </para>
    </abstract>
  </info>
  
  <section xml:id="requirements-deploy-alp-zseries">
    <title>Requirements</title>
    <para>
      Before installing <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on IBM Z, you need to fulfill
      the following requirements:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          The deployment on an IBM Z architecture is specific and requires
          that you review its basic concepts. Although <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is
          different from SUSE Linux Enterprise systems, study the information provided by
          <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-zseries.html#sec-zseries-prep"/>
          before starting the deployment because most of it is valid for the
          <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment as well.
        </para>
      </listitem>
      <listitem>
        <para>
          Refer to
          <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-zseries.html#sec-zseries-requirements"/>
          for generic system requirements.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-zseries-prepare-iso-image">
    <title>Download and prepare the installation image</title>
    <para>
      Download the Agama installer image for the IBM Z
      architecture from <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://susealp.io/downloads/"/>
    </para>
    <para>
      Prepare the ISO image to be served by the FTP server. Extract its content
      so that you can modify it:
    </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> mv agama-live.s390x-1.0-ALP-M2.iso /srv/ftp/agama.iso
<prompt>&gt; </prompt><command>sudo</command> cd /srv/ftp/
<prompt>&gt; </prompt><command>sudo</command> isoinfo -R -X -i agama.iso
<prompt>&gt; </prompt><command>sudo</command> chmod a+u boot s390x/initrd
</screen>
    <tip>
      <para>
        Setting up an installation server (NFS or FTP) is out of the scope of
        this article. For further information, refer to
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-zseries.html#sec-zseries-prep"/>.
      </para>
    </tip>
  </section>
  <section xml:id="alp-zseries-example-deployment">
    <title>Example deployment procedure</title>
    <para>
      The following procedure describes steps to deploy <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on
      an IBM Z machine via a z/VM console.
    </para>
    <procedure>
      <step>
        <para>
          Install the <package>x3270</package> package that provides the
          3270-type terminal emulator.
        </para>
      </step>
      <step>
        <para>
          Connect to the LPAR server using the x3270 console. When asked,
          provide your login credentials.
        </para>
        <figure>
          <title>Login prompt inside the z/VM console</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-zseries-zvm.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-zseries-zvm.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Login prompt inside the z/VM console</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Enter the Conversational Monitoring System (CMS):
        </para>
<screen>#CP IPL CMS</screen>
      </step>
      <step>
        <para>
          Link the TCPMAINT disk to have the FTP command available:
        </para>
<screen>VMLINK TCPMAINT 592</screen>
      </step>
      <step>
        <para>
          Connect to the FTP server and download the required files for IPLing
          installation. In our case, the anonymous user is allowed:
        </para>
<screen>FTP example.org (addr ipv4 
anonymous 

cd boot/s390x
locsite fix 80
ascii
get parmfile sles.parmfile.a (repl
get sles.exec sles.exec.a (repl
locsite fix 80
binary
get linux sles.linux.a (repl 
get initrd sles.initrd.a (repl 
quit</screen>
        <note>
          <para>
            The <command>locsite fix 80</command> command sets the VM file
            format to a fixed length of 80. This file format is necessary for
            <emphasis>punching</emphasis> the binary files to a virtual machine
            reader.
          </para>
        </note>
      </step>
      <step>
        <para>
          Optionally, you can use the <command>FILELIST</command> command to
          list the files and edit the <emphasis role="bold">parmfile</emphasis>
          with XEDIT. Our example parmfile has the following content:
        </para>
<screen>cio_ignore=all,!condev,!0.0.0160<co xml:id="alp-zseries-cioignore"/>
rd.zdev=qeth,0.0.0800:0.0.0801:0.0.0802,layer2=1,portno=0<co xml:id="alp-zseries-rdzdev"/>
ip=192.168.0.111::192.168.0.1:24:zvmtest.example.org:enc800:none<co xml:id="alp-zseries-ip"/>
nameserver=192.168.0.1
root=live:ftp://example.org/agama.iso<co xml:id="alp-zseries-live"/></screen>
        <calloutlist>
          <callout arearefs="alp-zseries-cioignore">
            <para>
              Although the <option>cio_ignore</option> parameter is optional,
              it is used to list only the relevant installation devices and
              accept the devices that are used for the installation.
            </para>
          </callout>
          <callout arearefs="alp-zseries-rdzdev alp-zseries-ip">
            <para>
              Because we do not have an interactive dialog for enabling and
              configuring our network device, we need to provide the settings
              through the kernel command line. The <option>rd.zdev</option>
              option activates the <literal>qeth</literal> device and the
              <option>ip</option> option configures network settings for the
              <literal>enc800</literal> Linux network interface.
            </para>
          </callout>
          <callout arearefs="alp-zseries-live">
            <para>
              The system boots from a live image retrieved from the specified
              URL. Our example uses FTP protocol, but it can be HTTP as well.
            </para>
          </callout>
        </calloutlist>
      </step>
      <step>
        <para>
          The following is the content of the <filename>sles.exec</filename>
          file:
        </para>
<screen>/* REXX LOAD EXEC FOR SUSE LINUX S/390 VM GUESTS       */
/* LOADS SUSE LINUX S/390 FILES INTO READER            */
SAY ''
SAY 'LOADING SLES FILES INTO READER...'
'CP CLOSE RDR'
'PURGE RDR ALL'
'SPOOL PUNCH * RDR'
'PUNCH SLES LINUX A (NOH'
'PUNCH SLES PARMFILE A (NOH'
'PUNCH SLES INITRD A (NOH'
'IPL 00C'</screen>
        <para>
          Boot the installation image by running the
          <filename>sles.exec</filename> REXX file:
        </para>
<screen>sles</screen>
        <figure>
          <title>Booting the installation image</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-zseries-zvm-sles.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-zseries-zvm-sles.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Booting the installation image</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          After the installation system finishes the booting process, connect
          to the machine either with the Web browser (for example,
          https://example.host.org:9090) or via SSH using the default
          credentials (user name: <literal>root</literal>, password:
          <literal>linux</literal>). The rest of the installation process is
          identical to <xref linkend="task-deploy-alp-installer"/>.
        </para>
      </step>
    </procedure>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp-raw-image"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Deployment using a raw disk image</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <section xml:id="introduction-deploy-alp-raw-image">
    <title>Introduction</title>
    <para>
      This article describes how to deploy the <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>
      (<phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>) raw disk image. It applies to <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>
      running both on encrypted and unencrypted disk.
    </para>
    <section xml:id="alp-deployment-firstboot-detection">
      <title>First boot detection</title>
      <para>
        The deployment configuration runs on the first boot only. To
        distinguish between the first and subsequent boots, the flag file
        <filename>/boot/writable/firstboot_happened</filename> is created after
        the first boot finishes. If the file is not present in the file system,
        the attribute <literal>ignition.firstboot</literal> is passed to the
        kernel command line and thus both Ignition and Combustion are
        triggered to run (in the initrd). After completing the first boot, the
        <filename>/boot/writable/firstboot_happened</filename> flag file is
        created.
      </para>
      <note>
        <title>The flag file is always created</title>
        <para>
          Even though the configuration may not be successful due to improper
          or missing configuration files, the
          <filename>/boot/writable/firstboot_happened</filename> flag file is
          created.
        </para>
      </note>
      <tip>
        <para>
          You may force the first boot configuration on subsequent boot by
          passing the <literal>ignition.firstboot</literal> attribute to the
          kernel command line or by deleting the
          <filename>/boot/writable/firstboot_happened</filename> flag file.
        </para>
      </tip>
    </section>
    <section xml:id="alp-deployment-default-partitioning">
      <title>Default partitioning</title>
      <para>
        The pre-built images are delivered with a default partitioning scheme.
        You can change it during the first boot by using Ignition or
        Combustion.
      </para>
      <important>
        <title>Btrfs is mandatory for the root file system</title>
        <para>
          If you intend to perform any changes to the default partitioning
          scheme, the root file system must be Btrfs.
        </para>
      </important>
      <para>
        Each image has the following subvolumes:
      </para>
<screen>
 /home
 /root
 /opt
 /srv
 /usr/local
 /var
 </screen>
      <para>
        The <literal>/etc</literal> directory is mounted as overlayFS, where
        the upper directory is mounted to
        <filename>/var/lib/overlay/1/etc/</filename>.
      </para>
      <para>
        You can recognize the subvolumes mounted by default by the option
        <literal>x-initrd.mount</literal> in <filename>/etc/fstab</filename>.
        Other subvolumes or partitions must be configured either by Ignition
        or Combustion.
      </para>
    </section>
  </section>
  <section xml:id="deploy-alp-jeos-firstboot">
    <title>Deploying <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> with JeOS Firstboot</title>
    <tip>
      <para>
        When booting the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> raw image for the first time,
        <emphasis>JeOS Firstboot</emphasis> enables you to perform a minimal
        configuration of your system. If you need more control over the
        deployment process, find more information in
        <xref linkend="concept-configure-ignition"/> and
        <xref linkend="concept-configure-combustion"/>.
      </para>
    </tip>
    <tip>
      <para>
        If you wish to inspect the installation image before installation, the
        default LUKS password <literal>1234</literal> is required to
        successfully map the image on a local Linux system.
      </para>
    </tip>
    <procedure>
      <step>
        <para>
          Download the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> raw disk image from
          <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://susealp.io/downloads/"/>. There are two
          types of raw images, depending on whether you intend to run
          <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> on an encrypted disk or an unencrypted disk.
        </para>
      </step>
      <step>
        <para>
          If you are deploying <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> as a VM Guest, you need to
          first prepare the virtual machine by following
          <xref linkend="task-prepare-alp-vm"/>.
        </para>
      </step>
      <step>
        <para>
          After booting the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> disk image, you are presented
          with a boot loader screen. Select
          <guimenu><phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></guimenu> and confirm with
          <keycap function="enter"/>.
        </para>
      </step>
      <step>
        <para>
          <guimenu>JeOS Firstboot</guimenu> displays a welcome screen. Confirm
          with <keycap function="enter"/>.
        </para>
        <figure>
          <title>Installation welcome screen</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-firstboot.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-firstboot.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Installation welcome screen</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          On the next screens, select keyboard, confirm the license agreement
          and select the time zone.
        </para>
      </step>
      <step>
        <para>
          In the <guimenu>Enter root password</guimenu> dialog window, enter a
          password for the <systemitem class="username">root</systemitem> and confirm it.
        </para>
        <figure>
          <title>Enter root password</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-rootpwd.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-rootpwd.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Enter root password</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          For encrypted deployments, JeOS Firstboot does the following:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              Asks for a new passphrase that replaces the default passphrase.
            </para>
          </listitem>
          <listitem>
            <para>
              Generates a new LUKS key and re-encrypts the partition.
            </para>
          </listitem>
          <listitem>
            <para>
              Adds a secondary key slot to the LUKS header and seals it against
              the TPM device.
            </para>
          </listitem>
        </itemizedlist>
        <para>
          If you are deploying an encrypted image, follow these steps:
        </para>
        <substeps>
          <step>
            <para>
              Select the desired protection method and confirm with
              <guimenu>OK</guimenu>.
            </para>
          </step>
          <step>
            <para>
              Enter a recovery password for LUKS encryption and retype it. The
              root file system re-encryption begins.
            </para>
          </step>
        </substeps>
        <figure>
          <title>Select method for encryption</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-encrypted-passkey.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-encrypted-passkey.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Select method for encryption</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is successfully deployed using a minimal initial
          configuration.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="next-deploy-alp-raw-image">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Install additional software with <command>transactional-update</command>. Refer to
          <xref linkend="concept-transactional-update"/> for more details.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. Refer to
          <xref linkend="available-alp-workloads"/> for more details.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-ignition"><info>
            <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Configuring with Ignition</title>
          <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  
  <section xml:id="what-is-ignition">
    <title>What is Ignition?</title>
    <para>
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://coreos.github.io/ignition/">Ignition</link>
      is a provisioning tool that enables you to configure a system according
      to your specification on the first boot.
    </para>
  </section>
  <section xml:id="how-it-works-ignition">
    <title>How does Ignition work?</title>
    <para>
      When the system is booted for the first time, Ignition is loaded as
      part of an <filename>initramfs</filename> and searches for a
      configuration file within a specific directory (on a USB flash disk, or
      you can provide a URL). All changes are performed before the kernel
      switches from the temporary file system to the real root file system
      (before the <literal>switch_root</literal> command is issued).
    </para>
    <para>
      Ignition uses a configuration file in the JSON format named
      <filename>config.ign</filename>. You can either write the configuration
      manually or use the Fuel Ignition Web application at
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ignite.opensuse.org"/> to generate it.
    </para>
    <important>
      <para>
        Fuel Ignition does not cover the complete Ignition
        vocabulary yet, and the resulting JSON file may need additional manual
        tweaking.
      </para>
    </important>
    <tip>
      <para>
        If you decide to write the Ignition configuration manually and prefer
        the YAML format over JSON, you can create a YAML file and convert this
        file to JSON using a <literal>Butane</literal> tool. For details, refer
        to <xref linkend="task-convert-yaml-to-json"/>.
      </para>
    </tip>
    <section xml:id="sec-ignition-configuration">
      <title><filename>config.ign</filename></title>
      <para>
        When installing on bare metal, the configuration file
        <filename>config.ign</filename> must reside in the
        <filename>ignition</filename> subdirectory on the configuration media,
        for example, a USB stick labeled <literal>ignition</literal>. The
        directory structure must look as follows:
      </para>
<screen>
&lt;root directory&gt;
└── ignition
    └── config.ign

 </screen>
      <tip>
        <para>
          To create a disk image with the Ignition configuration, you can use
          the Fuel Ignition Web application at https://ignite.opensuse.org.
        </para>
      </tip>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>config.ign</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/com.coreos/config" file="/location/to/config.ign"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <para>
        The <filename>config.ign</filename> contains various data types:
        objects, strings, integers, booleans and lists of objects. For a
        complete specification, refer to
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://coreos.github.io/ignition/configuration-v3_3/">Ignition
        specification v3.3.0</link>.
      </para>
      <para>
        The <literal>version</literal> attribute is mandatory and in case of
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>, its value must be set either to
        <literal>3.3.0</literal> or to any lower version. Otherwise, Ignition
        will fail.
      </para>
      <para>
        If you want to log in to your system as <systemitem class="username">root</systemitem>, you must at least
        include a password for <systemitem class="username">root</systemitem>. However, it is recommended to
        establish access via SSH keys. To configure a password, make sure to
        use a secure one. If you use a randomly generated password, use at
        least 10 characters. If you create your password manually, use even
        more than 10 characters and combine uppercase and lowercase letters and
        numbers.
      </para>
    </section>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-convert-yaml-to-json"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Converting YAML formatted files into JSON</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        JSON is a universal file format for storing structured data.
        Applications, for example, Ignition, use it to store and retrieve
        their configuration. Because JSON's syntax is complex and hard to read
        for human beings, you can write the configuration in a more friendly
        format called YAML and then convert it into JSON.
      </para>
    </abstract>
  </info>
  
  <section xml:id="sec-converting-config">
    <title>Converting YAML files into JSON format</title>
    <para>
      The tool that converts Ignition-specific vocabularies in YAML files
      into JSON format is <literal>butane</literal>. It also verifies the
      syntax of the YAML file to catch potential errors in the structure. For
      the latest version of <literal>butane</literal>, add the following
      repository:
    </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command>  zypper ar -f \
  https://download.opensuse.org/repositories/devel:/kubic:/ignition/openSUSE_Tumbleweed/ \
  devel_kubic_ignition
</screen>
    <para>
      Replace <literal>openSUSE_Tumbleweed</literal> with one of the following
      (depending on your distribution):
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>'openSUSE_Leap_$releasever'</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>15.3</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Now you can install the <literal>butane</literal> tool:
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command>  zypper ref &amp;&amp; zypper in butane</screen>
    <para>
      After the installation is complete, you can invoke
      <literal>butane</literal> by running:
    </para>
<screen><prompt>&gt; </prompt> butane -p -o config.ign config.fcc</screen>
    <itemizedlist>
      <listitem>
        <para>
          <filename>config.fcc</filename> is the path to the YAML configuration
          file.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>config.ign</filename> is the path to the output JSON
          configuration file.
        </para>
      </listitem>
      <listitem>
        <para>
          The <option>-p</option> command option adds line breaks to the output
          file and thus makes it more readable.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-ignition-configuration"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Ignition configuration examples</title>
  </info>
  
  <section xml:id="sec-ignition-examples">
    <title>Configuration examples</title>
    <para>
      This section provides several examples of the Ignition configuration in
      both the native JSON format and the YAML format in addition. Note that
      Ignition does not accept configuration in the YAML format, and you need
      to convert it to the JSON format. To do so, you can use the
      <literal>butane</literal> tool as described in
      <xref linkend="task-convert-yaml-to-json"/>.
    </para>
    <important>
      <para>
        <xref linkend="alp-deployment-default-partitioning"/> lists subvolumes
        that are mounted by default when running the pre-built image. If you
        want to add a new user or modify any of the files on a subvolume that
        is not mounted by default, you need to declare such subvolume first so
        that it is mounted as well. Find more details about mounting file
        systems in <xref linkend="sec-storage-filesystem"/>.
      </para>
    </important>
    <note>
      <title>The <literal>version</literal> attribute is mandatory</title>
      <para>
        Each <filename>config.fcc</filename> must include version 1.4.0 or
        lower that is then converted to the corresponding Ignition
        specification.
      </para>
    </note>
    <section xml:id="sec-ignition-storage">
      <title>Storage configuration</title>
      <para>
        The <literal>storage</literal> attribute is used to configure
        partitions, RAID, define file systems, create files, etc. To define
        partitions, use the <literal>disks</literal> attribute. The
        <literal>filesystems</literal> attribute is used to format partitions
        and define mount points of particular partitions. The
        <literal>files</literal> attribute can be used to create files in the
        file system. Each of the mentioned attributes is described in the
        following sections.
      </para>
      <section xml:id="sec-storage-disks">
        <title>The <literal>disks</literal> attribute</title>
        <para>
          The <literal>disks</literal> attribute is a list of devices that
          enables you to define partitions on these devices. The
          <literal>disks</literal> attribute must contain at least one
          <literal>device</literal>, other attributes are optional. The
          following example will use a single virtual device and divide the
          disk into four partitions:
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "disks": [
      {
        "device": "/dev/vda",
        "partitions": [
          {
            "label": "root",
            "number": 1,
            "typeGuid": "4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709"
          },
          {
            "label": "boot",
            "number": 2,
            "typeGuid": "BC13C2FF-59E6-4262-A352-B275FD6F7172"
          },
          {
            "label": "swap",
            "number": 3,
            "typeGuid": "0657FD6D-A4AB-43C4-84E5-0933C84B4F4F"
          },
          {
            "label": "home",
            "number": 4,
            "typeGuid": "933AC7E1-2EB4-4F13-B844-0E14E2AEF915"
          }
        ],
        "wipeTable": true
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  disks:
    - device: "/dev/vda"
      wipe_table: true
      partitions:
       - label: root
         number: 1
         type_guid: 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709
       - label: boot
         number: 2
         type_guid: BC13C2FF-59E6-4262-A352-B275FD6F7172
       - label: swap
         number: 3
         type_guid: 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
       - label: home
         number: 4
         type_guid: 933AC7E1-2EB4-4F13-B844-0E14E2AEF915
 </screen>
      </section>
      <section xml:id="sec-storage-raid">
        <title>The <literal>raid</literal> attribute</title>
        <para>
          The <literal>raid</literal> is a list of RAID arrays. The following
          attributes of <literal>raid</literal> are mandatory:
        </para>
        <variablelist>
          <varlistentry>
            <term>level</term>
            <listitem>
              <para>
                a level of the particular RAID array (linear, raid0, raid1,
                raid2, raid3, raid4, raid5, raid6)
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>devices</term>
            <listitem>
              <para>
                a list of devices in the array referenced by their absolute
                paths
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>name</term>
            <listitem>
              <para>
                a name that will be used for the md device
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "raid": [
      {
        "devices": [
          "/dev/sda",
          "/dev/sdb"
        ],
        "level": "raid1",
        "name": "system"
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  raid:
    - name: system
      level: raid1
      devices:
        - "/dev/sda"
        - "/dev/sdb"
 </screen>
      </section>
      <section xml:id="sec-storage-filesystem">
        <title>The <literal>filesystems</literal> attribute</title>
        <para>
          <literal>filesystems</literal> must contain the following attributes:
        </para>
        <variablelist>
          <varlistentry>
            <term>device</term>
            <listitem>
              <para>
                the absolute path to the device, typically
                <literal>/dev/sda</literal> in case of physical disk
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>format</term>
            <listitem>
              <para>
                the file system format (btrfs, ext4, xfs, vfat or swap)
              </para>
              <note>
                <para>
                  In case of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>, the <literal>root</literal>
                  file system must be formatted to btrfs.
                </para>
              </note>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>
          The following example demonstrates using the
          <literal>filesystems</literal> attribute. The
          <filename>/opt</filename> directory will be mounted to the
          <literal>/dev/sda1</literal> partition, which is formatted to btrfs.
          The device will not be erased.
        </para>
        <para>
          JSON
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "filesystems": [
      {
        "device": "/dev/sda1",
        "format": "btrfs",
        "path": "/opt",
        "wipeFilesystem": false
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  filesystems:
    - path: /opt
      device: "/dev/sda1"
      format: btrfs
      wipe_filesystem: false
 </screen>
        <para>
          Normally, a regular user's home directory is located in the
          <filename>/home/<replaceable>USER_NAME</replaceable></filename>
          directory. Since <filename>/home</filename> is not mounted by default
          in the initrd, the mount has to be explicitly defined for the user
          creation to succeed:
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.1.0"
  },
  "passwd": {
    "users": [
      {
        "name": "root",
        "passwordHash": "$6$Dxkc092R4JdlFeLE$bfO3TPV1n3a4I1to1/2EkfvU2GiSKpR...",
        "sshAuthorizedKeys": [
          "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCpNOU+nwWRnZYoMV3biUgCC..."
        ]
      }
    ]
  },
  "storage": {
    "filesystems": [
      {
        "device": "/dev/sda3",
        "format": "btrfs",
        "mountOptions": [
          "subvol=/@/home"
        ],
        "path": "/home",
        "wipeFilesystem": false
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.1.0
storage:
  filesystems:
    - path: /home
      device: /dev/sda3
      format: btrfs
      wipe_filesystem: false
      mount_options:
       - "subvol=/@/home"
passwd:
  users:
   - name: root
     password_hash: $6$Dxkc092R4JdlFeLE$bfO3TPV1n3a4I1to1/2EkfvU2GiSKpR...
     ssh_authorized_keys:
       - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCpNOU+nwWRnZYoMV3biUgCC...
</screen>
      </section>
      <section xml:id="sec-storage-files">
        <title>The <literal>files</literal> attribute</title>
        <para>
          You can use the <literal>files</literal> attribute to create any
          files on your machine. Bear in mind that if you want to create files
          outside the default partitioning schema, you need to define the
          directories by using the <literal>filesystems</literal> attribute.
        </para>
        <para>
          In the following example, a host name is created by using the
          <literal>files</literal> attribute. The file
          <filename>/etc/hostname</filename> will be created with the
          <emphasis>alp-1</emphasis> host name:
        </para>
        <important>
          <para>
            Note that the file mode specification is different for JSON and
            YAML. While JSON accepts file modes in decimal numbers, for
            example, <literal>420</literal>, YAML accepts octal numbers
            (<literal>0644</literal>).
          </para>
        </important>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "files": [
      {
        "overwrite": true,
        "path": "/etc/hostname",
        "contents": {
          "source": "data:,alp-1"
        },
        "mode": 420
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      overwrite: true
      contents:
        inline: "alp-1"
 </screen>
      </section>
      <section xml:id="sec-storage-directories">
        <title>The <literal>directories</literal> attribute</title>
        <para>
          The <literal>directories</literal> attribute is a list of directories
          that will be created in the file system. The
          <literal>directories</literal> attribute must contain at least one
          <literal>path</literal> attribute.
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "directories": [
      {
        "path": "/home/tux",
        "user": {
          "name": "tux"
        }
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  directories:
    - path: /home/tux
      user:
        name: tux
 </screen>
      </section>
    </section>
    <section xml:id="sec-ignition-users">
      <title>Users administration</title>
      <para>
        The <literal>passwd</literal> attribute is used to add users. If you
        intend to log in to your system, create <systemitem class="username">root</systemitem> and set the
        <systemitem class="username">root</systemitem>'s password and/or add the SSH key to the Ignition
        configuration. You need to hash the <systemitem class="username">root</systemitem> password, for example by
        using the <command>openssl</command> command:
      </para>
<screen>
 openssl passwd -6
 </screen>
      <para>
        The command creates a hash of the password you chose. Use this hash as
        the value of the <literal>password_hash</literal> attribute.
      </para>
      <para>
        JSON:
      </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "passwd": {
    "users": [
      {
        "name": "root",
        "passwordHash": "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1",
        "sshAuthorizedKeys": [
          "ssh-rsa long...key user@host"
        ]
      }
    ]
  }
}
</screen>
      <para>
        YAML:
      </para>
<screen>
variant: fcos
version: 1.0.0
passwd:
  users:
   - name: root
     password_hash: "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1"
     ssh_authorized_keys:
       - ssh-rsa long...key user@host
 </screen>
      <para>
        The <literal>users</literal> attribute must contain at least one
        <literal>name</literal> attribute.
        <literal>ssh_authorized_keys</literal> is a list of ssh keys for the
        user.
      </para>
    </section>
    <section xml:id="sec-ignition-systemd">
      <title>Enabling <literal>systemd</literal> services</title>
      <para>
        You can enable <systemitem class="daemon">systemd</systemitem> services by specifying them in the
        <literal>systemd</literal> attribute.
      </para>
      <para>
        JSON:
      </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "systemd": {
    "units": [
      {
        "enabled": true,
        "name": "sshd.service"
      }
    ]
  }
}
</screen>
      <para>
        YAML:
      </para>
<screen>
variant: fcos
version: 1.0.0
systemd:
  units:
  - name: sshd.service
    enabled: true
 </screen>
      <para>
        The <literal>name</literal> must be the exact name of a service to be
        enabled (including the suffix).
      </para>
    </section>
  </section>
</section></section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-combustion"><info>
            <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Configuring with Combustion</title>
          <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <section xml:id="what-is-combustion">
    <title>What is Combustion?</title>
    <para>
      Combustion is a dracut module that enables you to configure your system
      on the first boot. You can use Combustion, for example, to change the
      default partitions, set user passwords, create files, or install
      packages.
    </para>
  </section>
  <section xml:id="how-it-works-combustion">
    <title>How does Combustion work?</title>
    <para>
      Combustion is invoked after the <literal>ignition.firstboot</literal>
      argument is passed to the kernel command line. Combustion reads a
      provided file named <filename>script</filename>, executes included
      commands, and thus performs changes to the file system. If
      <filename>script</filename> includes the network flag, Combustion tries
      to configure the network. After <literal>/sysroot</literal> is mounted,
      Combustion tries to activate all mount points in
      <filename>/etc/fstab</filename> and then calls
      <command>transactional-update</command> to apply other changes, for
      example, setting <systemitem class="username">root</systemitem> password or installing packages.
    </para>
    <section xml:id="sec-combustion-configuration">
      <title>The <filename>script</filename> file</title>
      <para>
        When installing on bare metal, the configuration file
        <filename>script</filename> must reside in the
        <filename>combustion</filename> subdirectory on the configuration media
        labeled <literal>combustion</literal>. The directory structure must
        look as follows:
      </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
</screen>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>script</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/org.opensuse.combustion/script" file="/location/to/script"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <tip>
        <title>Using Combustion together with Ignition</title>
        <para>
          Combustion can be used along with Ignition. If you intend to do
          so, label your configuration medium <literal>ignition</literal> and
          include the <filename>ignition</filename> directory with the
          <filename>config.ign</filename> to your directory structure as shown
          below:
        </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
└── ignition
    └── config.ign
</screen>
        <para>
          In this scenario, Ignition runs before Combustion.
        </para>
      </tip>
    </section>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-combustion-configuration"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Combustion configuration examples</title>
  </info>
  
  <section xml:id="configuring-combustion-script">
    <title>The <filename>script</filename> configuration file</title>
    <para>
      The <filename>script</filename> configuration file is a set of commands
      that are parsed and executed by Combustion in a <command>transactional-update</command> shell. This
      article provides examples of configuration tasks performed by
      Combustion.
    </para>
    <important>
      <title>Include interpreter declaration</title>
      <para>
        As the <filename>script</filename> file is interpreted by Bash, always
        start the file with the interpreter declaration at its first line:
      </para>
<screen>#!/usr/bin/bash</screen>
    </important>
    <para>
      To log in to your system, include at least the <systemitem class="username">root</systemitem> password.
      However, it is recommended to establish the authentication using SSH
      keys. If you need to use a <systemitem class="username">root</systemitem> password, make sure to configure a
      secure password. If you use a randomly generated password, use at least
      10 characters. If you create your password manually, use even more than
      10 characters and combine uppercase and lowercase letters and numbers.
    </para>
    <section xml:id="sec-script-network">
      <title>Network configuration</title>
      <para>
        To configure and use the network connection during the first boot, add
        the following statement to <filename>script</filename>:
      </para>
<screen># combustion: network</screen>
      <para>
        Using this statement will pass the <literal>rd.neednet=1</literal>
        argument to dracut. If you do not use the statement, the system will be
        configured without any network connection.
      </para>
    </section>
    <section xml:id="combustion-script-partitioning">
      <title>Partitioning</title>
      <para>
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> raw images are delivered with a default partitioning
        scheme as described in
        <xref linkend="alp-deployment-default-partitioning"/>. You might want
        to use a different partitioning. The following set of example snippets
        moves the <filename>/home</filename> to a different partition.
      </para>
      <note>
        <title>Performing changes outside of directories included in snapshots</title>
        <para>
          The following script performs changes that are not included in
          snapshots. If the script fails and the snapshot is discarded, some
          changes remain visible and cannot be reverted, for example, the
          changes to the <literal>/dev/vdb</literal> device.
        </para>
      </note>
      <para>
        The following snippet creates a GPT partitioning schema with a single
        partition on the <literal>/dev/vdb</literal> device:
      </para>
<screen>
sfdisk /dev/vdb &lt;&lt;EOF
label: gpt
type=linux
EOF

partition=/dev/vdb1
</screen>
      <para>
        The partition is formatted to BTRFS:
      </para>
<screen>
wipefs --all ${partition}
mkfs.btrfs ${partition}
</screen>
      <para>
        Possible content of <filename>/home</filename> is moved to the new
        <filename>/home</filename> folder location by the following snippet:
      </para>
<screen>
mount /home
mount ${partition} /mnt
rsync -aAXP /home/ /mnt/
umount /home /mnt
</screen>
      <para>
        The snippet below removes an old entry in
        <filename>/etc/fstab</filename> and creates a new entry:
      </para>
<screen>
awk -i inplace '$2 != "/home"' /etc/fstab
echo "$(blkid -o export ${partition} | grep ^UUID=) /home btrfs defaults 0 0" &gt;&gt;/etc/fstab
</screen>
    </section>
    <section xml:id="combustion-script-security">
      <title>Setting a password for <systemitem class="username">root</systemitem></title>
      <para>
        Before you set the <systemitem class="username">root</systemitem> password, generate a hash of the
        password, for example, by using the <command>openssl passwd
        -6</command>. To set the password, add the following to the
        <filename>script</filename>:
      </para>
<screen>echo 'root:$5$.wn2BZHlEJ5R3B1C$TAHEchlU.h2tvfOpOki54NaHpGYKwdNhjaBuSpDotD7' | chpasswd -e</screen>
    </section>
    <section xml:id="combustion-script-sshkeys">
      <title>Adding SSH keys</title>
      <para>
        The following snippet creates a directory to store the <systemitem class="username">root</systemitem>'s SSH
        key and then copies the public SSH key located on the configuration
        device to the <filename>authorized_keys</filename> file.
      </para>
<screen>
mkdir -pm700 /root/.ssh/
cat id_rsa_new.pub &gt;&gt; /root/.ssh/authorized_keys
</screen>
      <note>
        <para>
          The SSH service must be enabled in case you need to use remote login
          via SSH. For details, refer to
          <xref linkend="combustion-script-services"/>.
        </para>
      </note>
    </section>
    <section xml:id="combustion-script-services">
      <title>Enabling services</title>
      <para>
        To enable system services, for example, the SSH service, add the
        following line to <filename>script</filename>:
      </para>
<screen>systemctl enable sshd.service</screen>
    </section>
    <section xml:id="combustion-script-install">
      <title>Installing packages</title>
      <important>
        <title>Network connection and registering your system may be necessary</title>
        <para>
          As some packages may require additional subscription, you may need to
          register your system beforehand. An available network connection may
          also be needed to install additional packages.
        </para>
      </important>
      <para>
        During the first boot configuration, you can install additional
        packages to your system. For example, you can install the
        <literal>vim</literal> editor by adding:
      </para>
<screen>zypper --non-interactive install vim-small</screen>
      <note>
        <para>
          Bear in mind that you will not be able to use
          <command>zypper</command> after the configuration is complete and you
          boot to the configured system. To perform changes later, you must use
          the <command>transactional-update</command> command to create a
          changed snapshot.
        </para>
      </note>
    </section>
  </section>
</section></section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-post-deployment-considerations"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Post-deployment considerations</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article includes important information and tasks that you need to
        consider after you successfully deploy <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>
        (<phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>).
      </para>
    </abstract>
  </info>
  
  <section xml:id="alp-post-deploy-full-disk-encryption">
    <title>Full disk encryption</title>
    <section xml:id="alp-post-deploy-full-disk-encryption-password">
      <title>Change encryption password</title>
      <para>
        During the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployment, you entered a password that
        is used for disk encryption. To change the password, run the following
        command:
      </para>
<screen><prompt role="root"># </prompt>fdectl passwd</screen>
    </section>
    <section xml:id="alp-post-deploy-full-disk-encryption-tpm">
      <title>TPM device</title>
      <para>
        Without a TPM chip, you need to enter the encryption password to
        decrypt the disk on each <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> boot. On systems that have
        a TPM 2.0 chip, <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> deployed with the Agama
        installer supports the automatic protection of the LUKS volume with a
        TPM device. The requirement is that the machine must use the
        UEFI Secure Boot enabled.
      </para>
      <para>
        If the Agama installer detects a TPM 2.0 chip and
        UEFI Secure Boot, it creates a secondary LUKS key. On the first boot,
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> uses the TPM to protect this key and configure the
        GRUB 2 boot loader to unwrap the key automatically. Be aware that you
        must remove the ISO after the installer has finished and before the
        system boots for the first time. This is because we use the TPM to
        ensure that the system comes up with exactly the same configuration
        before unlocking the LUKS partition.
      </para>
      <para>
        This allows you to use the full disk encryption without having to type
        the disk password on each reboot. However, the disk password is still
        there and can be used for recovery. For example, after updating the
        GRUB 2 boot loader or the SHIM loader, the TPM can no longer unseal the
        secondary key correctly, and GRUB 2 has to fall back to the password.
      </para>
    </section>
  </section>
  <section xml:id="alp-post-deploy-selinux">
    <title>SELinux</title>
    <para>
      Security-Enhanced Linux (SELinux) is a security framework that increases
      system security by defining access controls for applications, processes
      and files on the file system.
    </para>
    <para>
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> ships with SELinux enabled and set to the restrictive
      <emphasis>enforce</emphasis> mode for increased security. The enforce
      mode can lead to processes or workloads not behaving correctly because
      the default policy may be too strict. If you observe such unexpected
      issues, set SELinux to the <emphasis>permissive</emphasis> mode that does
      not enforce SELinux policies but still logs offenses against them called
      <emphasis>Access Vector Rules</emphasis> (AVCs).
    </para>
    <para>
      To set SELinux to the permissive mode temporarily, run:
    </para>
<screen><prompt role="root"># </prompt>setenforce 0</screen>
    <tip>
      <para>
        To set SELinux to the permissive mode permanently, edit
        <filename>/etc/selinux/config</filename> and update it to include the
        following line:
      </para>
<screen>SELINUX=permissive</screen>
    </tip>
    <important>
      <para>
        If you entered an SELinux permissive mode, you need to relabel your
        system until it is back in a good state. The reason is that the
        permissive mode allows you to reach states that are not reachable
        otherwise. To relabel the system, run the following command and reboot
        the system:
      </para>
<screen>
<prompt role="root"># </prompt>touch /etc/selinux/.autorelabel
<prompt role="root"># </prompt>reboot
</screen>
    </important>
    <para>
      To monitor AVCs, search the Audit log and <systemitem class="daemon">systemd</systemitem> journal for log
      messages similar to the following one:
    </para>
<screen>
type=AVC msg=audit(1669971354.731:25): avc:  denied  { create } \
for pid=1264 comm="ModemManager" scontext=system_u:system_r:modemmanager_t:s0 \
tcontext=system_u:system_r:modemmanager_t:s0 tclass=qipcrtr_socket permissive=0
</screen>
    <para>
      To filter such messages, you can use the following commands:
    </para>
<screen><prompt role="root"># </prompt>tail -f /var/log/audit/audit.log | grep -i AVC</screen>
    <para>
      and
    </para>
<screen><prompt role="root"># </prompt>journalctl -f | grep -i AVC</screen>
    <para>
      For more advanced search, use the following command:
    </para>
<screen><prompt role="root"># </prompt>ausearch -m avc,user_avc,selinux_err -i</screen>
    <para>
      If such messages appear while using the application that did not behave
      correctly when SELinux was set to the enforce mode, the policies are too
      restrictive and need updating. You can help to fine-tune SELinux policies
      by creating a bug report at
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bugzilla.suse.com/enter_bug.cgi?classification=SUSE%20ALP%20-%20SUSE%20Adaptable%20Linux%20Platform"/>.
      Specify <literal>Basesystem</literal> as a component, include the word
      <literal>SELinux</literal> in the bug subject, and attach the gathered
      unique lines that include AVCs together with reproduction steps.
    </para>
  </section>
  <section xml:id="alp-post-deploy-enable-root-ssh-login">
    <title>Enabling <systemitem class="username">root</systemitem> login via SSH</title>
    <para>
      <systemitem class="username">root</systemitem> login via SSH is not permitted in <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> by
      default for security reasons. To enable it, you have the following
      options:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          install the <package>openssh-server-config-rootlogin</package>
          package and reboot the system.
        </para>
<screen>
<prompt role="root"># </prompt><command>transactional-update pkg in openssh-server-config-rootlogin</command>
<prompt role="root"># </prompt><command>reboot</command>
</screen>
      </listitem>
      <listitem>
        <para>
          Add a file containing the snippet <literal>PermitRootLogin
          yes</literal> in the <filename>/etc/sshd/sshd_config.d/</filename>
          directory and reboot, for example:
        </para>
<screen>
<prompt role="root"># </prompt>echo 'PermitRootLogin yes' &gt;&gt; /etc/sshd/sshd_config.d/root_login_config
<prompt role="root"># </prompt><command>reboot</command>
</screen>
      </listitem>
    </itemizedlist>
  </section>
</section><section role="glue" xml:lang="en" version="5.2" xml:id="glue-alp-deployment-more-info"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">For more information</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
  </info>
  
  <itemizedlist>
    <listitem>
      <para>
        Find detailed information about using the Virtual Machine Manager in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
        and
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Transactional updates are described in
        <xref linkend="concept-transactional-update"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Installing software packages and patterns is detailed in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        The SELinux framework is detailed in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-selinux.html"/>.
      </para>
    </listitem>
  </itemizedlist>
</section></chapter>
  <chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-transactional-update"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Transactional updates</title>
  </info>
  
  <section xml:id="what-is-transactional-update">
    <title>What are transactional updates?</title>
    <para>
      To keep the base operating system stable and consistent, the
      <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> uses a read-only root file system.
      Therefore, you cannot perform direct changes to the root file system, for
      example, by using the <command>zypper</command> command. Instead,
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> introduces <emphasis>transactional updates</emphasis>
      that allow you to apply one or more changes to the root file system.
    </para>
  </section>
  <section xml:id="how-transactional-update-works">
    <title>How do transactional updates work?</title>
    <para>
      Each time you call the <command>transactional-update</command> command to change your system—either
      to install a package, perform an update, or apply a patch—the
      following actions take place:
    </para>
    <procedure>
      <title>Modifying the root file system</title>
      <step>
        <para>
          A new read-write snapshot is created from your current root file
          system, or from a snapshot that you specified.
        </para>
      </step>
      <step>
        <para>
          All changes are applied (updates, patches or package installation).
        </para>
      </step>
      <step>
        <para>
          The snapshot is switched back to read-only mode.
        </para>
      </step>
      <step>
        <para>
          If the changes were applied successfully, the new root file system
          snapshot is set as default.
        </para>
      </step>
      <step>
        <para>
          After rebooting, the system boots into the new snapshot.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="benefits-transactional-update">
    <title>Benefits of transactional updates</title>
    <itemizedlist>
      <listitem>
        <para>
          They are atomic—the update is applied only if it completes
          successfully.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes are applied in a separate snapshot and so do not influence
          the running system.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes can easily be rolled back.
        </para>
      </listitem>
    </itemizedlist>
    <section xml:id="tr-up-environment">
      <title>Environment within transactional-update command</title>
      <para>
        Each time you run the <command>transactional-update</command> command, the changes are performed in a
        new snapshot. The environment in the snapshot may differ from
        the one in the shell you run the <command>transactional-update</command> command from. For example, the
        current working directory (<literal>$PWD</literal>) is not set to the
        directory from which you run the <command>transactional-update</command>, but is set to
        <literal>/</literal>.
      </para>
      <para>
        From within the snapshot, you cannot access the
        <filename>/var</filename> directory. This directory is also not
        included in the snapshot as described in
        <xref linkend="snapshots-excluded-dirs"/>. However, some directories
        are not included in the snapshot but are accessible inside the <command>transactional-update</command>
        environment, for example, the <filename>/root</filename> directory.
      </para>
    </section>
  </section>
  
<section role="concept" xml:lang="en" version="5.2" xml:id="how-it-works-transactional-update-etc"><info>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion"><filename>/etc</filename> on a read-only file system</title><meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/></info>
  
  <para>
    Even though <filename>/etc</filename> is part of the read-only file system,
    using an <literal>OverlayFS</literal> layer on this directory enables you
    to write to this directory. All modifications that you performed on the
    content of <filename>/etc</filename> are written to the
    <filename>/var/lib/overlay/<replaceable>SNAPSHOT_NUMBER</replaceable>/etc</filename>.
    Each snapshot has one associated <literal>OverlayFS</literal> directory.
  </para>
  <para>
    Whenever a new snapshot is created (for example, as a result of a system
    update), the content of <filename>/etc</filename> is synchronized and used
    as a base in the new snapshot. In the <literal>OverlayFS</literal>
    terminology, the current snapshot's <filename>/etc</filename> is mounted as
    <literal>lowerdir</literal>. The new snapshot's <filename>/etc</filename>
    is mounted as <literal>upperdir</literal>. If there were no changes in the
    <literal>upperdir</literal> <filename>/etc</filename>, any changes
    performed to the <literal>lowerdir</literal> are visible to the
    <literal>upperdir</literal>. Therefore, the new snapshot also contains the
    changes from the current snapshot's <filename>/etc</filename>.
  </para>
  <important>
    <title>Concurrent modification of <literal>lowerdir</literal> and <literal>upperdir</literal></title>
    <para>
      If <filename>/etc</filename> in both snapshots is modified, only the
      changes in the new snapshot (<literal>upperdir</literal>) persist.
      Changes made to the current snapshot (<literal>lowerdir</literal>) are
      not synchronized to the new snapshot. Therefore, we do not recommend
      changing <filename>/etc</filename> after a new snapshot has been created
      and the system has not been rebooted. However, you can still find the
      changes in the <filename>/var/lib/overlay/</filename> directory for the
      snapshot in which the changes were performed.
    </para>
  </important>
  <note>
    <title>Using the <literal>--continue</literal> option of the <command>transactional-update</command> command</title>
    <para>
      When using the <option>--continue</option> option and the new snapshot is
      a descendant of the current snapshot, then the <filename>/etc</filename>
      overlays of all the snapshots in between will be added as additional
      directories to the <filename>lowerdir</filename> (the
      <filename>lowerdir</filename> can have several mount points).
    </para>
  </note>
</section><section role="concept" xml:lang="en" version="5.2" xml:id="snapshots-excluded-dirs"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Directories excluded from snapshots</title>
    
    
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      
      <para>
        As certain directories store user-specific or volatile data, these
        directories are excluded from snapshots:
      </para>
    </abstract>
  </info>
  
  <variablelist>
    <varlistentry>
      <term><filename>/home</filename></term>
      <listitem>
        <para>
          Contains users' data. Excluded so that the data is not included
          in snapshots and thus potentially overwritten by a rollback
          operation.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/root</filename></term>
      <listitem>
        <para>
          Contains <systemitem class="username">root</systemitem> data. Excluded so that the data is not included
          in snapshots and thus potentially overwritten by a rollback
          operation.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/opt</filename></term>
      <listitem>
        <para>
          Third-party products are usually installed to
          <filename>/opt</filename>. Excluded so that these applications are
          not uninstalled during rollbacks.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/srv</filename></term>
      <listitem>
        <para>
          Contains data for Web and FTP servers. Excluded to avoid
          data loss on rollbacks.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/usr/local</filename></term>
      <listitem>
        <para>
          This directory is used when manually installing software. It is
          excluded to avoid uninstalling these installations on rollbacks.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/var</filename></term>
      <listitem>
        <para>
          This directory contains many variable files, including logs,
          temporary caches, third-party products in
          <filename>/var/opt</filename>, and is the default location for
          virtual machine images and databases. Therefore, a separate subvolume
          is created with Copy-On-Write disabled to exclude all such
          variable data from snapshots.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><filename>/tmp</filename></term>
      <listitem>
        <para>
          The directory contains temporary data.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>the architecture-specific <filename>/boot/grub2</filename> directory</term>
      <listitem>
        <para>
          Rollback of the boot loader binaries is not supported.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section><section role="task" xml:lang="en" version="5.2" xml:id="transactional-update-no-reboot"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Applying multiple changes without reboot</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    The default <command>transactional-update</command> behavior is to create a new snapshot from the current
    root file system after each change. To apply the changes, you need to reboot
    the host. You cannot run the <command>transactional-update</command> command multiple times without
    rebooting to add more changes to the snapshot, because this creates
    separate independent snapshots that do not include changes from the
    previous snapshots.
  </para>
  <para>
    To make multiple changes to the root file system, you have several options, which are
    described in the following sections:
  </para>
  <section xml:id="transactional-update-continue-option">
    <title>The <command>transactional-update</command> <option>--continue</option> option</title>
    <para>
      Use the <command>transactional-update</command> command together with the <option>--continue</option>
      option to make multiple changes without rebooting. A separate snapshot is
      created on each run that contains all changes from the previous snapshot,
      plus your new changes. The final snapshot includes all changes. To apply
      them, reboot the system and your final snapshot becomes the new root file
      system.
    </para>
  </section>
  <section xml:id="transactional-update-run">
    <title>The <command>transactional-update run</command> command</title>
    <para>
      The <command>transactional-update</command> <command>run</command> command normally runs only a single
      command. However, you can use it to run multiple commands in one
      transactional session by concatenating them within a command shell such as
      <command>bash</command>, for example:
    </para>
<screen><prompt role="root"># </prompt><command>transactional-update run bash -c 'ls &amp;&amp; date; if [ true ]; then echo -n "Hello "; echo '\''world'\''; fi'</command></screen>
    <note>
      <para>
        The <command>transactional-update run</command> command has the same limitations as the <command>transactional-update shell</command>
        command described in <xref linkend="transactional-update-shell"/>
        except that the entered commands are logged in the
        <filename>/transactional-update.log</filename> file.
      </para>
    </note>
  </section>
  <section xml:id="transactional-update-shell">
    <title>The <command>transactional-update</command> shell</title>
    <para>
      The <command>transactional-update shell</command> command opens a shell in the transactional-update
      environment. In the shell, you can enter almost any Linux command to make
      changes to the file system, for example, install multiple packages with
      the <command>zypper</command> command or perform changes to files that
      are part of the read-only file system. You can also verify that the
      changes you previously made with the <command>transactional-update</command> command are correct.
    </para>
    <important>
      <para>
        The transactional shell has several limitations. For example, you
        cannot operate start or stop services using <systemitem class="daemon">systemd</systemitem> commands, or
        modify the <filename>/var</filename> partition because it is not
        mounted. Also, commands entered during a shell session are not logged
        in the <filename>/transactional-update.log</filename> file.
      </para>
    </important>
    <para>
      All changes that you make to the file system are part of a single
      snapshot. After you finish making changes to the file system and leave
      the shell with the <command>exit</command> command, you need to reboot
      the host to apply the changes.
    </para>
    <tip>
      <para>
        Another approach to making multiple changes to the file system without
        rebooting the host is to use the <option>--continue</option> option.
        For more details, refer to
        <xref linkend="transactional-update-continue"/>.
      </para>
    </tip>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-transactional-update-usage"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Usage of the <command>transactional-update</command> command</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        The <command>transactional-update</command> command enables the atomic installation or removal of
        updates. Updates are applied only if all can be successfully installed.
        <command>transactional-update</command> creates a snapshot of your system and uses it to update the
        system. Later you can restore this snapshot. All changes become active
        only after reboot.
      </para>
    </abstract>
  </info>
  
  <para>
    The <command>transactional-update</command> command syntax is as follows:
  </para>
<screen>
transactional-update <option>[option]</option> <replaceable>[general_command]</replaceable> <replaceable>[package_command]</replaceable> <replaceable>standalone_command</replaceable>
</screen>
  <note>
    <title>Running <command>transactional-update</command> without arguments</title>
    <para>
      If you do not specify any command or option while running the <command>transactional-update</command>
      command, the system updates itself.
    </para>
  </note>
  <para>
    Possible command parameters are described further.
  </para>
  <variablelist>
    <title><command>transactional-update</command> options</title>
    <varlistentry>
      <term><option>--interactive, -i</option></term>
      <listitem>
        <para>
          Can be used along with a package command to turn on interactive mode.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--non-interactive, -n</option></term>
      <listitem>
        <para>
          Can be used along with a package command to turn on non-interactive
          mode.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="transactional-update-continue">
      <term><option>--continue [<replaceable>number</replaceable>], -c</option></term>
      <listitem>
        <para>
          The <option>--continue</option> option is for making multiple changes
          to the root file system without rebooting. Refer to
          <xref linkend="transactional-update-no-reboot"/> for more details.
        </para>
        <para>
          Another useful feature of the <option>--continue</option> option is
          that you may select any existing snapshot as the base for your new
          snapshot. The following example demonstrates running <command>transactional-update</command> to
          install a new package in a snapshot based on snapshot 13, and then
          running it again to install another package:
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_1</replaceable></command></screen>
<screen><prompt role="root"># </prompt><command>transactional-update --continue 13 pkg install <replaceable>package_2</replaceable></command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--no-selfupdate</option></term>
      <listitem>
        <para>
          Disables self-updating of <command>transactional-update</command>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--drop-if-no-change, -d</option></term>
      <listitem>
        <para>
          Discards the snapshot created by <command>transactional-update</command> if there were no changes to
          the root file system. If there are changes to the <filename>
          /etc</filename> directory, those changes merged back to the current
          file system.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--quiet</option></term>
      <listitem>
        <para>
          The <command>transactional-update</command> command does not output to <literal>stdout</literal> .
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--help, -h</option></term>
      <listitem>
        <para>
          Prints help for the <command>transactional-update</command> command.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--version</option></term>
      <listitem>
        <para>
          Displays the version of the <command>transactional-update</command> command.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    The general commands are the following:
  </para>
  <variablelist>
    <title>General commands</title>
    <varlistentry>
      <term><command>cleanup-snapshots</command></term>
      <listitem>
        <para>
          The command marks all unused snapshots that are intended to be
          removed.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>cleanup-overlays</command></term>
      <listitem>
        <para>
          The command removes all unused overlay layers of <filename>
          /etc</filename>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>cleanup</command></term>
      <listitem>
        <para>
          The command combines the <command>cleanup-snapshots</command> and
          <command>cleanup-overlays</command> commands.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grub.cfg</command></term>
      <listitem>
        <para>
          Use this command to rebuild the GRUB boot loader configuration file.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>bootloader</command></term>
      <listitem>
        <para>
          The command reinstalls the boot loader.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>initrd</command></term>
      <listitem>
        <para>
          Use the command to rebuild <literal>initrd</literal>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kdump</command></term>
      <listitem>
        <para>
          In case you perform changes to your hardware or storage, you may need
          to rebuild the Kdump initrd.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>shell</command></term>
      <listitem>
        <para>
          Opens a read-write shell in the new snapshot before exiting. The
          command is typically used for debugging purposes.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>reboot</command></term>
      <listitem>
        <para>
          The system reboots after the <command>transactional-update</command> command is complete.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>run <replaceable>&lt;command&gt;</replaceable></command></term>
      <listitem>
        <para>
          Runs the provided command in a new snapshot.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>setup-selinux</command></term>
      <listitem>
        <para>
          Installs and enables targeted SELinux policy.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    The package commands are the following:
  </para>
  <variablelist>
    <title>Package commands</title>
    <varlistentry>
      <term><command>dup</command></term>
      <listitem>
        <para>
          Performs upgrade of your system. The default option for this command
          is <option>--non-interactive</option>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>migration</command></term>
      <listitem>
        <para>
          The command migrates your system to a selected target. Typically, it
          is used to upgrade your system if it has been registered via SUSE Customer Center.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>patch</command></term>
      <listitem>
        <para>
          Checks for available patches and installs them. The default option
          for this command is <option>--non-interactive</option>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>pkg install</command></term>
      <listitem>
        <para>
          Installs individual packages from the available channels using the
          <command>zypper install</command> command. This command can also be
          used to install Program Temporary Fix (PTF) RPM files. The default
          option for this command is <option>--interactive</option>.
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_name</replaceable></command></screen>
        <para>
          or
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>rpm1 rpm2</replaceable></command></screen>
        <para>
          Or, to install a software pattern:
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install -t pattern <replaceable>pattern_name</replaceable></command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>pkg remove</command></term>
      <listitem>
        <para>
          Removes individual packages from the active snapshot using the
          <command>zypper remove</command> command. This command can also be
          used to remove PTF RPM files. The default option for this command is
          <option> --interactive</option>.
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg remove <replaceable>package_name</replaceable></command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>pkg update</command></term>
      <listitem>
        <para>
          Updates individual packages from the active snapshot using the
          <command>zypper update</command> command. Only packages that are part
          of the snapshot of the base file system can be updated. The default
          option for this command is <option>--interactive</option>.
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg update <replaceable>package_name</replaceable></command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>register</command></term>
      <listitem>
        <para>
          Registers or deregisters your system. For a complete usage
          description, refer to
          <xref linkend="sec-register-command"/>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>up</command></term>
      <listitem>
        <para>
          Updates installed packages to newer versions. The default option for
          this command is <option>--non-interactive</option>.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    The stand-alone commands are the following:
  </para>
  <variablelist>
    <title>Stand-alone commands</title>
    <varlistentry>
      <term><command>rollback <replaceable>&lt;snapshot number&gt;</replaceable></command></term>
      <listitem>
        <para>
          This sets the default subvolume. The current system is set as the new
          default root file system. If you specify a number, that snapshot is
          used as the default root file system. On a read-only file system, it
          does not create any additional snapshots.
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update rollback <replaceable>snapshot_number</replaceable></command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>rollback last</command></term>
      <listitem>
        <para>
          This command sets the last known to be working snapshot as the
          default.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>status</command></term>
      <listitem>
        <para>
          This prints a list of available snapshots. The currently booted one
          is marked with an asterisk, the default snapshot is marked with a
          plus sign.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="sec-register-command">
    <title>The <command>register</command> command</title>
    <para>
      The <command>register</command> command enables you to handle all tasks
      regarding registration and subscription management. You can supply the
      following options:
    </para>
    <variablelist>
      <varlistentry>
        <term><option>--list-extensions</option></term>
        <listitem>
          <para>
            With this option, the command lists available extensions for your
            system. You can use the output to find a product identifier for
            product activation.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>-p, --product</option></term>
        <listitem>
          <para>
            Use this option to specify a product for activation. The product
            identifier has the following format: <emphasis>
            &lt;name&gt;/&lt;version&gt;/&lt;architecture&gt;</emphasis>, for
            example, <literal>sle-module-live-patching/15.3/x86_64</literal>.
            The corresponding command has the following form:
          </para>
<screen><prompt role="root"># </prompt>transactional-update register -p sle-module-live-patching/15.3/x86_64</screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>-r, --regcode</option></term>
        <listitem>
          <para>
            Register your system with the registration code provided. The
            command registers the subscription and enables software
            repositories.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>-d, --de-register</option></term>
        <listitem>
          <para>
            The option deregisters the system, or when used along with the
            <literal>-p</literal> option, deregisters an extension.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>-e, --email</option></term>
        <listitem>
          <para>
            Specify an email address that is used in SUSE Customer Center for registration.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--url</option></term>
        <listitem>
          <para>
            Specify the URL of your registration server. The URL is stored in
            the configuration and is used in subsequent command invocations.
            For example:
          </para>
<screen><prompt role="root"># </prompt>transactional-update register --url https://scc.suse.com</screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>-s, --status</option></term>
        <listitem>
          <para>
            Displays the current registration status in JSON format.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--write-config</option></term>
        <listitem>
          <para>
            Writes the provided options value to the <filename>
            /etc/SUSEConnect</filename> configuration file.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--cleanup</option></term>
        <listitem>
          <para>
            Removes old system credentials.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--version</option></term>
        <listitem>
          <para>
            Prints the version.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--help</option></term>
        <listitem>
          <para>
            Displays the usage of the command.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
</section></chapter>
  <chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-containers-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Containers and Podman</title>
  </info>
  
  
  <section xml:id="what-is-containers-podman">
    <title>What are containers and Podman?</title>
    <para>
      Containers offer a lightweight virtualization method to run multiple
      virtual environments (containers) simultaneously on a single host. Unlike
      technologies such as Xen or KVM, where the processor simulates a
      complete hardware environment and a hypervisor controls virtual machines,
      containers provide virtualization on the operating system level, where
      the kernel controls the isolated containers.
    </para>
    <para>
      <emphasis>Podman</emphasis> is a short name for Pod Manager Tool. It is
      a daemonless container engine that enables you to run and deploy
      applications using containers and container images. Podman provides a
      command line interface to manage containers.
    </para>
  </section>
  <section xml:id="how-it-works-podman">
    <title>How does Podman work?</title>
    <para>
      Podman provides integration with <systemitem class="daemon">systemd</systemitem>. This way you can control
      containers via <systemitem class="daemon">systemd</systemitem> units. You can create these units for existing
      containers as well as generate units that can start containers if they do
      not exist in the system. Moreover, Podman can run <systemitem class="daemon">systemd</systemitem> inside
      containers.
    </para>
    <para>
      Podman enables you to organize your containers into pods. Pods share
      the same network interface and resources. A typical use case for
      organizing a group of containers into a pod is a container that runs a
      database and a container with a client that accesses the database.
    </para>
    <section xml:id="pod-architecture">
      <title>Pods architecture</title>
      <para>
        A pod is a group of containers that share the same namespace, ports and
        network connection. Usually, containers within one pod can communicate
        directly with each other. Each pod contains an infrastructure container
        (<literal>INFRA</literal>), whose purpose is to hold the namespace.
        <literal>INFRA</literal> also enables Podman to add other containers
        to the pod. Port bindings, cgroup-parent values, and kernel namespaces
        are all assigned to the infrastructure container. Therefore, later
        changes of these values are not possible.
      </para>
      <figure xml:id="fig-pod-architecture">
        <title>Pods architecture</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <textobject role="description"><phrase>Pods architecture</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        Each container in a pod has its own instance of a monitoring program.
        The monitoring program watches the container's process and if the
        container dies, the monitoring program saves its exit code. The program
        also holds open the tty interface for the particular container. The
        monitoring program enables you to run containers in the detached mode
        when Podman exits, because this program continues to run and enables
        you to attach tty later.
      </para>
    </section>
  </section>
  <section xml:id="benefits-containers-podman">
    <title>Benefits of containers</title>
    <itemizedlist mark="bullet" spacing="normal">
      <listitem>
        <para>
          Containers make it possible to isolate applications in self-contained
          units.
        </para>
      </listitem>
      <listitem>
        <para>
          Containers provide near-native performance. Depending on the runtime,
          a container can use the host kernel directly, thus minimizing
          overhead.
        </para>
      </listitem>
      <listitem>
        <para>
          It is possible to control network interfaces and apply resources
          inside containers through kernel control groups.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-enable-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Enabling Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article helps you verify that Podman is installed on the
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> system and provides guidelines to enable its
        <systemitem class="daemon">systemd</systemitem> service when Cockpit requires it.
      </para>
    </abstract>
  </info>
  
  <section xml:id="requirements-enable-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> base OS.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="install-podman">
    <title>Installing Podman</title>
    <procedure>
      <step>
        <para>
          Verify that Podman is installed on your system by running the
          following command:
        </para>
<screen><prompt role="root"># </prompt>zypper se -i podman</screen>
      </step>
      <step>
        <para>
          If Podman is not listed in the output, install it by running:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install podman*</screen>
      </step>
      <step>
        <para>
          Reboot the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host for the changes to take effect.
        </para>
      </step>
      <step>
        <para>
          Optionally, enable and start the
          <systemitem class="daemon">podman.service</systemitem> service for
          applications that require it, such as Cockpit. You can enable it
          either in Cockpit by clicking <menuchoice><guimenu>Podman
          containers</guimenu><guimenu>Start podman</guimenu></menuchoice>, or
          by running the following command:
        </para>
<screen><prompt role="root"># </prompt>systemctl enable --now podman.service</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="install-podman-rootless">
    <title>Enabling rootless mode</title>
    <para>
      By default, Podman requires <systemitem class="username">root</systemitem> privileges. To enable rootless
      mode for the current user, run the following command:
    </para>
<screen>
<prompt>&gt; </prompt>sudo usermod --add-subuids 100000-165535 \
  --add-subgids 100000-165535 <replaceable>USER</replaceable>
</screen>
    <para>
      Reboot the machine to enable the change. The command above defines a
      range of local UIDs to which the UIDs allocated to users inside the
      container are mapped on the host. Note that the ranges defined for
      different users must not overlap. It is also important that the ranges do
      not reuse the UID of an existing local user or group. By default, adding
      a user with the <command>useradd</command> command automatically
      allocates subUID and subGID ranges.
    </para>
    <note>
      <title>Limitations of rootless containers</title>
      <para>
        Running a container with Podman in rootless mode on SLE Micro may fail,
        because the container might need access to directories or files that
        require <systemitem class="username">root</systemitem> privileges.
      </para>
    </note>
  </section>
  <section xml:id="next-enable-podman">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Run containerized workloads. For details, refer to
          <xref linkend="available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="podman-autostarting-containers"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Autostarting containers</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    Podman does not have a command-line option to enable autostarting a
    specific container after the system boot. Podman can, however, create a
    <systemitem class="daemon">systemd</systemitem> service for an existing container. After you enable such service,
    the container starts on every system boot.
  </para>
  <procedure>
    <title>Creating and enabling a container service as a non-<systemitem class="username">root</systemitem> user</title>
    <step>
      <para>
        Identify the container that you want to start on system boot.
      </para>
<screen><prompt>&gt; </prompt><command>podman ps</command>
CONTAINER ID  IMAGE [...]                                    NAMES
9a0fdeee9320  registry.opensuse.org/.../cockpit-ws:latest    cockpit-ws</screen>
    </step>
    <step>
      <para>
        Create a <systemitem class="daemon">systemd</systemitem> unit file for the service related to the
        <literal>cockpit-ws</literal> container and save it as
        <filename>container-cockpit-ws.service</filename> in the current
        directory.
      </para>
<screen><prompt>&gt; </prompt><command>podman generate systemd --new --name cockpit-ws \
  &gt; container-cockpit-ws.service</command></screen>
    </step>
    <step>
      <para>
        Move the <systemitem class="daemon">systemd</systemitem> unit file to
        <filename>~/.config/systemd/user/</filename> in the user's home
        directory.
      </para>
<screen><prompt>&gt; </prompt><command>mv container-cockpit-ws.service</command> \
  ~/.config/systemd/user/</screen>
    </step>
    <step>
      <para>
        Make <systemitem class="daemon">systemd</systemitem> aware of the previously created unit file.
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>systemctl --user daemon-reload</command></screen>
    </step>
    <step>
      <para>
        Enable the <systemitem class="daemon">systemd</systemitem> service.
      </para>
<screen><prompt>&gt; </prompt><command>systemctl --user enable container-cockpit-ws.service</command></screen>
    </step>
    <step>
      <para>
        Because the service is run by a non-<systemitem class="username">root</systemitem> user, you need to make
        sure that the user is logged in at boot and stays active even after
        they are logged out of a display manager or a terminal session. This
        mechanism is called <emphasis>lingering</emphasis> and is achieved by
        the following command:
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>loginctl enable-linger <replaceable>USER_NAME</replaceable></command></screen>
    </step>
    <step>
      <para>
        Reboot the host and check if the service is running.
      </para>
<screen><prompt>&gt; </prompt><command>systemctl status --user
container-cockpit-ws.service</command></screen>
    </step>
  </procedure>
  <procedure>
    <title>Creating and enabling a container service as a <systemitem class="username">root</systemitem> user</title>
    <step>
      <para>
        Identify the container that you want to start on system boot.
      </para>
<screen><prompt>&gt; </prompt><command>podman ps</command>
CONTAINER ID  IMAGE [...]                                    NAMES
9a0fdeee9320  registry.opensuse.org/.../cockpit-ws:latest    cockpit-ws</screen>
    </step>
    <step>
      <para>
        Create a <systemitem class="daemon">systemd</systemitem> unit file for the service related to the
        <literal>cockpit-ws</literal> container and save it as
        <filename>container-cockpit-ws.service</filename> in the current
        directory.
      </para>
<screen><prompt>&gt; </prompt><command>podman generate systemd --new --name cockpit-ws \
  &gt; container-cockpit-ws.service</command></screen>
    </step>
    <step>
      <para>
        Move the <systemitem class="daemon">systemd</systemitem> unit file to
        <filename>/etc/systemd/system/</filename>.
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>mv container-cockpit-ws.service</command> \
  /etc/systemd/system/</screen>
    </step>
    <step>
      <para>
        Make <systemitem class="daemon">systemd</systemitem> aware of the previously created unit file.
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>systemctl daemon-reload</command></screen>
    </step>
    <step>
      <para>
        Enable the <systemitem class="daemon">systemd</systemitem> service.
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> <command>systemctl enable container-cockpit-ws.service</command></screen>
    </step>
    <step>
      <para>
        Reboot the host and check if the service is running.
      </para>
<screen><prompt>&gt; </prompt><command>systemctl status container-cockpit-ws.service</command></screen>
    </step>
  </procedure>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-podman-usage"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Podman usage</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article introduces basic Podman usage that you may need when
        running containerized workloads.
      </para>
    </abstract>
  </info>
  
  <section xml:id="sec-getting-images">
    <title>Getting container images</title>
    <para>
      To run a container, you need an image. An image includes all dependencies
      needed to run an application. You can obtain images from an image
      registry. Available registries are defined in the
      <filename>/etc/containers/registries.conf</filename> configuration file.
      If you have a local image registry or want to use other registries, add
      the registries into the configuration file.
    </para>
    <important>
      <title>No tools for building images in <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></title>
      <para>
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> does not provide tools for building custom images.
        Therefore, the only way to get an image is to pull it from an image
        registry.
      </para>
    </important>
    <para>
      The <command>podman pull</command> command pulls an image from an image
      registry. The syntax is as follows:
    </para>
<screen><prompt role="root"># </prompt>podman pull <replaceable>[OPTIONS]</replaceable> <replaceable>SOURCE</replaceable></screen>
    <para>
      The <replaceable>source</replaceable> can be an image without the
      registry name. In that case, Podman tries to pull the image from all
      registries configured in the
      <filename>/etc/containers/registries.conf</filename> file. The default
      image tag is <literal>latest</literal>. The default location of pulled
      images is
      <filename>/var/lib/containers/storage/overlay-images/</filename>.
    </para>
    <para>
      To view all possible options of the <command>podman pull</command>
      command, run:
    </para>
<screen><prompt role="root"># </prompt>podman pull --help</screen>
    <note>
      <title>Getting images using Cockpit</title>
      <para>
        If you are using Cockpit, you can also pull images from an image
        registry in the <guimenu>Podman containers</guimenu> menu by clicking
        <guimenu>+ Get new image</guimenu>.
      </para>
    </note>
    <para>
      Podman enables you to search for images in an image registry or a list
      of registries using the command:
    </para>
<screen><prompt role="root"># </prompt>podman search <replaceable>IMAGE_NAME</replaceable></screen>
  </section>
  <section xml:id="sec-working-containers">
    <title>Working with containers</title>
    <para>
      The following section covers common container management tasks. This
      includes creating, starting, and modifying containers.
    </para>
    <warning>
      <para>
        The current version of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> does not provide tools for
        building custom images. Therefore, the only way to get a container
        image is to pull it from an image registry.
      </para>
    </warning>
    <section xml:id="sec-podman-run">
      <title>Running containers</title>
      <tip>
        <para>
          For specific details on running <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> containers, refer
          to links in the <xref linkend="available-alp-workloads"/> article.
        </para>
      </tip>
      <para>
        After you have pulled your container image, you can create containers
        based on it. You can run an instance of the image using the
        <command>podman run</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman run [<replaceable>OPTIONS</replaceable>] <replaceable>IMAGE</replaceable> [<replaceable>CONTAINER_NAME</replaceable>]</screen>
      <para>
        <replaceable>IMAGE</replaceable> is specified in format
        <emphasis>transport:path</emphasis>. If <emphasis>transport</emphasis>
        is omitted, the default <literal>docker</literal> is used. The
        <emphasis>path</emphasis> can reference to a specific image registry.
        If omitted, Podman searches for the image in registries defined in
        the <filename>/etc/containers/registries.conf</filename> file. An
        example that runs a container called <literal>sles15</literal> based on
        the <literal>sle15</literal> image follows:
      </para>
<screen><prompt role="root"># </prompt>podman run registry.opensuse.org/suse/templates/images/sle-15-sp3/base/images/suse/sle15 sles15</screen>
      <para>
        Below is a list of frequently used options. For a complete list of
        available options, run the command: <command>podman run
        --help</command>.
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--detach, -d</literal></term>
          <listitem>
            <para>
              The container will run in the background.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--env, -e=env</literal></term>
          <listitem>
            <para>
              This option allows arbitrary environment variables that are
              available for the process to be launched inside of the container.
              If an environment variable is specified without a value, Podman
              will check the host environment for a value and set the variable
              only if it is set on the host.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--help</literal></term>
          <listitem>
            <para>
              Prints help for the <command>podman run</command> command.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--hostname=</literal><emphasis>name</emphasis>,<literal> -h</literal></term>
          <listitem>
            <para>
              Sets the container host name that is available inside the
              container.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--pod=</literal><emphasis>name</emphasis></term>
          <listitem>
            <para>
              Runs the container in an existing pod. To create a pod, prefix
              the pod name with <literal>new:</literal>.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--read-only</literal></term>
          <listitem>
            <para>
              Mounts the container’s root file system as read-only.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--systemd=true|false|always</literal></term>
          <listitem>
            <para>
              Runs the container in systemd mode. The default is true.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="sec-podman-stop">
      <title>Stopping containers</title>
      <para>
        If the <command>podman run</command> command finished successfully, a
        new container has been started. You can stop the container by running:
      </para>
<screen><prompt role="root"># </prompt>podman stop <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        You can specify a single container name or ID or a space-separated list
        of containers. The command takes the following options:
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--all, -a</literal></term>
          <listitem>
            <para>
              Stops all running containers.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--latest, -l</literal></term>
          <listitem>
            <para>
              Instead of providing a container name, the last created container
              will be stopped.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--time, -t=</literal><emphasis>seconds</emphasis></term>
          <listitem>
            <para>
              Seconds to wait before forcibly stopping the container.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        To view all possible options of the <command>podman stop</command>
        command, run the following:
      </para>
<screen><prompt role="root"># </prompt>podman stop --help</screen>
    </section>
    <section xml:id="sec-podman-start">
      <title>Starting containers</title>
      <para>
        To start already created but stopped containers, use the
        <command>podman start</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman start <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        For a complete list of possible options of <command>podman
        start</command>, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman start --help</screen>
    </section>
    <section xml:id="podman-update-containers">
      <title>Updating containers</title>
      <para>
        To update an existing container, follow these steps:
      </para>
      <procedure>
        <step>
          <para>
            Identify the image of the container that you want to update, for
            example, <literal>yast-mgmt-qt</literal>:
          </para>
<screen>
<prompt>&gt; </prompt>podman image ls
REPOSITORY                                                                                                  TAG         IMAGE ID      CREATED      SIZE
[...]
registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt  latest      f349194a439d  13 days ago  674 MB
</screen>
        </step>
        <step>
          <para>
            Pull the image from the registry to find out if there is a newer
            version. If you do not specify a version tag, the
            <literal>latest</literal> tag is used:
          </para>
<screen>
<prompt role="root"># </prompt>podman pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
Trying to pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest...
Getting image source signatures
Copying blob 6bfbcdeee2ec done
[...]
Writing manifest to image destination
Storing signatures
f349194a439da249587fbd8baffc5659b390aa14c8db1d597e95be703490ffb1
</screen>
        </step>
        <step>
          <para>
            If the container is running, identify its ID and stop it:
          </para>
<screen>
<prompt role="root"># </prompt>podman ps
CONTAINER ID  IMAGE                                                                             COMMAND     CREATED         STATUS
[...]
28fef404417b /workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest               2 weeks ago     Up 24 seconds ago
<prompt role="root"># </prompt>podman stop 28fef404417b
</screen>
        </step>
        <step>
          <para>
            Run the container following specific instructions at
            <xref linkend="available-alp-workloads"/>, for example:
          </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="sec-podman-commit">
      <title>Committing modified containers</title>
      <para>
        You can run a new container with specific attributes that are not part
        of the original image. To save the container with these attributes as a
        new image, you can use the <command>podman commit</command> command:
      </para>
<screen><prompt role="root"># </prompt>podman commit <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> is a container name or a container
        ID. <replaceable>IMAGE</replaceable> is the new image name. If the
        image name does not start with a registry name, the value
        <literal>localhost</literal> is used.
      </para>
      <para>
        When using Cockpit, you can perform the <command>commit</command>
        operation directly from a container's <guimenu>Details</guimenu>, by
        clicking <guimenu>Commit</guimenu>. A dialog box opens. Specify all
        required details as shown below and click <guimenu>Commit</guimenu>:
      </para>
      <figure>
        <title>Committing a container in Cockpit</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
    <section xml:id="sec-podman-ps">
      <title>Listing containers</title>
      <para>
        Podman enables you to list all running containers using the
        <command>podman ps</command> command. The generic syntax of the command
        is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman  ps <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        Command options can change the displayed information. For example,
        using the <literal>--all</literal> option will output all containers
        created by Podman (not only the running containers).
      </para>
      <para>
        For a complete list of <command>podman ps</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman ps --help</screen>
    </section>
    <section xml:id="sec-podman-rm">
      <title>Removing containers</title>
      <para>
        To remove one or more unused containers from the host, use the
        <command>podman rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman rm <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        The command does not remove the specified container if the container is
        running. To remove a running container, use the <literal>-f</literal>
        option.
      </para>
      <para>
        For a complete list of <command>podman rm</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman rm --help</screen>
      <note>
        <title>Deleting all stopped containers</title>
        <para>
          You can delete all stopped containers from your host with a single
          command:
        </para>
<screen><prompt role="root"># </prompt>podman container prune</screen>
        <para>
          Make sure that each stopped container is intended to be removed
          before you run the command, otherwise you might remove containers
          that are still in use and were stopped only temporarily.
        </para>
      </note>
    </section>
  </section>
  <section xml:id="sec-working-pods">
    <title>Working with pods</title>
    <para>
      Containers can be grouped into a pod. The containers in the pod then
      share network, pid, and IPC namespace. Pods can be managed by
      <command>podman pod</command> commands. This section provides an overview
      of the commands for managing pods.
    </para>
    <section xml:id="sec-creating-pods">
      <title>Creating pods</title>
      <para>
        The command <command>podman pod create</command> is used to create a
        pod. The syntax of the command is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod create <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        The command outputs the pod ID. By default, the pods are created
        without being started. You can start a pod by running a container in
        the pod, or by starting the pod as described in
        <xref linkend="sec-starting-pods"/>.
      </para>
      <note>
        <title>Default pod names</title>
        <para>
          If you do not specify a pod name with the <literal>--name</literal>
          option, Podman will assign a default name for the pod.
        </para>
      </note>
      <para>
        For a complete list of possible options, run the following command:
      </para>
<screen><prompt role="root"># </prompt>podman pod create --help</screen>
    </section>
    <section xml:id="sec-listing-pods">
      <title>Listing pods</title>
      <para>
        You can list all pods by running the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod list</screen>
      <para>
        The output looks as follows:
      </para>
<screen>
POD ID        NAME               STATUS   CREATED       # OF CONTAINERS  INFRA ID
30fba506fecb  upbeat_mcclintock  Created  19 hours ago  1                4324f40c9651
976a83b4d88b  nervous_feynman    Running  19 hours ago  2                daa5732ecd02
</screen>
      <para>
        As each pod includes the <literal>INFRA</literal> container, the number
        of containers in a pod is always larger than zero.
      </para>
    </section>
    <section xml:id="sec-starting-pods">
      <title>Starting/stopping/restarting pods</title>
      <para>
        After a pod is created, you must start it, as it is not in the state
        <literal>running</literal> by default. In the commands below,
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To start a pod, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod start <replaceable>[OPTIONS]</replaceable> <replaceable>POD</replaceable></screen>
      <para>
        For a complete list of possible options, run:
      </para>
<screen><prompt role="root"># </prompt>podman pod start --help</screen>
      <para>
        To stop a pod, use the <command>podman pod stop</command> as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod stop <replaceable>POD</replaceable></screen>
      <para>
        To restart a pod, use the <command>podman pod restart</command> command
        as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod restart <replaceable>POD</replaceable></screen>
    </section>
    <section xml:id="sec-adding-pods">
      <title>Managing containers in a pod</title>
      <para>
        To add a new container to a pod, use the <command>podman run</command>
        command with the option <literal>--pod</literal>. A general syntax of
        the command follows:
      </para>
<screen><prompt role="root"># </prompt>podman run <replaceable>[OPTIONS]</replaceable> --pod <replaceable>POD_NAME</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        For details about the <command>podman run</command> command, refer to
        <xref linkend="sec-podman-run"/>.
      </para>
      <note>
        <title>Only new containers can be added to a pod</title>
        <para>
          The <command>podman start</command> command does not allow for
          starting a container in a pod if the container was not added to the
          pod during the container's initial running.
        </para>
      </note>
      <para>
        You cannot remove a container from a pod and keep the container
        running, because the container itself is removed from the host.
      </para>
      <para>
        Other actions like start, restart and stop can be performed on specific
        containers without affecting the status of the pod.
      </para>
    </section>
    <section xml:id="sec-removing-pods">
      <title>Removing pods</title>
      <para>
        There are two ways to remove pods. You can use the <command>podman pod
        rm</command> command to remove one or more pods. Alternatively, you can
        remove all stopped pods using the <command>podman pod prune</command>
        command.
      </para>
      <para>
        To remove a pod or several pods, run the <command>podman pod
        rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod rm <replaceable>POD</replaceable></screen>
      <para>
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To remove all currently stopped pods, use the <command>podman pod
        prune</command> command. Make sure that all stopped pods are intended
        to be removed before you run the <command>podman pod prune</command>
        command, otherwise you might remove pods that are still in use.
      </para>
    </section>
    <section xml:id="sec-monitoring-pods">
      <title>Monitoring processes in pods</title>
      <para>
        To view all containers in all pods, use the following command:
      </para>
<screen><prompt role="root"># </prompt>podman ps -a --pod</screen>
      <para>
        The output of the command will be similar to the following one:
      </para>
      
<screen>
<?dbsuse-fo font-size="0.70em"?>

CONTAINER ID  IMAGE                       COMMAND    CREATED       STATUS                 [...]
4324f40c9651  k8s.gcr.io/pause:3.2                   21 hours ago  Created
daa5732ecd02  k8s.gcr.io/pause:3.2                   22 hours ago  Up 3 hours ago
e5c8e360c54b  localhost/test:latest       /bin/bash  3 days ago    Exited (137) 3 days ago
82dad15828f7  localhost/opensuse/toolbox  /bin/bash  3 days ago    Exited (137) 3 days ago
1a23da456b6f  docker.io/i386/ubuntu       /bin/bash  4 days ago    Exited (0) 6 hours ago
df890193f651  localhost/opensuse/toolbox  /bin/bash  4 days ago    Created
  </screen>
      <para>
        The first two records are the <literal>INFRA</literal> containers of
        each pod, based on the <literal>k8s.gcr.io/pause:3.2</literal> image.
        Other containers in the output are stand-alone containers that do not
        belong to any pod.
      </para>
    </section>
  </section>
</section></chapter>
  <chapter role="glue" xml:lang="en" version="5.2" xml:id="available-alp-workloads"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">SUSE Workloads</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> runs containerized workloads instead
        of traditional applications. Images of these containers are stored in
        image registries online. <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> can run any containerized
        workload that is supported by the default container manager Podman.
        This article lists and describes workloads securely distributed and
        supported by SUSE. You can find the source files of the workloads at
        <link xlink:href="https://build.opensuse.org/project/show/SUSE:ALP:Workloads"/>.
      </para>
    </abstract>
  </info>
  
  <section xml:id="suse-workload-common-requirements">
    <title>Common requirements</title>
    <para>
      To run workloads on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> using Podman, you generally need
      to have:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Deployed <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-run-yast-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the YaST workload using Podman</title>
  </info>
  
  <section xml:id="introduction-run-yast-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to start the YaST workload on
      <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>. The following YaST container images
      are available:
    </para>
    <variablelist>
      <varlistentry>
        <term>yast-mgmt-ncurses</term>
        <listitem>
          <para>
            The base YaST workload. It contains the text version of YaST
            (ncurses).
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-qt</term>
        <listitem>
          <para>
            This workload adds the Qt-based graphical user interface.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-web</term>
        <listitem>
          <para>
            This workload exposes the standard graphical interface via a VNC
            server and uses a JavaScript VNC client to render the screen in a
            Web browser.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="alp-starting-yast-text-mode">
    <title>Starting YaST in text mode</title>
    <para>
      To start the text version (ncurses) of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-ncurses
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest
</screen>
        <figure>
          <title>YaST running in text mode on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>YaST running in text mode on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-starting-yast-qt">
    <title>Starting graphical YaST</title>
    <para>
      To start the graphical Qt version of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          To view the graphical YaST on your local X server, you need to use
          SSH X forwarding. It requires the <package>xauth</package> package
          installed, applied by the host reboot:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install xauth &amp;&amp; reboot</screen>
      </step>
      <step>
        <para>
          Connect to the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host using <command>ssh</command>
          with the X forwarding enabled:
        </para>
<screen><prompt>&gt; </prompt>ssh -X <replaceable>ALP_HOST</replaceable></screen>
      </step>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-qt
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
[...]
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest
</screen>
        <figure>
          <title>Running graphical YaST on top of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Running graphical YaST on top of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-kvm-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="caleb.crane@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the KVM virtualization workload using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to run a KVM VM Host Server on <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>
        (<phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>). This workload adds virtualization capability to
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> so that you can use it as a VM Host Server. It uses the
        KVM hypervisor supported by the <systemitem class="library">libvirt</systemitem> toolkit.
      </para>
      <important>
        <para>
          When running <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> in a virtualized environment, you
          need to enable the nested KVM virtualization on the bare-metal host
          operating system and use <literal>kernel-default</literal> kernel
          instead of the default <literal>kernel-default-base</literal> in
          <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>.
        </para>
      </important>
    </abstract>
  </info>
  
  <section xml:id="alp-starting-kvm">
    <title>Deploying the KVM Server workload</title>
    <para>
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> can serve as a virtualization host for running virtual
      machines. The following procedure describes steps to prepare the
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host to run the containerized KVM Server workload.
    </para>
    <procedure>
      <step>
        <para>
          Identify the KVM Server container image.
        </para>
<screen>
<prompt role="root"># </prompt>podman search kvm-server
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm-server
</screen>
      </step>
      <step>
        <para>
          Install the KVM Server workload. This pulls the container image from
          the registry and installs all <systemitem class="daemon">systemd</systemitem> services, helper scripts, and
          configuration files on the host.
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm-server:latest</screen>
      </step>
      <step>
        <para>
          Deploy the KVM Server workload.
        </para>
<screen><prompt role="root"># </prompt>kvm-server-manage enable</screen>
        <para>
          Refer to <xref linkend="reference-kvm-server-manage"/> for more info
          on managing the deployment of the KVM Server workload.
        </para>
      </step>
      <step>
        <para>
          Verify successful deployment of the KVM Server workload.
        </para>
<screen><prompt role="root"># </prompt>kvm-server-manage verify
✔ All required services are currently active
</screen>
        <important>
          <para>
            The KVM Server container does not include client tooling by
            default. Refer to <xref linkend="task-run-kvm-client-with-podman"/>
            for instructions on containerized client tooling.
          </para>
          <para>
            Alternatively, client tooling can be installed on a separate host
            for remote VM management.
          </para>
        </important>
      </step>
    </procedure>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-kvm-server-manage"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Usage of the <command>kvm-server-manage</command> script</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        The <command>kvm-server-manage</command> script is used to manage the
        KVM Server container and its dependent services on
        <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>. This article describes each subcommand of the
        script. <command>kvm-server-manage</command> is installed together with the
        KVM Server container.
      </para>
    </abstract>
  </info>
  
  <variablelist>
    <varlistentry>
      <term><command>kvm-server-manage enable</command></term>
      <listitem>
        <para>
          Disables the monolithic <systemitem class="daemon">libvirtd</systemitem> daemon (if present).
        </para>
        <para>
          Starts the KVM Server container if it is not currently running.
        </para>
        <para>
          Enables and (re)starts the modular <systemitem class="library">libvirt</systemitem> daemons that are run
          inside the KVM Server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-server-manage restart</command></term>
      <listitem>
        <para>
          Performs the same actions as <command>kvm-server-manage
          enable</command> and also restarts the KVM Server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-server-manage disable</command></term>
      <listitem>
        <para>
          Disables and stops the KVM Server container and all <systemitem class="library">libvirt</systemitem>
          daemons.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-server-manage stop</command></term>
      <listitem>
        <para>
          Stops the running KVM Server container and all <systemitem class="library">libvirt</systemitem> daemons.
        </para>
        <para>
          The services are stopped but are still enabled to run on the next
          host boot. Use <command>kvm-server-manage disable</command> to
          disable the KVM Server container on the next host boot.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-server-manage verify</command></term>
      <listitem>
        <para>
          Verifies that all required services are active. Any inactive services
          are shown and can be addressed by the administrator.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-kvm-client-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="caleb.crane@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Using the KVM Client workload</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        The KVM Client workload can be installed on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> to
        provide client tooling to interact with the KVM Server. The
        KVM Client workload provides wrappers for many virtualization tools to
        be run inside of a container.
      </para>
    </abstract>
  </info>
  
  <procedure>
    <step>
      <para>
        Identify the KVM Client workload image:
      </para>
<screen>
<prompt role="root"># </prompt>podman search kvm-client
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm-client
</screen>
    </step>
    <step>
      <para>
        Install the KVM Client workload. This pulls the container image from
        the registry and installs the included wrappers on the host.
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm-client:latest</screen>
    </step>
    <step>
      <para>
        Optionally, print the list of installed wrappers.
      </para>
<screen><prompt role="root"># </prompt>find -L /usr -xtype l -samefile $(command -v kvm-client-wrapper)</screen>
    </step>
    <step>
      <para>
        Run the desired tool from the list of installed wrappers. The wrapper
        transparently handles passing through the given executable, flags, and
        arguments into the container.
      </para>
    </step>
  </procedure>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-cockpit-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the Cockpit Web server using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        Cockpit is a tool to administer one or more hosts from one place via
        a Web user interface. Its default functionality is extended by plug-ins
        that you can install additionally. You do not need the Cockpit Web
        user interface installed on every <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host. One instance
        of the Web interface can connect to multiple hosts if they have the
        <package>alp_cockpit</package> pattern installed.
      </para>

      <para>
        This article describes how to run a containerized Cockpit Web server
        on <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> using Podman. This workload adds the Cockpit Web
        server to <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> so that you can administer the system and
        container via a user-friendly interface in your Web browser.
      </para>
    </abstract>
  </info>
  
  <tip>
    <para>
      Find more comprehensive information about managing Cockpit in <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/alp/dolomite/html/cockpit-alp-dolomite/index.html"/>.
    </para>
  </tip>
  <important>
    <para>
      You need to have the <package>alp_cockpit</package> software pattern
      installed before deploying the Cockpit container.
    </para>
  </important>
  <note>
    <para>
      An alternative way of installing and enabling the Cockpit Web server is
      described in
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://en.opensuse.org/openSUSE:ALP/Workgroups/SysMngmnt/Cockpit#Install_the_Web_Server_Via_Packages"/>.
    </para>
  </note>
  <para>
    <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> has the base part of the Cockpit component installed
    by default. It is included in the <package>alp_cockpit</package> pattern.
    To install and run Cockpit's Web interface, follow these steps:
  </para>
  <procedure>
    <step>
      <para>
        Identify the Cockpit Web server workload image:
      </para>
<screen>
<prompt role="root"># </prompt>podman search cockpit-ws
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws
</screen>
    </step>
    <step>
      <para>
        Pull the image from the registry:
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
    </step>
    <step>
      <para>
        Run the Cockpit's containerized Web server:
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name cockpit-ws run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
    </step>
    <step>
      <para>
        To run the Cockpit's Web server on each <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> boot,
        enable its service:
      </para>
<screen><prompt role="root"># </prompt>systemctl enable cockpit.service</screen>
    </step>
    <step>
      <para>
        To view the Cockpit Web user interface, point your Web browser to the
        following address and accept the self-signed certificate:
      </para>
<screen>https://<replaceable>HOSTNAME_OR_IP_OF_ALP_HOST:9090</replaceable></screen>
      <figure>
        <title>Cockpit running on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="alp-cockpit.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="alp-cockpit.png" width="100%"/>
          </imageobject>
          <textobject role="description"><phrase>Cockpit running on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase>
          </textobject>
        </mediaobject>
      </figure>
    </step>
  </procedure>
  <section xml:id="cockpit-software-updates">
    <title>Software updates</title>
    <para>
      To be able to perform transactional software updates from Cockpit,
      install the <package>cockpit-tukit</package> package:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-tukit
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Software updates in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <textobject role="description"><phrase><guimenu>Software Updates</guimenu> in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-gdm-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the GNOME Display Manager workload using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to deploy and run the GNOME Display Manager (GDM) on
        <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>. This workload runs GDM and
        basic GNOME environment. For more details, refer to
        <xref linkend="task-run-gdm-with-podman"/>.
      </para>
    </abstract>
  </info>
  
  <section xml:id="alp-starting-gdm">
    <title>Starting the GDM workload</title>
    <procedure>
      <step>
        <para>
          On the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host system, install
          <package>accountsservice</package> and
          <package>systemd-experimental</package> packages:
        </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install accountsservice systemd-experimental
<prompt role="root"># </prompt>reboot
</screen>
      </step>
      <step>
        <para>
          Verify that SELinux is configured in the
          <emphasis>permissive</emphasis> mode and enable the
          <emphasis>permissive</emphasis> mode if required:
        </para>
<screen><prompt role="root"># </prompt>setenforce 0</screen>
      </step>
      <step>
        <para>
          Identify the GDM container:
        </para>
<screen>
<prompt>&gt; </prompt>podman search gdm
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm
</screen>
      </step>
      <step>
        <para>
          Download and recreate the GDM container locally:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
      </step>
      <step>
        <para>
          Reload the affected <systemitem class="daemon">systemd</systemitem> services:
        </para>
<screen>
<prompt role="root"># </prompt>systemctl daemon-reload
<prompt role="root"># </prompt>systemctl reload dbus
<prompt role="root"># </prompt>systemctl restart accounts-daemon
</screen>
      </step>
      <step>
        <para>
          Run the GDM container.
        </para>
        <substeps>
          <step>
            <para>
              For a standalone process in a container, run:
            </para>
<screen><prompt role="root"># </prompt>systemctl start gdm.service</screen>
            <para>
              Alternatively, run the command manually:
            </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name gdm run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
          </step>
          <step>
            <para>
              For systems with <systemitem class="daemon">systemd</systemitem> running in a container, run:
            </para>
<screen><prompt role="root"># </prompt>systemctl start gdm-systemd.service</screen>
            <para>
              Alternatively, run the command manually:
            </para>
<screen><prompt role="root"># </prompt>podman container runlabel run-systemd --name gdm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          The GDM starts. After you log in, a basic GNOME environment
          opens.
        </para>
        <figure>
          <title>GNOME Settings on top of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-gdm-workload.png" width="100%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-gdm-workload.png" width="100%"/>
            </imageobject>
            <textobject role="description"><phrase>GNOME Settings on top of <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
    <tip>
      <title>Uninstalling deployed files</title>
      <para>
        If you need to clean the environment from all deployed files, run the
        following command:
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
    </tip>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-firewalld-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running <systemitem class="daemon">firewalld</systemitem> using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to run a containerized <systemitem class="daemon">firewalld</systemitem> on
        <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase> using Podman. This workload adds
        firewall capability to <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> to define the trust level of
        network connections or interfaces.
      </para>
      <important>
        <para>
          The <systemitem class="daemon">firewalld</systemitem> container needs access to the host network and needs
          to run as a privileged container. The container image uses the system
          dbus instance. Therefore, you need to install <package>dbus</package>
          and <package>polkit</package> configuration files first.
        </para>
      </important>
    </abstract>
  </info>
  
  <section xml:id="alp-running-firewalld">
    <title>Running the <systemitem class="daemon">firewalld</systemitem> workload</title>
    <procedure>
      <step>
        <para>
          Identify the <systemitem class="daemon">firewalld</systemitem> workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search firewalld
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld
</screen>
      </step>
      <step>
        <para>
          Verify that <package>firewalld</package> is not installed in the host
          system. Remove it, if necessary, and reboot the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>
          host:
        </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg remove firewalld
reboot
</screen>
      </step>
      <step>
        <para>
          Initialize the environment:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
        <para>
          The command prepares the system and creates the following files on
          the host system:
        </para>
<screen>
/etc/dbus-1/system.d/FirewallD.conf
/etc/polkit-1/actions/org.fedoraproject.FirewallD1.policy <co xml:id="alp-firewalld-polkit-policy"/>
/etc/systemd/system/firewalld.service <co xml:id="alp-firewalld-systemd"/>
/etc/default/container-firewalld
/usr/local/bin/firewall-cmd <co xml:id="alp-firewalld-config"/>
</screen>
        <calloutlist>
          <callout arearefs="alp-firewalld-polkit-policy">
            <para>
              The <package>polkit</package> policy file will only be installed
              if <package>polkit</package> itself is installed. It may be
              necessary to restart the
              <systemitem class="daemon">dbus</systemitem> and
              <systemitem class="daemon">polkit</systemitem> daemon afterwards.
            </para>
          </callout>
          <callout arearefs="alp-firewalld-systemd">
            <para>
              The <systemitem class="daemon">systemd</systemitem> service and the corresponding configuration file
              <filename>/etc/default/container-firewalld</filename> allow to
              start and stop the container using <systemitem class="daemon">systemd</systemitem> if Podman is used
              as a container manager.
            </para>
          </callout>
          <callout arearefs="alp-firewalld-config">
            <para>
              <command>/usr/local/bin/firewall-cmd</command> is a wrapper to
              call the <command>firewall-cmd</command> inside the container.
              Docker and Podman are supported.
            </para>
          </callout>
        </calloutlist>
      </step>
      <step>
        <para>
          Run the container:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
        <para>
          The command will run the container as a privileged container with the
          host network. Additionally, <filename>/etc/firewalld</filename> and
          the <systemitem class="daemon">dbus</systemitem> socket are mounted
          into the container.
        </para>
        <tip>
          <para>
            If your container manager is Podman, you can operate <systemitem class="daemon">firewalld</systemitem>
            by using its <systemitem class="daemon">systemd</systemitem> unit files, for example:
          </para>
<screen><prompt role="root"># </prompt>systemctl start firewalld</screen>
        </tip>
      </step>
      <step>
        <para>
          Optionally, you can remove the <systemitem class="daemon">firewalld</systemitem> workload and clean the
          environment from all related files. Configuration files are left on
          the system.
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
      </step>
    </procedure>
    <section xml:id="alp-manage-firewalld-instance">
      <title>Managing the <systemitem class="daemon">firewalld</systemitem> instance</title>
      <para>
        After the <systemitem class="daemon">firewalld</systemitem> container is started, you can manage its instance
        in two ways. You can manually call its client application via the
        <command>podman exec</command> command, for example:
      </para>
<screen>podman exec firewalld firewall-cmd <replaceable>OPTIONS</replaceable></screen>
      <para>
        Alternatively, you can use a shorter syntax by running the
        <command>firewall-cmd</command> wrapper script.
      </para>
    </section>
    <section xml:id="alp-firewalld-documentation">
      <title><systemitem class="daemon">firewalld</systemitem> manual pages</title>
      <para>
        To read the <systemitem class="daemon">firewalld</systemitem> manual page, run the following command:
      </para>
<screen><prompt>&gt; </prompt>podman run -i --rm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld \
 man firewalld</screen>
      <para>
        To read the <command>firewall-cmd</command> manual page, run the
        following command:
      </para>
<screen><prompt>&gt; </prompt>podman run -i --rm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld \
 man firewall-cmd</screen>
    </section>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-grafana-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the Grafana workload using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        This article describes how to run the Grafana visualization tool on
        <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>. This workload adds a Web-based
        dashboard to the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host that lets you query, monitor,
        visualize and better understand existing data residing on any client
        host.
      </para>
    </abstract>
  </info>
  
  <section xml:id="alp-starting-grafana">
    <title>Starting the Grafana workload</title>
    <para>
      This section describes how to start the Grafana workload, set up a
      client so that we can test it with real data, and configure the Grafana
      Web application to visualize the client's data.
    </para>
    <procedure>
      <step>
        <para>
          Identify the Grafana workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search grafana
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/grafana
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and prepare the environment:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/grafana:latest
</screen>
      </step>
      <step>
        <para>
          Create the <literal>grafana</literal> container from the downloaded
          image:
        </para>
<screen><prompt role="root"># </prompt>grafana-container-manage.sh create</screen>
      </step>
      <step>
        <para>
          Start the container with the Grafana server:
        </para>
<screen><prompt role="root"># </prompt>grafana-container-manage.sh start</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-setting-grafana-client">
    <title>Setting up a Grafana client</title>
    <para>
      To test Grafana, you need to set up a client that will provide real
      data to the Grafana server.
    </para>
    <procedure>
      <step>
        <para>
          Log in to the client host and install the
          <package>golang-github-prometheus-node_exporter</package> and
          <package>golang-github-prometheus-prometheus</package> packages:
        </para>
<screen><prompt role="root"># </prompt>zypper in golang-github-prometheus-node_exporter golang-github-prometheus-prometheus</screen>
        <note>
          <para>
            If your Grafana server and client hosts are virtualized by a
            KVM containerized workload, use the <option>--network</option>
            option while creating the POD because the
            <option>--publish</option> option does not work in this scenario.
            To get the IP of the VM Host Server default network, run the following
            command on the VM Host Server:
          </para>
<screen><prompt>&gt; </prompt>virsh net-dhcp-leases default</screen>
        </note>
      </step>
      <step>
        <para>
          Restart the Prometheus services on the client host:
        </para>
<screen>
<prompt role="root"># </prompt>systemctl restart prometheus-node_exporter.service
<prompt role="root"># </prompt>systemctl restart prometheus
</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-setting-grafana-web">
    <title>Configuring the Grafana Web application</title>
    <para>
      To configure a data source for the Grafana Web dashboard, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          Open the Grafana Web page that is running on port 3000 on the
          <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host where the Grafana workload is running, for
          example:
        </para>
<screen><prompt>&gt; </prompt>firefox http://<replaceable>ALP_HOST_IP_ADDRESS</replaceable>:3000</screen>
      </step>
      <step>
        <para>
          Log in to Grafana. The default user name and password are both set
          to <literal>admin</literal>. After logging in, enter a new password.
        </para>
      </step>
      <step>
        <para>
          Add the Prometheus data source provided by the client. In the left
          panel, hover your mouse over the gear icon and select <guimenu>Data
          sources</guimenu>.
        </para>
        <figure>
          <title>Grafana data sources</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-data-sources.png" width="25%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-data-sources.png" width="25%"/>
            </imageobject>
            <textobject role="description"><phrase>Grafana data sources</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Click <guimenu>Add data source</guimenu> and select
          <guimenu>Prometheus</guimenu>. Fill the <guimenu>URL</guimenu>
          field with the URL of the client where the Prometheus service runs
          on port 9090, for example:
        </para>
        <figure>
          <title>Prometheus URL configuration in Grafana</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-prometheus-url.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-prometheus-url.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Prometheus URL configuration in Grafana</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <para>
          Confirm with <guimenu>Save &amp; test</guimenu>
        </para>
      </step>
      <step>
        <para>
          Create a dashboard based on Prometheus data. Hover your mouse over
          the plus sign in the left panel and select <guimenu>Import</guimenu>.
        </para>
        <figure>
          <title>Creating a Grafana dashboard</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-pimport-dashboard.png" width="25%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-pimport-dashboard.png" width="25%"/>
            </imageobject>
            <textobject role="description"><phrase>Creating Grafana dashboard</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Enter <literal>405</literal> as the dashboard ID and confirm with
          <guimenu>Load</guimenu>.
        </para>
      </step>
      <step>
        <para>
          From the <guimenu>Prometheus</guimenu> drop-down list at the bottom,
          select the data source you have already created. Confirm with
          <guimenu>Import</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Grafana shows your newly created dashboard.
        </para>
        <figure>
          <title>New Grafana dashboard</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-dashboard.png" width="100%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-dashboard.png" width="100%"/>
            </imageobject>
            <textobject role="description"><phrase>New Grafana dashboard</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-grafana-container-manage"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Usage of the <command>grafana-container-manage.sh</command> script</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        The <command>grafana-container-manage.sh</command> script is used to
        manage the Grafana container on <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>.
        This article lists each subcommand of the script and describes its
        purpose.
      </para>
    </abstract>
  </info>
  
  <variablelist>
    <varlistentry>
      <term><command>grafana-container-manage.sh create</command></term>
      <listitem>
        <para>
          Pulls the Grafana image and creates the corresponding container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh install</command></term>
      <listitem>
        <para>
          Installs additional files that are required to manage the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh start</command></term>
      <listitem>
        <para>
          Starts the container called <literal>grafana</literal>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh uninstall</command></term>
      <listitem>
        <para>
          Uninstalls all files on the host that were required to manage the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh stop</command></term>
      <listitem>
        <para>
          Stops the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh rm</command></term>
      <listitem>
        <para>
          Deletes the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh rmcache</command></term>
      <listitem>
        <para>
          Removes the container image in cache.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh </command></term>
      <listitem>
        <para>
          Runs the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh bash</command></term>
      <listitem>
        <para>
          Runs the <command>bash</command> shell inside the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh logs</command></term>
      <listitem>
        <para>
          Displays log messages of the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-neuvector-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the NeuVector workload using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        NeuVector is a powerful container security platform that includes
        end-to-end vulnerability scanning and complete runtime protection for
        containers, pods and hosts. This article describes how to run
        NeuVector on <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>.
      </para>
      <important>
        <para>
          NeuVector requires SELinux to be set into the
          <emphasis>permissive</emphasis> mode by running the following
          command:
        </para>
<screen><prompt role="root"># </prompt>setenforce 0</screen>
        <para>
          You can find more details about SELinux in
          <xref linkend="alp-post-deploy-selinux"/>.
        </para>
      </important>
    </abstract>
  </info>
  
  <section xml:id="starting-neuvector">
    <title>Starting NeuVector</title>
    <procedure>
      <step>
        <para>
          Identify the NeuVector workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search neuvector
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/neuvector
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and install <systemitem class="daemon">systemd</systemitem> services to
          handle NeuVector container start-up and shutdown:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
  registry.opensuse.org/suse/alp/workloads/bci_containerfiles/suse/alp/workloads/neuvector-demo:latest</screen>
      </step>
      <step>
        <para>
          Start the NeuVector service:
        </para>
<screen><prompt role="root"># </prompt><command>systemctl start neuvector.service</command></screen>
      </step>
      <step>
        <para>
          Connect to NeuVector in the Web browser by entering the following
          URL:
        </para>
<screen>https://<replaceable>HOST_RUNNING_NEUVECTOR_SERVICE</replaceable>:8443</screen>
        <para>
          You need to accept the warning about the self-signed SSL certificate
          and log in with the following default credentials:
          <literal>admin</literal> / <literal>admin</literal>.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="uninstalling-neuvector">
    <title>Uninstalling NeuVector</title>
    <para>
      To uninstall NeuVector, run the following command:
    </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
  registry.opensuse.org/suse/alp/workloads/bci_containerfiles/suse/alp/workloads/neuvector-demo:latest</screen>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-ansible-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the Ansible workload using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        Ansible is a suite of tools for managing and provisioning data
        centers via definition files. This article describes how to run
        Ansible on <phrase><phrase os="alp-dolomite">SUSE <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase></phrase></phrase>.
      </para>
      <important>
        <para>
          <package>python3-lxml</package> and <package>python3-rpm</package>
          packages are required for Ansible to interact with <systemitem class="library">libvirt</systemitem> and
          gather package facts. The <package>kernel-default-base</package>
          package does not contain the required drivers for multiple NetworkManager or
          <command>nmcli</command> operations, such as creating bonded
          interfaces. Replace it with <package>kernel-default</package>:
        </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install python3-rpm python3-lxml kernel-default -kernel-default-base</command>
<prompt role="root"># </prompt><command>shutdown -r now</command></screen>
      </important>
    </abstract>
  </info>
  
  <section xml:id="starting-ansible">
    <title>Installing Ansible commands</title>
    <procedure>
      <step>
        <para>
          Identify the Ansible workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search ansible
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/ansible
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and install Ansible commands.
        </para>
        <substeps>
          <step>
            <para>
              For <systemitem class="username">root</systemitem>, the Ansible commands are placed in the
              <filename>/usr/local/bin</filename> directory. Run the following
              command to install Ansible commands for <systemitem class="username">root</systemitem>:
            </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
  registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/ansible:latest</screen>
            <tip>
              <title>Example Ansible playbooks</title>
              <para>
                If you installed the Ansible commands as <systemitem class="username">root</systemitem>, you can
                find example playbooks in the
                <filename>/usr/local/share/ansible-container/examples</filename>
                directory.
              </para>
            </tip>
          </step>
          <step>
            <para>
              For non-<systemitem class="username">root</systemitem>, the Ansible commands are placed in the
              <filename>bin/</filename> subdirectory of the current working
              directory. When installing them in your home directory, verify
              that the <filename>bin/</filename> subdirectory exists. Run the
              following commands to install Ansible commands in your home
              directory:
            </para>
<screen><prompt>&gt; </prompt>cd &amp;&amp; podman container runlabel user-install \
  registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/ansible:latest</screen>
          </step>
        </substeps>
      </step>
    </procedure>
    <para>
      After the successful installation of Ansible, the following commands
      are available:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          ansible
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-community
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-config
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-connection
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-console
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-doc
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-galaxy
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-inventory
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-lint
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-playbook
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-pull
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-test
        </para>
      </listitem>
      <listitem>
        <para>
          ansible-vault
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="uninstalling-ansible">
    <title>Uninstalling Ansible</title>
    <para>
      To uninstall Ansible as <systemitem class="username">root</systemitem>, run the following command:
    </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
  registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/ansible:latest</screen>
    <para>
      To uninstall Ansible as non-<systemitem class="username">root</systemitem>, run the following commands:
    </para>
<screen><prompt>&gt; </prompt>cd &amp;&amp; podman container runlabel user-uninstall \
  registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/ansible:latest</screen>
  </section>
  <section xml:id="ansible-operation-ssh">
    <title>Operation via SSH</title>
    <para>
      Because Ansible is running inside a container, the default localhost
      environment is the container itself and not the system hosting the
      container instance. Therefore, any changes made to the localhost
      environment are made to the container and are lost when the container
      exits.
    </para>
    <para>
      Instead, Ansible can be targeted via an SSH connection at the host that
      is running the container, namely
      <literal>host.containers.internal</literal>, using an Ansible inventory
      similar to the following example:
    </para>
<screen>
alhost_group:
  hosts:
    alphost:
      ansible_host: host.containers.internal
      ansible_python_interpreter: /usr/bin/python3</screen>
    <para>
      An equivalent <literal>alphost</literal> default inventory item has also
      been added to the container's <filename>/etc/ansible/hosts</filename>
      inventory, which can be used by the <command>ansible</command>
      command-line tool. For example, to run the <literal>setup</literal>
      module to collect and show system facts from the
      <literal>alphost</literal>, run a command similar to the following:
    </para>
<screen><prompt role="root"># </prompt><command>ansible alphost -m setup</command>
  alphost | SUCCESS =&gt; {
    "ansible_facts": {
[...]
    },
    "changed": false
}</screen>
    <tip>
      <para>
        The inventory record may also contain other hosts to be managed.
      </para>
    </tip>
    <important>
      <title>Set up SSH keys</title>
      <para>
        The container must be able to connect to the system being managed. The
        following conditions must be fulfilled:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            The system supports SSH access.
          </para>
        </listitem>
        <listitem>
          <para>
            SSH keys are created using <command>ssh-keygen</command>.
          </para>
        </listitem>
        <listitem>
          <para>
            The public SSH key is included in the
            <filename>.ssh/authorized_keys</filename> file for the target user.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        The preferred method is to use a non-<systemitem class="username">root</systemitem> account that has
        passwordless <command>sudo</command> privileges. Any operations in
        Ansible playbooks that require system privileges need to use the
        <literal>become: true</literal> setting.
      </para>
      <para>
        Note that the SSH access can be validated with the <command>ssh
        localhost</command> command.
      </para>
    </important>
  </section>
<section role="reference" xml:lang="en" version="5.2" xml:id="reference-ansible-playbook-examples"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Examples of Ansible playbooks</title>
  </info>
  
  <section xml:id="reference-ansible-introduction">
    <title>Introduction</title>
    <para>
      On the <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> system where the Ansible workload container
      has been installed using the <literal>install</literal> runlabel (refer
      to <xref linkend="starting-ansible"/> for more details), the examples are
      available in
      <filename>/usr/local/share/ansible-container/examples/ansible</filename>.
    </para>
  </section>
  <section xml:id="alp-ansible-playbook-test">
    <title>Simple Ansible test</title>
    <para>
      The <filename>playbook.yml</filename> playbook tests several common
      Ansible operations, such as gathering facts and testing for installed
      packages. To invoke the play, change to the directory
      <filename>/usr/local/share/ansible-container/examples/ansible</filename>
      and enter the following command:
    </para>
<screen><prompt>&gt; </prompt><command>ansible-playbook playbook.yml</command>
...
PLAY RECAP *********************************************************************
alphost    : ok=8 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</screen>
  </section>
  <section xml:id="alp-ansible-networking-example">
    <title>Drive <command>nmcli</command> to change system networking</title>
    <para>
      The <filename>network.yml</filename> playbook uses the
      <literal>community.general.nmcli</literal> plugin to test common network
      operations, such as assigning static IP addresses to NICs or creating
      bonded interfaces.
    </para>
    <para>
      The NICs, IP addresses, bond names, and bonded NICs are defined in the
      <literal>vars</literal> section of the <filename>network.yml</filename>
      file. Update it to reflect the current environment. To invoke the play,
      change to the directory
      <filename>/usr/local/share/ansible-container/examples/ansible</filename>
      and enter the following command:
    </para>
<screen><prompt>&gt; </prompt><command>ansible-playbook network.yml</command>
...
ASK [Ping test Bond IPs] ************************************************************************************************
ok: [alphost] =&gt; (item={'name': 'bondcon0', 'ifname': 'bond0', 'ip4': '192.168.181.10/24', 'gw4': '192.168.181.2', 'mode': 'active-backup'})
ok: [alphost] =&gt; (item={'name': 'bondcon1', 'ifname': 'bond1', 'ip4': '192.168.181.11/24', 'gw4': '192.168.181.2', 'mode': 'balance-alb'})

TASK [Ping test static nics IPs] *****************************************************************************************
ok: [alphost] =&gt; (item={'name': 'enp2s0', 'ifname': 'enp2s0', 'ip4': '192.168.181.3/24', 'gw4': '192.168.181.2', 'dns4': ['8.8.8.8']})
ok: [alphost] =&gt; (item={'name': 'enp3s0', 'ifname': 'enp3s0', 'ip4': '192.168.181.4/24', 'gw4': '192.168.181.2', 'dns4': ['8.8.8.8']})

PLAY RECAP ***************************************************************************************************************
alphost                    : ok=9    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0</screen>
  </section>
  <section xml:id="alp-ansible-libvirt-host">
    <title>Set up <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> as a <systemitem class="library">libvirt</systemitem> host</title>
    <para>
      The <filename>setup_libvirt_host.yml</filename> playbook installs the
      <literal>kvm-container</literal> workload and enables the <systemitem class="daemon">libvirtd</systemitem>
      <systemitem class="daemon">systemd</systemitem> service. To invoke the play, change to the directory
      <filename>/usr/local/share/ansible-container/examples/ansible</filename>
      and enter the following command:
    </para>
<screen><prompt>&gt; </prompt><command>ansible-playbook setup_libvirt_host.yml</command>
...
PLAY RECAP *********************************************************************
alphost    : ok=9 changed=2 unreachable=0 failed=0 skipped=4 rescued=0 ignored=0

<prompt>&gt; </prompt><command>sudo</command> /usr/local/bin/virsh list --all
using /etc/kvm-container.conf as configuration file
+ podman exec -ti libvirtd virsh list --all
Authorization not available.
Check if polkit service is running or see debug message for more information.</screen>
    <note>
      <para>
        If the required kernel and supporting packages are not already
        installed, a reboot is required to complete the installation of missing
        packages. Follow the directions generated by the playbook. After the
        reboot has completed successfully, rerun the playbook to finish the
        setup of the <systemitem class="daemon">libvirtd</systemitem> service.
      </para>
    </note>
  </section>
  <section xml:id="alp-ansible-tumbleweed-vm">
    <title>Create an openSUSE Tumbleweed appliance VM</title>
    <para>
      The
      <alt>create_tumbleweed_vm.yml</alt>
      playbook creates and starts a <systemitem class="library">libvirt</systemitem> managed VM called
      <literal>tumbleweed</literal> that is based on the latest available openSUSE Tumbleweed
      appliance VM image.
    </para>
    <para>
      It uses the <filename>setup_libvirt_host.yml</filename> playbook (see
      <xref linkend="alp-ansible-libvirt-host"/>) to ensure that the
      <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> host is ready to manage VMs before creating the new
      one. It may fail prompting you to reboot before running the playbook
      again to finish setting up <systemitem class="library">libvirt</systemitem> and creating the VM.
    </para>
    <para>
      To invoke the play, change to the directory
      <filename>/usr/local/share/ansible-container/examples/ansible</filename>
      and enter the following command:
    </para>
<screen><prompt>&gt; </prompt><command>ansible-playbook create_tumbleweed_vm.yml</command>
...
TASK [Query list of libvirt VMs] ***********************************************
ok: [alphost]

TASK [Show that Tumbleweed appliance has been created] *************************
ok: [alphost] =&gt; {
    "msg": "Running VMs: tumbleweed"
}

PLAY RECAP *********************************************************************</screen>
  </section>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="kea-dhcp-running-with-podman"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Running the Kea DHCP server using Podman</title>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">
      <para>
        Kea is an open-source DHCP server that supports both DHCPv4 and
        DHCPv6 protocols. It provides IPv6 prefix delegation, host reservations
        optionally stored in a database, PXE boot, high-availability setup and
        other features.
      </para>
    </abstract>
  </info>
  
  <section xml:id="kea-dhcp-run-procedure">
    <title>Deploying and running the Kea workload</title>
    <procedure>
      <step>
        <para>
          Identify the Kea DHCP server container image:
        </para>
<screen><prompt role="root"># </prompt><command>podman search kea</command>
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry:
        </para>
<screen><prompt role="root"># </prompt><command>podman pull \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
      </step>
      <step>
        <para>
          Install all required parts of the Kea workload:
        </para>
<screen><prompt role="root"># </prompt><command>podman container runlabel install \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
        <para>
          The previous command installs:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              Default configuration files in the <filename>/etc/kea</filename>
              directory
            </para>
          </listitem>
          <listitem>
            <para>
              The <command>keactrl</command> wrapper in the
              <filename>/usr/local/bin</filename> directory
            </para>
          </listitem>
          <listitem>
            <para>
              <systemitem class="daemon">systemd</systemitem> service files for the dhcp4 and dhcp6 containers in the
              <filename>/etc/systemd/system/</filename> directory
            </para>
          </listitem>
        </itemizedlist>
      </step>
      <step>
        <para>
          Run the Kea DHCP server. You can run it either using <systemitem class="daemon">systemd</systemitem> unit
          files, or manually.
        </para>
        <tip>
          <para>
            To run DHCP server with <systemitem class="daemon">firewalld</systemitem> active, you need to add
            exception rules based on the version of DHCP you're using.
          </para>
          <para>
            For DHCPv4:
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> firewall-cmd --add-service=dhcp</screen>
          <para>
            For DHCPv6:
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> firewall-cmd --add-service=dhcpv6</screen>
        </tip>
        <substeps>
          <step>
            <para>
              To run Kea as a <systemitem class="daemon">systemd</systemitem> service, use one of the following
              commands:
            </para>
<screen><prompt role="root"># </prompt><command>systemctl start kea-dhcp4.service</command></screen>
            <para>
              Or, for DHCPv6:
            </para>
<screen><prompt role="root"># </prompt><command>systemctl start kea-dhcp6.service</command></screen>
          </step>
          <step>
            <para>
              To run Kea manually, use one of the following commands:
            </para>
<screen><prompt role="root"># </prompt><command>podman container runlabel run \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
            <para>
              Or, for DHCPv6:
            </para>
<screen><prompt role="root"># </prompt><command>podman container runlabel run_dhcp6 \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          Optionally, you can uninstall the Kea workload. The following
          command removes all Kea-related files except for the configuration
          directory and its content:
        </para>
<screen><prompt role="root"># </prompt><command>podman container runlabel uninstall \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
        <tip>
          <para>
            The <literal>purge</literal> runlabel removes the Kea
            configuration directory <filename>/etc/kea</filename> but leaves
            the rest of Kea deployment in place:
          </para>
<screen><prompt role="root"># </prompt><command>podman container runlabel purge \
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kea:latest</command></screen>
        </tip>
      </step>
    </procedure>
  </section>
  <section xml:id="kea-dhcp-configuration-files">
    <title>Configuration files</title>
    <para>
      The Kea configuration files—<filename>kea-dhcp4.conf</filename>
      and <filename>kea-dhcp6.conf</filename>—are located in the
      <filename>/etc/kea</filename> directory. They include the default
      configuration. You can find detailed information about configuring the
      DHCP server in the official documentation at
      <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://kea.readthedocs.io/"/>.
    </para>
    <tip>
      <para>
        If you modify configuration files, run <command>keactrl
        reload</command> to apply them to running servers.
      </para>
    </tip>
  </section>
  <section xml:id="kea-dhcp-keactrl">
    <title>The <command>keactrl</command> wrapper</title>
    <para>
      The installed <command>keactrl</command> wrapper uses the original
      <command>keactrl</command> tool to send commands to deployed containers.
      It uses the same options as the original tool with one exception: the
      <option>-s</option> option is adjusted to send commands to the DHCPv4
      (<option>-s dhcp4</option>) or DHCPv6 (<option>-s dhcp6</option>). If
      <option>-s</option> is not specified, <command>keactrl</command> sends
      commands to both servers if they are started.
    </para>
  </section>
</section><section role="glue" xml:lang="en" version="5.2" xml:id="glue-alp-workloads-more-info"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">For more information</title>
  </info>
  
  <itemizedlist>
    <listitem>
      <para>
        The general concept of Podman is described in
        <xref linkend="concept-containers-podman"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Podman usage is explained in
        <xref linkend="reference-podman-usage"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        YaST is generally described in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-yast-gui.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        <systemitem class="library">libvirt</systemitem> virtualization is described in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/part-virt-libvirt.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Installing software packages and patterns is detailed in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Details about the usage of the <command>kvm-server-manage</command>
        script are described in <xref linkend="reference-kvm-server-manage"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Enabling KVM nested virtualization is described in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-vt-installation.html#sec-vt-installation-nested-vms"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Managing <systemitem class="daemon">systemd</systemitem> services is described in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/linux/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Find more details about Grafana at its home page
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://grafana.com/grafana/"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        The NeuVector documentation is at
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://open-docs.neuvector.com/"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        The Ansible documentation is at
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://docs.ansible.com/?extIdCarryOver=true&amp;sc_cid=701f2000001OH7YAAW"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        On <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> with the Ansible workload using the
        <literal>install</literal> runlabel, the examples are available in the
        <filename>/usr/local/share/ansible-container/examples/ansible</filename>
        directory.
      </para>
    </listitem>
  </itemizedlist>
</section></chapter>
  <chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-virt-scenario"><info>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Creating customized VMs using <command>virt-scenario</command></title>
  </info>
  
  <para>
    <command>virt-scenario</command> is a tool that helps you create virtual machines (VM)
    suitable for a specific scenario. It provides predefined
    <emphasis>profiles</emphasis> that include optimal settings for each
    scenario. You can override settings that are common to all profiles.
  </para>
  <important>
    <para>
      Although <command>virt-scenario</command> generally provides the best possible
      configuration for a specific scenario, this cannot be guaranteed because
      each environment may have specific requirements.
    </para>
  </important>
  <section xml:id="how-it-works-virt-scenario">
    <title>How does <command>virt-scenario</command> work?</title>
    <para>
      Interactive <command>virt-scenario</command> script creates a <systemitem class="library">libvirt</systemitem> XML configuration
      file for a VM based on the following:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          selected basic profile
        </para>
      </listitem>
      <listitem>
        <para>
          overridden values for common parameters
        </para>
      </listitem>
      <listitem>
        <para>
          parameters that you specify interactively
        </para>
      </listitem>
    </itemizedlist>
    <para>
      After the configuration is validated, <command>virt-scenario</command> adjusts the
      VM Host Server system and creates the image file for the VM Guest. You can
      then operate the VM using standard <systemitem class="library">libvirt</systemitem> commands.
    </para>
  </section>
  <section xml:id="benefits-virt-scenario">
    <title>Benefits of using <command>virt-scenario</command></title>
    <itemizedlist>
      <listitem>
        <para>
          Creating virtual machines is fast and simple. <command>virt-scenario</command> leaves
          all the virtualization complexity aside. You can focus on basic
          features only.
        </para>
      </listitem>
      <listitem>
        <para>
          Fine-tuned profiles already offer optimal settings for specified
          scenarios. You do not have to search and copy them to each virtual
          machine of the same type, just use the same profile.
        </para>
      </listitem>
      <listitem>
        <para>
          The override mechanism lets you specify other values than the
          profile's default for selected options. This way, you can customize
          virtual machines to your needs.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-virt-scenario-create-vm"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Creating VMs</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    The <command>virt-scenario</command> command presents an interactive shell where you
    optionally specify configuration parameters and finally select the target
    scenario. Its welcome screen shows categories with available commands.
  </para>
  <figure>
    <title><command>virt-scenario</command> welcome screen</title>
    <mediaobject>
      <imageobject role="fo">
        <imagedata fileref="alp-virt-scenario-welcome.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
        <imagedata fileref="alp-virt-scenario-welcome.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase><command>virt-scenario</command> welcome screen</phrase>
      </textobject>
    </mediaobject>
  </figure>
  <tip>
    <para>
      Each command has a built-in description of its usage. Enter <command>help
      <replaceable>COMMAND_NAME</replaceable></command> to view it on the
      screen.
    </para>
  </tip>
  <warning>
    <title>Unsafe <option>force_sev</option> option</title>
    <para>
      The <option>force_sev</option> option forces the extraction of the
      Platform Diffie-Hellman key (PDH) on the current AMD SEV system. The PDH
      file is used to negotiate a master secret between the SEV firmware and
      the external entities. This file must be stored in a secure place, and
      this option is only provided for <emphasis>testing</emphasis> purposes.
    </para>
  </warning>
  <para>
    After you finish optional configuration steps, enter the name of one of the
    scenarios—<literal>computation</literal>, <literal>desktop</literal>
    or <literal>securevm</literal>. <command>virt-scenario</command> then compiles all the
    configuration, prepares the VM Host Server, and saves the <systemitem class="library">libvirt</systemitem> XML of the
    new VM.
  </para>
  <section xml:id="requirements-virt-scenario-create-vm" os="alp-dolomite">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Running <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> with the KVM workload deployed. Refer to
          <xref linkend="task-run-kvm-with-podman" os="alp-dolomite"/>
          for detailed steps.
        </para>
      </listitem>
      <listitem>
        <para>
          Secure virtual machines are supported only on an AMD processor that
          supports AMD SEV or SEV-ES technology. For more information about
          SUSE and AMD SEV, refer to
          <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/sles/single-html/SLES-amd-sev/"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="virt-scenario-override">
    <title>Overriding default scenario settings</title>
    <para>
      On each run, <command>virt-scenario</command> reads settings for a new VM from the
      <filename>/etc/virt-scenario/virtscenario.yaml</filename> file. Change
      settings in this file to affect <emphasis>all</emphasis> VMs created by
      <command>virt-scenario</command>.
    </para>
    <tip>
      <para>
        Always backup <filename>/etc/virt-scenario/virtscenario.yaml</filename>
        before modifying it as incorrect parameters may lead to an invalid VM
        configuration.
      </para>
    </tip>
    <para>
      To override settings for specific VMs only, copy
      <filename>/etc/virt-scenario/virtscenario.yaml</filename> to a different
      location and modify the settings that you need to override. On next
      <command>virt-scenario</command> run, specify the path to the new configuration file with
      the <command>conf</command> command, for example:
    </para>
<screen>conf /home/tux/virt-scenarios/my-overriden-scenario.yaml</screen>
    <important>
      <para>
        In the overriding <filename>virtscenario.yaml</filename> file, you need
        to include <emphasis>all</emphasis> available settings from the
        original <filename>virtscenario.yaml</filename> file, not only the
        modified settings.
      </para>
    </important>
    <warning>
      <para>
        Never change section names in the
        <filename>/etc/virt-scenario/virtscenario.yaml</filename> file or its
        overriding copies.
      </para>
    </warning>
    <para>
      The following is an example of
      <filename>/etc/virt-scenario/virtscenario.yaml</filename>:
    </para>
<screen>
config:
  - path: /etc/virt-scenario
  - vm-config-store: ~/.local/virtscenario/
emulator:
  - emulator: /usr/bin/qemu-system-x86_64
input:
  - keyboard: virtio
  - mouse: virtio
architecture:
  - arch: x86_64
STORAGE_DATA:
# some options are only available with qcow2 format and
# will be ignored in case of any other image format
  - disk_type: file
  - disk_cache: none
  - disk_target: vda
  - disk_bus: virtio
  - path: /var/livirt/images
  - format: qcow2
# host side: qemu-img creation options (-o), qemu-img --help
  - unit: G
  - capacity: 20
  - cluster_size: 1024k
  - lazy_refcounts: on
preallocation: full
  - preallocation: off
  - compression_type: zlib
  - encryption: off
host_filesystem:
  - fmode: 644
  - dmode: 755
  - source_dir: /tmp
  - target_dir: /tmp/host
</screen>
  </section>
  <section xml:id="virt-scenario-modes">
    <title>Specifying <command>virt-scenario</command> mode</title>
    <para>
      By default, <command>virt-scenario</command> creates the <systemitem class="library">libvirt</systemitem> XML configuration of
      the new guest and adjusts the VM Host Server. You can instruct <command>virt-scenario</command>
      to perform only part of the task. After entering the
      <command>mode</command> command, you can enter one of the following:
    </para>
    <variablelist>
      <varlistentry>
        <term>guest</term>
        <listitem>
          <para>
            Creates only the <systemitem class="library">libvirt</systemitem> XML configuration of the guest.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>host</term>
        <listitem>
          <para>
            Prepares the VM Host Server system only.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>both</term>
        <listitem>
          <para>
            Creates the guest configuration and prepares the host. This is the
            default mode.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
<section role="reference" xml:lang="en" version="5.2" xml:id="reference-virt-scenario-interactive-commands"><info>
            <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Interactive commands</title>
            <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
              <para>
                You can use the following commands when configuring a new VM
                using the <command>virt-scenario</command> interactive shell.
              </para>
            </abstract>
          <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <tip>
    <para>
      Each command has a built-in description of its usage. Enter <command>help
      <replaceable>COMMAND_NAME</replaceable></command> to view it on the
      screen.
    </para>
  </tip>
  <variablelist>
    <title>Hypervisor configuration</title>
    <varlistentry>
      <term>hvconf</term>
      <listitem>
        <para>
          Loads hypervisor configuration.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>hvselect</term>
      <listitem>
        <para>
          Sets the hypervisor for which VMs are configured.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>hvlist</term>
      <listitem>
        <para>
          Lists available hypervisors.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>overwrite</term>
      <listitem>
        <para>
          Forces overwriting previous configuration.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <variablelist>
    <title>Guest configuration</title>
    <varlistentry>
      <term>name</term>
      <listitem>
        <para>
          Defines the name of the VM.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>vcpu</term>
      <listitem>
        <para>
          Specifies the number of virtual CPUs.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>memory</term>
      <listitem>
        <para>
          Specifies the memory size (in GiB).
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>machine</term>
      <listitem>
        <para>
          Selects the machine type.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>bootdev</term>
      <listitem>
        <para>
          Selects the boot device.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>diskpath</term>
      <listitem>
        <para>
          Specifies the directory where to store the VM disk image.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>conf</term>
      <listitem>
        <para>
          Specifies the path to the custom <command>virtscenario.yaml</command>
          file.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>cdrom</term>
      <listitem>
        <para>
          Specifies the path to the CD/DVD installation media.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <variablelist>
    <title>Generate VM configuration</title>
    <varlistentry>
      <term>computation</term>
      <listitem>
        <para>
          Creates a <systemitem class="library">libvirt</systemitem> XML configuration and VM Host Server adjustments for
          the computation scenario.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>desktop</term>
      <listitem>
        <para>
          Creates a <systemitem class="library">libvirt</systemitem> XML configuration and VM Host Server adjustments for
          the desktop scenario.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>securevm</term>
      <listitem>
        <para>
          Creates a <systemitem class="library">libvirt</systemitem> XML configuration and VM Host Server adjustments for
          the secure VM scenario.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section></section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-virt-scenarios"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Predefined scenarios</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para>
              When creating a VM, you can specify one of the following
              scenarios:
            </para>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <variablelist>
    <varlistentry>
      <term>securevm</term>
      <listitem>
        <para>
          Selecting this scenario results in an encrypted VM image with a high
          level of isolation and data security.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>computation</term>
      <listitem>
        <para>
          This scenario puts emphasis on the high performance of the resulting
          VM.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>desktop</term>
      <listitem>
        <para>
          The result of this scenario is a VM suitable for running desktop
          applications.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    The following tables show default settings for each scenario:
  </para>
  <table>
    <title>Default storage settings</title>
    <tgroup cols="4">
      <colspec colname="c1" align="left"/>
      <colspec colname="c2" align="center"/>
      <colspec colname="c3" align="center"/>
      <colspec colname="c4" align="center"/>
      <thead>
        <row>
          <entry>
            <para>
              Setting
            </para>
          </entry>
          <entry>
            <para>
              securevm
            </para>
          </entry>
          <entry>
            <para>
              computation
            </para>
          </entry>
          <entry>
            <para>
              desktop
            </para>
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            <para>
              preallocation
            </para>
          </entry>
          <entry>
            <para>
              metadata
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              metadata
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              encryption
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              disk_cache
            </para>
          </entry>
          <entry>
            <para>
              writethrough
            </para>
          </entry>
          <entry>
            <para>
              unsafe
            </para>
          </entry>
          <entry>
            <para>
              none
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              lazy_refcounts
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              format
            </para>
          </entry>
          <entry>
            <para>
              qcow2
            </para>
          </entry>
          <entry>
            <para>
              raw
            </para>
          </entry>
          <entry>
            <para>
              qcow2
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              disk bus
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              capacity
            </para>
          </entry>
          <entry>
            <para>
              20G
            </para>
          </entry>
          <entry>
            <para>
              20G
            </para>
          </entry>
          <entry>
            <para>
              20G
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              cluster_size
            </para>
          </entry>
          <entry>
            <para>
              1024k
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              1024k
            </para>
          </entry>
        </row>
      </tbody>
    </tgroup>
  </table>
  <table>
    <title>Default host settings</title>
    <tgroup cols="4">
      <colspec colname="c1" align="left"/>
      <colspec colname="c2" align="center"/>
      <colspec colname="c3" align="center"/>
      <colspec colname="c4" align="center"/>
      <thead>
        <row>
          <entry>
            <para>
              Setting
            </para>
          </entry>
          <entry>
            <para>
              securevm
            </para>
          </entry>
          <entry>
            <para>
              computation
            </para>
          </entry>
          <entry>
            <para>
              desktop
            </para>
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            <para>
              Transparent HugePages
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              KSM
            </para>
          </entry>
          <entry>
            <para>
              disable
            </para>
          </entry>
          <entry>
            <para>
              enable
            </para>
          </entry>
          <entry>
            <para>
              enable
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              KSM merge across
            </para>
          </entry>
          <entry>
            <para>
              disable
            </para>
          </entry>
          <entry>
            <para>
              enable
            </para>
          </entry>
          <entry>
            <para>
              enable
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              swappiness
            </para>
          </entry>
          <entry>
            <para>
              0
            </para>
          </entry>
          <entry>
            <para>
              0
            </para>
          </entry>
          <entry>
            <para>
              35
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              IO Scheduler
            </para>
          </entry>
          <entry>
            <para>
              bfq
            </para>
          </entry>
          <entry>
            <para>
              mq-deadline
            </para>
          </entry>
          <entry>
            <para>
              mq-deadline
            </para>
          </entry>
        </row>
      </tbody>
    </tgroup>
  </table>
  <table>
    <title>Default guest settings</title>
    <tgroup cols="4">
      <colspec colname="c1" align="left"/>
      <colspec colname="c3" align="center"/>
      <colspec colname="c2" align="center"/>
      <colspec colname="c4" align="center"/>
      <thead>
        <row>
          <entry>
            <para>
              Setting
            </para>
          </entry>
          <entry>
            <para>
              securevm
            </para>
          </entry>
          <entry>
            <para>
              computation
            </para>
          </entry>
          <entry>
            <para>
              desktop
            </para>
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            <para>
              CPU migratable
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              machine
            </para>
          </entry>
          <entry>
            <para>
              pc-q35-6.2
            </para>
          </entry>
          <entry>
            <para>
              pc-q35-6.2
            </para>
          </entry>
          <entry>
            <para>
              pc-q35-6.2
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              watchdog
            </para>
          </entry>
          <entry>
            <para>
              none
            </para>
          </entry>
          <entry>
            <para>
              i6300esb poweroff
            </para>
          </entry>
          <entry>
            <para>
              none
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              boot UEFI
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              vTPM
            </para>
          </entry>
          <entry>
            <para>
              tpm-crb 2.0
            </para>
          </entry>
          <entry>
            <para>
              none
            </para>
          </entry>
          <entry>
            <para>
              none
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              iothreads
            </para>
          </entry>
          <entry>
            <para>
              disable
            </para>
          </entry>
          <entry>
            <para>
              4
            </para>
          </entry>
          <entry>
            <para>
              4
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              video
            </para>
          </entry>
          <entry>
            <para>
              qxl
            </para>
          </entry>
          <entry>
            <para>
              qxl
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              network
            </para>
          </entry>
          <entry>
            <para>
              e1000
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
          <entry>
            <para>
              e1000
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              keyboard
            </para>
          </entry>
          <entry>
            <para>
              ps2
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              mouse
            </para>
          </entry>
          <entry>
            <para>
              disable
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
          <entry>
            <para>
              virtio
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              on_poweroff
            </para>
          </entry>
          <entry>
            <para>
              destroy
            </para>
          </entry>
          <entry>
            <para>
              restart
            </para>
          </entry>
          <entry>
            <para>
              destroy
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              on_reboot
            </para>
          </entry>
          <entry>
            <para>
              destroy
            </para>
          </entry>
          <entry>
            <para>
              restart
            </para>
          </entry>
          <entry>
            <para>
              restart
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              on_crash
            </para>
          </entry>
          <entry>
            <para>
              destroy
            </para>
          </entry>
          <entry>
            <para>
              restart
            </para>
          </entry>
          <entry>
            <para>
              destroy
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              suspend_to_mem
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              suspend_to_disk
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              off
            </para>
          </entry>
          <entry>
            <para>
              on
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              features
            </para>
          </entry>
          <entry>
            <para>
              acpi apic pae
            </para>
          </entry>
          <entry>
            <para>
              acpi apic pae
            </para>
          </entry>
          <entry>
            <para>
              acpi apic pae
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              host fs fmode, dmode, source_dir, target_dir
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              644 755 /tmp/ /tmp/host
            </para>
          </entry>
        </row>
      </tbody>
    </tgroup>
  </table>
  <table>
    <title>Default SEV settings</title>
    <tgroup cols="4">
      <colspec colname="c1" align="left"/>
      <colspec colname="c2" align="center"/>
      <colspec colname="c3" align="center"/>
      <colspec colname="c4" align="center"/>
      <thead>
        <row>
          <entry>
            <para>
              Setting
            </para>
          </entry>
          <entry>
            <para>
              securevm
            </para>
          </entry>
          <entry>
            <para>
              computation
            </para>
          </entry>
          <entry>
            <para>
              desktop
            </para>
          </entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>
            <para>
              kvm SEV
            </para>
          </entry>
          <entry>
            <para>
              mem_encrypt=on kvm_amd sev=1 sev_es=1
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              sec cbitpos
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              sec reducedPhysBits
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
        </row>
        <row>
          <entry>
            <para>
              sec policy
            </para>
          </entry>
          <entry>
            <para>
              auto
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
          <entry>
            <para>
              N/A
            </para>
          </entry>
        </row>
      </tbody>
    </tgroup>
  </table>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="ref-virt-scenario-manage-vm"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Managing VMs</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    After you created a VM using the <command>virt-scenario</command> interactive shell, use the
    <command>virt-scenario-launch</command> command to manage it. The command
    identifies VMs by their name as displayed by the <option>--list</option>
    option.
  </para>
<screen><prompt role="root"># </prompt><command>virt-scenario-launch --list</command>
  Version: 2.1.2
  Available VMs:
    ALP_OS
    desktop
    testing_vm
    SLE15_HPC</screen>
  <para>
    When the VM is identified, you can manage it with the following options.
  </para>
  <variablelist>
    <varlistentry>
      <term><option>--help</option></term>
      <listitem>
        <para>
          Prints short descriptions of available options.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--start</option></term>
      <listitem>
        <para>
          Starts the VM and prints security attestation information, for
          example:
        </para>
<screen><prompt role="root"># </prompt><command>virt-scenario-launch --start ALP_OS</command>
  Connected to libvirtd socket; Version: 7001000
  SEV(-ES) attestation passed!
  Validation successfull for domain ALPOS</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--status</option></term>
      <listitem>
        <para>
          Shows the status of the VM, for example:
        </para>
<screen><prompt role="root"># </prompt><command>virt-scenario-launch --status ALP_OS</command>
  Version: 2.1.2
  Connecting to libvirt qemu:///system ...
  Connected to libvirtd socket; Version: 7001000
  Domain SLE15SP5HPC state: Shutoff</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--off</option></term>
      <listitem>
        <para>
          Shuts a VM down.
        </para>
<screen><prompt role="root"># </prompt><command>virt-scenario-launch --off ALP_OS</command></screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><option>--force</option></term>
      <listitem>
        <para>
          Forces a VM off.
        </para>
<screen><prompt role="root"># </prompt><command>virt-scenario-launch --force ALP_OS</command></screen>
      </listitem>
    </varlistentry>
  </variablelist>
</section></chapter>
  <chapter role="concept" xml:lang="en" version="5.2" xml:id="keylime-remote-attestation"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Remote attestation using Keylime</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        With the growing demand on securing devices against unauthorized
        changes, the use of the security mechanism called <emphasis>remote
        attestation (RA)</emphasis> has been experiencing significant growth.
        Using RA, a host (client) can authenticate its boot chain status and
        running software on a remote host (verifier). RA is often combined with
        public-key encryption (using TPM2), thus the sent information can only
        be read by the services that requested the attestation, and the
        validity of the data can be verified.
      </para>

      <para>
        Remote attestation on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> is implemented by
        <emphasis>Keylime</emphasis>.
      </para>
    </abstract>
  </info>
  
  <section xml:id="keylime-terminology">
    <title>Terminology</title>
    <para>
      Remote attestation technology uses the following terms:
    </para>
    <variablelist>
      <varlistentry>
        <term>Attestation key (AK)</term>
        <listitem>
          <para>
            A data signing key that proves that the data comes from a real TPM
            and has not been tampered with.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Core root of trust for measurement</term>
        <listitem>
          <para>
            Calculates its own hash and the hash of the next step in the boot process, initiating the chain of measurements.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Endorsement key (EK)</term>
        <listitem>
          <para>
            An encryption key that is permanently embedded in the TPM when it
            is manufactured. The public part of the key and the certification
            stored in the TPM are used to recognize a genuine TPM.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Integrity management architecture (IMA)</term>
        <listitem>
          <para>
            A kernel integrity subsystem that provides a means of detecting
            malicious changes to files.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Measured boot</term>
        <listitem>
          <para>
            A method with which each component in the booting sequence
            calculates a hash of the next one before delegating the execution
            of the next component. The hash extends one or several PCRs of the
            TPM. An event is created with the information about where the
            measurement took place and what was measured. Such events are
            collected in an event log, and, along with the extended PCR values,
            the events can be compared with the expected values representing a
            healthy system.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Platform Configuration Register (PCR)</term>
        <listitem>
          <para>
            A memory location in TPM that, for example, stores hashes of
            booting layers. PCR can be updated only by using the non-reversible
            operation: <literal>extend</literal>. A signed list of current
            PCR values can be obtained by the <command>quote</command> command
            on TPM, and this quote can be verified by a third party during the
            attestation process.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Secure boot</term>
        <listitem>
          <para>
            Each step of the booting process checks a cryptographic signature
            on the executable of the next step before launching it.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Trusted Platform Module (TPM)</term>
        <listitem>
          <para>
            A self-contained security cryptographic processor present in the
            system as hardware or implemented in the firmware that serves as a
            root of trust. TPM provides a PCR for storing the hashes of booting
            layers. A typical TPM provides several functions, like a random
            number generator, counters or a local clock. It also stores 24 PCRs
            grouped by banks per each supported cryptographic hash function
            (SHA1, SHA256, SHA384 or SHA512).
          </para>
          <note>
            <para>
              By default, TPM usage is disabled. Therefore, the measured boot
              does not take place. To enable the remote attestation, enable TPM
              in the EFI/BIOS menu.
            </para>
          </note>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Secure payload</term>
        <listitem>
          <para>
            A mechanism to deliver encrypted data to healthy agents. Payloads
            are used to provide keys, passwords, certificates, configurations
            or scripts that are further used by the agent.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="keylime-what-is">
    <title>What is Keylime?</title>
    <para>
      Keylime is a remote attestation solution that enables you to monitor
      the health of remote nodes using a TPM as a root of trust for
      measurement. With Keylime, you can perform multiple tasks, for example:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Validate of the PCRs extended during the measured boot.
        </para>
      </listitem>
      <listitem>
        <para>
          Create analysis and make assertions of the event log.
        </para>
      </listitem>
      <listitem>
        <para>
          Make assertion of the value of any PCR in the remote system.
        </para>
      </listitem>
      <listitem>
        <para>
          Monitor the validity of open or executed files.
        </para>
      </listitem>
      <listitem>
        <para>
          Deliver encrypted data to verified nodes via <emphasis>secure
          payloads</emphasis>.
        </para>
      </listitem>
      <listitem>
        <para>
          Execute custom scripts that are triggered when a machine fails the
          attested measurements.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="keylime-architecture">
    <title>Architecture</title>
    <para>
      Keylime consists of an agent, a verifier, a registrar and a
      command-line tool (tenant). Agents are on those systems that need to be
      attested. The verifier and registrar are on remote systems that perform
      the registration and attestation of agents. Keep in mind that only the
      agent role is available on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase>. For details about each
      component, refer to the following sections.
    </para>
    <section xml:id="keylime-agent">
      <title>Keylime agent</title>
      <para>
        The agent is a service that runs on the system that needs to be
        attested. The agent sends the event log, IMA hashes, and information
        about the measured boot to the verifier, using the local TPM as a
        certifier of the data validity.
      </para>
      <para>
        When a new agent is started, it needs to register itself in the
        registrar first. To do so, the agent needs a TLS certificate to
        establish the connection. The TLS certificate is generated by the
        registrar, but it needs to be installed manually to the agent. After
        the registration, the agent sends its attestation key and the public
        part of the endorsement key to the registrar. The registrar responds to
        the agent with a challenge in a process called credential activation,
        which validates the TPM of the agent. Once the agent has been
        registered, it is ready to be enrolled for attestation.
      </para>
    </section>
    <section xml:id="keylime-registrar">
      <title>Keylime registrar</title>
      <para>
        The registrar is used to register agents that should be attested. The
        registrar collects the agent's attestation key, the public part of the
        endorsement key and the endorsement key certification, and verifies
        that the agent attestation key belongs to the endorsement key.
      </para>
    </section>
    <section xml:id="sec-keylime-verifier">
      <title>Keylime verifier</title>
      <para>
        The verifier performs the actual attestation of agents and continuously
        pulls the required attestation data from agents (among others, the
        PCR values, IMA logs, and UEFI event logs).
      </para>
    </section>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="keylime-run-with-podman"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Setting up the verifier, registrar and tenant</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <note>
    <para>
      The container described in this article delivers control plane services
      <emphasis>verifier</emphasis> and <emphasis>registrar</emphasis> and a
      <emphasis>tenant</emphasis> command-line tool (CLI) that are part of the
      Keylime project.
    </para>
  </note>
  <para>
    Before you start installing and registering agents, prepare the verifier
    and the registrar on remote hosts, as described in the following procedure.
  </para>
  <procedure>
    <step>
      <para>
        Identify the Keylime workload image.
      </para>
<screen>
<prompt role="root"># </prompt>podman search keylime
[...]
registry.opensuse.org/devel/microos/containers/containerfile/opensuse/keylime-control-plane
</screen>
    </step>
    <step>
      <para>
        Pull the image from the registry.
      </para>
<screen><prompt role="root"># </prompt>podman pull\
  registry.opensuse.org/devel/microos/containers/containerfile/opensuse/keylime-control-plane:latest</screen>
    </step>
    <step>
      <para>
        Create the <literal>keylime-control-plane</literal> volume to
        persist the database and certificates required during the attestation
        process.
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
  registry.opensuse.org/devel/microos/containers/containerfile/opensuse/keylime-control-plane:latest</screen>
    </step>
    <step>
      <para>
        Start the container and related services.
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
  registry.opensuse.org/devel/microos/containers/containerfile/opensuse/keylime-control-plane:latest</screen>
      <para>
        The <literal>keylime-control-plane</literal> container is
        created. It includes configured and running registrar and verifier
        services. Internally, the container exposes ports 8881, 8890 and 8891
        to the host using the default values. Validate the firewall
        configuration to allow access to the ports and to allow communication
        between containers, because the tenant CLI requires it.
      </para>
    </step>
  </procedure>
  <tip>
    <para>
      If you need to stop Keylime services, run the following command:
    </para>
<screen><prompt role="root"># </prompt><command>podman kill keylime-control-plane-container</command></screen>
  </tip>
  <section xml:id="keylime-monitor">
    <title>Monitoring Keylime services</title>
    <para>
      To get the status of running containers on the host, run the following
      command:
    </para>
<screen><prompt role="root"># </prompt>podman ps</screen>
    <para>
      To view the logs of Keylime services, run the following command:
    </para>
<screen><prompt role="root"># </prompt>podman logs keylime-control-plane-container</screen>
  </section>
  <section xml:id="keylime-executing-tenant">
    <title>Executing the tenant CLI</title>
    <para>
      The tenant CLI tool is included in the container, and if the host
      firewall does not interfere with the ports exposed by Keylime services,
      you can execute it using the same image, for example:
    </para>
<screen><prompt role="root"># </prompt><command>podman run --rm \
-v keylime-control-plane-volume:/var/lib/keylime/ \
keylime-control-plane:latest \
keylime_tenant -v 10.88.0.1 -r 10.88.0.1 --cert default -c reglist</command></screen>
  </section>
  <section xml:id="keylime-extract-certificates">
    <title>Extracting the Keylime certificate</title>
    <para>
      The first time that the Keylime container is executed, its services
      create a certificate required by several agents. You need to extract the
      certificate from the container and copy it to the agent's
      <filename>/var/lib/keylime/cv_ca/</filename> directory.
    </para>
<screen><prompt role="root"># </prompt><command>podman cp \
keylime-control-plane-container:/var/lib/keylime/cv_ca/cacert.crt
.</command>
<prompt role="root"># </prompt>scp cacert.crt
<replaceable>AGENT_HOST:/var/lib/keylime/cv_ca/</replaceable></screen>
    <tip>
      <para>
        Find more details about installing the agent in
        <xref linkend="keylime-install-agent"/>.
      </para>
    </tip>
  </section>
</section><section role="task" xml:lang="en" version="5.2" xml:id="keylime-install-agent"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Installing the agent</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    The Keylime agent is not present on <phrase><phrase os="alp-dolomite">ALP Dolomite</phrase></phrase> by default, and you
    need to install it manually. To install the agent, proceed as follows:
  </para>
  <procedure>
    <step>
      <para>
        Install the <package>rust-keylime</package> package as follows:
      </para>
<screen><prompt role="root"># </prompt>transactional-update pkg in rust-keylime</screen>
      <para>
        Then reboot the system.
      </para>
    </step>
    <step>
      <para>
        Adjust the default agent's configuration.
      </para>
      <substeps>
        <step>
          <para>
            Create a directory to store a new configuration file for your
            changes in <filename>/etc/keylime/agent.conf.d/</filename>. The
            default configuration is stored in
            <filename>/usr/etc/keylime/agent.conf</filename>, but we do not
            recommend editing this file because it may be overwritten in upcoming
             system updates.
          </para>
<screen><prompt role="root"># </prompt>mkdir -p /etc/keylime/agent.conf.d</screen>
        </step>
        <step>
          <para>
            Create a new file
            <filename>/etc/keylime/agent.conf.d/agent.conf</filename>:
          </para>
<screen><prompt role="root"># </prompt>cat &lt;&lt; EOF &gt; /etc/keylime/agent.conf.d/agent.conf
 [agent]
 
 uuid = "d111ec46-34d8-41af-ad56-d560bc97b2e8"<co xml:id="co-keylime-agent-uuid"/>
 registrar_ip = "<replaceable>&lt;REMOTE_IP&gt;</replaceable>"<co xml:id="co-keylime-agent-remoteip1"/>
 revocation_notification_ip = "<replaceable>&lt;REMOTE_IP&gt;</replaceable>"<co xml:id="co-keylime-agent-remoteip2"/>
 EOF</screen>
          <calloutlist>
            <callout arearefs="co-keylime-agent-uuid">
              <para>
                The unique identifier is generated each time the agent is run.
                However, you can define a specific value by this option.
              </para>
            </callout>
            <callout arearefs="co-keylime-agent-remoteip1">
              <para>
                IP address of the registrar.
              </para>
            </callout>
            <callout arearefs="co-keylime-agent-remoteip2">
              <para>
                IP address of the verifier.
              </para>
            </callout>
          </calloutlist>
        </step>
        <step>
          <para>
            Change the owner of the <filename>/etc/keylime/</filename>
            directory to <literal>keylime:tss</literal>:
          </para>
<screen><prompt role="root"># </prompt>chown -R keylime:tss /etc/keylime</screen>
        </step>
        <step>
          <para>
            Change permissions on the <filename>/etc/keylime/</filename>
            directory:
          </para>
<screen><prompt role="root"># </prompt>chmod -R 600 /etc/keylime</screen>
        </step>
      </substeps>
    </step>
    <step>
      <para>
        Copy the certificates generated by the CA to the agent node. On the
        agent node, do the following:
      </para>
      <substeps>
        <step>
          <para>
            Prepare a directory for the certificate:
          </para>
<screen><prompt role="root"># </prompt>mkdir -p /var/lib/keylime/cv_ca</screen>
        </step>
        <step>
          <para>
            Copy the certificate to the agent:
          </para>
<screen><prompt role="root"># </prompt>scp <replaceable>CERT_SERVER_ADDRESS</replaceable>:/var/lib/keylime/cv_ca/cacert.crt /var/lib/keylime/cv_ca</screen>
        </step>
        <step>
          <para>
            Change the owner of the certificate to
            <literal>keylime:tss</literal>:
          </para>
<screen><prompt role="root"># </prompt>chown -R keylime:tss /var/lib/keylime/cv_ca</screen>
        </step>
      </substeps>
    </step>
    <step>
      <para>
        Start and enable the <literal>keylime_agent.service</literal>:
      </para>
<screen><prompt role="root"># </prompt>systemctl enable --now keylime_agent.service</screen>
    </step>
  </procedure>
</section><section role="task" xml:lang="en" version="5.2" xml:id="keylime-register-agent"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Registering the agent</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  
 
  <para>
    You can register a new agent either by using the CLI tenant or by editing
    the configuration of the verifier. Using the tenant on the verifier host,
    run the following:
  </para>
<screen><prompt role="root"># </prompt>keylime_tenant -v 127.0.0.1 \
  -t <replaceable>AGENT</replaceable> \<co xml:id="co-keylime-tenant-agent"/>
  -u <replaceable>UUID</replaceable> \<co xml:id="co-keylime-tenant-uuid"/>
  --cert default \
  -c add
  [--include <replaceable>PATH_TO_ZIP_FILE</replaceable>]<co xml:id="co-keylime-tenant-include"/></screen>
  <calloutlist>
    <callout arearefs="co-keylime-tenant-agent">
      <para>
        <replaceable>AGENT</replaceable> is an IP address of the agent to be
        registered.
      </para>
    </callout>
    <callout arearefs="co-keylime-tenant-uuid">
      <para>
        <replaceable>UUID</replaceable> is the agent's unique identifier.
      </para>
    </callout>
    <callout arearefs="co-keylime-tenant-include">
      <para>
        The file passed by the <option>include</option> option is used to
        deliver secret payload data to the agent. For details, refer to
        <xref linkend="keylime-payload"/>.
      </para>
    </callout>
  </calloutlist>
  <para>
    You can list registered agents by using the <command>reglist</command>
    command on the verifier host as follows:
  </para>
<screen><prompt role="root"># </prompt>keylime_tenant -v 127.0.0.1 \
  --cert default \
  -c reglist</screen>
  <para>
    To remove a registered agent, specify the agent using the
    <option>-t</option> and <option>-u</option> options and the <option>-c
    delete</option> command as follows:
  </para>
<screen><prompt role="root"># </prompt>keylime_tenant -v 127.0.0.1 \
  -t <replaceable>AGENT</replaceable> \
  -u <replaceable>UUID</replaceable> \
  -c delete</screen>
</section><section role="task" xml:lang="en" version="5.2" xml:id="keylime-payload"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Secure payloads</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <section xml:id="keylime-payload-what-is">
    <title>What is a secure payload?</title>
    <para>
      A Keylime secure payload enables you to deliver encrypted data to
      healthy agents. Payloads are used to provide keys, passwords,
      certificates, configurations or scripts that are used by the
      Keylime agent at a later stage.
    </para>
  </section>
  <section xml:id="keylime-payload-how-it-works">
    <title>How does a secure payload work?</title>
    <para>
      A secure payload is delivered to the agent in a <filename>zip</filename>
      file that must contain a shell script named
      <filename>autorun.sh</filename>. The script is executed only if the agent
      has been properly registered and verified. To deliver the
      <filename>zip</filename> file, use the <option>--include</option> option
      of the <command>keylime_tenant</command> command.
    </para>
    <para>
      For example, the following <filename>autorun.sh</filename> script creates
      a directory structure and copies SSH keys there. The related
      <filename>zip</filename> archive must include these SSH keys.
    </para>
<screen><prompt>&gt; </prompt>cat autorun.sh
#!/bin/bash
 
 mkdir -p /root/.ssh/
 cp id_rsa* /root/.ssh/
 chmod 600 /root/.ssh/id_rsa*
 cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys</screen>
  </section>
</section><section role="task" xml:lang="en" version="5.2" xml:id="keylime-enable-ima-tracking"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Enabling IMA tracking</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para/>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com" its:translate="no"/></info>
  
  <para>
    When using IMA, the kernel calculates a hash of accessed files. The hash is
    then used to extend the PCR 10 in the TPM and also log a list of accessed
    files. The verifier can request a signed quote to the agent for PCR 10 to
    get the logs of all accessed files including the file hashes. Verifiers
    then compare the accessed files with a local allowlist of approved files.
    If any of the hashes are not recognized, the system is considered unsafe,
    and a revocation event is triggered.
  </para>
  <para>
    Before Keylime can collect information, IMA/EVM needs to be enabled. To
    enable the process, boot a kernel of the agent with the
    <literal>ima_appraise=log</literal> and <literal>ima_policy=tcb</literal>
    parameters:
  </para>
  <procedure>
    <step>
      <para>
        Update the <option>GRUB_CMDLINE_LINUX_DEFAULT</option> option with the
        parameters in <filename>/etc/default/grub</filename>:
      </para>
<screen>GRUB_CMDLINE_LINUX_DEFAULT="ima_appraise=log ima_policy=tcb"</screen>
    </step>
    <step>
      <para>
        Regenerate <filename>grub.cfg</filename> by running:
      </para>
<screen><prompt role="root"># </prompt>transactional-update grub.cfg</screen>
    </step>
    <step>
      <para>
        Reboot your system.
      </para>
    </step>
  </procedure>
  <para>
    The procedure above uses the default kernel IMA policy. To avoid monitoring
    too many files and therefore creating long logs, create a new custom
    policy. Find more details in the
    <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://keylime-docs.readthedocs.io/en/latest/user_guide/runtime_ima.html">Keylime
    documentation</link>.
  </para>
  <para>
    To indicate the expected hashes, use the <option>--allowlist</option>option
    of the <command>keylime_tenant</command> command when registering the
    agent. To view the excluded or ignored files, use the
    <option>--exclude</option> option of the <command>keylime_tenant</command>
    command:
  </para>
<screen><prompt role="root"># </prompt>keylime_tenant --allowlist
    -v 127.0.0.1 \
    -u <replaceable>UUID</replaceable> 
  </screen>
</section><section role="glue" xml:lang="en" version="5.2" xml:id="cockpit-more-info"><info><title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">For more information</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="tbazant@suse.com"/>
  </info>
  
  
  <itemizedlist>
    <listitem>
      <para>
        Keylime home page is at <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://keylime.dev"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Latest Keylime documentation is at
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://keylime.readthedocs.io/en/latest/"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        For a high-level overview of IMA/EVM, refer to
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://en.opensuse.org/SDB:Ima_evm#Introduction"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Find more details about creating a new kernel IMA policy in
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://keylime-docs.readthedocs.io/en/latest/user_guide/runtime_ima.html"/>.
      </para>
    </listitem>
  </itemizedlist>
</section></chapter>
  <chapter role="concept" xml:lang="en" version="5.2" xml:id="systemd-timer-concept"><info>
        <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Scheduling jobs using <systemitem class="daemon">systemd</systemitem> timers</title>
      <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        <systemitem class="daemon">systemd</systemitem> timer units provide a mechanism for scheduling jobs on Linux. The execution time
        of these jobs can be based on the time and date or on events.
      </para>
    </abstract></info>
  
  <para>
    <systemitem class="daemon">systemd</systemitem> timer units are identified by the <filename class="extension">.timer</filename> file
    name extension. Each timer file requires a corresponding service file it controls. In other
    words, a timer file activates and manages the corresponding service file. <systemitem class="daemon">systemd</systemitem> timers
    support the following features:
  </para>
  <itemizedlist>
    <listitem>
      <para>
        Jobs scheduled using a timer unit can depend on other <systemitem class="daemon">systemd</systemitem> services. Timer units are
        treated as regular <systemitem class="daemon">systemd</systemitem> services, so can be managed with <command>systemctl</command>.
      </para>
    </listitem>
    <listitem>
      <para>
        Timers can be real-time (being triggered on calendar events) or monotonic (being triggered
        at a specified time elapsed from a certain starting point).
      </para>
    </listitem>
    <listitem>
      <para>
        Time units are logged to the system journal, which makes it easier to monitor and
        troubleshoot them.
      </para>
    </listitem>
    <listitem>
      <para>
        Timers use the centralized <systemitem class="daemon">systemd</systemitem> management services.
      </para>
    </listitem>
    <listitem>
      <para>
        If the system is off during the expected execution time, the timer is executed once the
        system is running again.
      </para>
    </listitem>
  </itemizedlist>
<section role="task" xml:lang="en" version="5.2" xml:id="systemd-create-timer"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Creating a timer</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        The following example shows how to set up a timer that triggers the
        <literal>helloworld.sh</literal> shell script after boot time and repeats its execution
        every 24 hours relative to its activation time. It also runs Monday to Friday at 10 a.m.
      </para>
    </abstract></info>
  
  <section>
    <title><emphasis>Hello World</emphasis> example</title>
    <procedure>
      <step>
        <para>
          Create the file <filename>/etc/systemd/system/helloworld.service</filename> with the
          following content:
        </para>
<screen>[Unit]
Description="Hello World script"

[Service]
ExecStart=/usr/local/bin/helloworld.sh
</screen>
        <para>
          This is a <systemitem class="daemon">systemd</systemitem> service file that tells <systemitem class="daemon">systemd</systemitem> which application to run.
        </para>
      </step>
      <step>
        <para>
          Create the file <filename>/etc/systemd/system/helloworld.timer</filename> with the
          following content:
        </para>
<screen>[Unit]
Description="Run helloworld.service 5min after boot and every 24 hours relative to activation time"

[Timer]
OnBootSec=5min
OnUnitActiveSec=24h
OnCalendar=Mon..Fri *-*-* 10:00:*
Unit=helloworld.service

[Install]
WantedBy=multi-user.target
</screen>
        <para>
          This is the timer file that controls the activation of the respective service file.
        </para>
      </step>
      <step>
        <para>
          Verify that the files you created above contain no errors:
        </para>
<screen><prompt>&gt; </prompt>systemd-analyze verify /etc/systemd/system/helloworld.*</screen>
        <para>
          If the command returns no output, the files have passed the verification successfully.
        </para>
      </step>
      <step>
        <para>
          Start the timer:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl start helloworld.timer</screen>
        <para>
          Activates the timer for the current session only.
        </para>
      </step>
      <step>
        <para>
          Enable the timer to make sure that it is activated on boot:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl enable helloworld.timer</screen>
      </step>
    </procedure>
  </section>
  <section>
    <title>The example explained</title>
    <example xml:id="systemd-timer-example-service">
      <title>The service file</title>
<screen>[Unit]
Description="Hello World script"<co xml:id="systemd-timer-example-service-descr"/>

[Service]
ExecStart=/usr/local/bin/helloworld.sh<co xml:id="systemd-timer-example-service-exec"/></screen>
      <calloutlist>
        <callout arearefs="systemd-timer-example-service-descr">
          <para>
            A brief description explaining the service file's purpose.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-service-exec">
          <para>
            The application to execute.
          </para>
        </callout>
      </calloutlist>
      <para>
        The <literal>[Unit]</literal> and <literal>[Service]</literal> sections are the minimum
        sections required for a service file to work. <systemitem class="daemon">systemd</systemitem> service files normally contain an
        <literal>[Install]</literal> section that determines one or more targets for a service to
        load. This section is not required in service files for timers, since this information is
        provided with the timer file. For advanced configuration, refer to
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/reference-managing-systemd-targets-systemctl/reference-systemctl-managing-targets.html">Managing
        <systemitem class="daemon">systemd</systemitem> targets with systemctl</link>.
      </para>
    </example>
    <example xml:id="systemd-timer-example-timer">
      <title>The timer file</title>
<screen>[Unit]
Description="Run helloworld.service 5min after boot and every 24 hours relative to activation time"<co xml:id="systemd-timer-example-timer-descr"/>

[Timer]
OnBootSec=5min<co xml:id="systemd-timer-example-timer-onboot"/>
OnUnitActiveSec=24h<co xml:id="systemd-timer-example-timer-onunit"/>
OnCalendar=Mon..Fri *-*-* 10:00:*<co xml:id="systemd-timer-example-timer-oncal"/>
Unit=helloworld.service<co xml:id="systemd-timer-example-timer-unit"/>

[Install]
WantedBy=multi-user.target<co xml:id="systemd-timer-example-timer-wanted"/></screen>
      <calloutlist>
        <callout arearefs="systemd-timer-example-timer-descr">
          <para>
            A brief description explaining the timer file's purpose.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-timer-onboot">
          <para>
            Specifies a timer that triggers the service five minutes after the system boot. See
            <xref linkend="systemd-timer-types-monotonic"/> for details.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-timer-onunit">
          <para>
            Specifies a timer that triggers the service 24 hours after the service has been
            activated (that is, the timer triggers the service once a day). See
            <xref linkend="systemd-timer-types-realtime"/> for details.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-timer-oncal">
          <para>
            Specifies a timer that triggers the service at fixed points in time (in this example,
            Monday to Friday at 10 a.m.). See <xref linkend="systemd-timer-types-realtime"/> for
            details.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-timer-unit">
          <para>
            The service file to execute.
          </para>
        </callout>
        <callout arearefs="systemd-timer-example-timer-wanted">
          <para>
            The <systemitem class="daemon">systemd</systemitem> target in which the timer gets activated. For more information on
            <systemitem class="daemon">systemd</systemitem> targets, refer to
            <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/reference-managing-systemd-targets-systemctl/reference-systemctl-managing-targets.html">Managing
            <systemitem class="daemon">systemd</systemitem> targets with systemctl</link>.
          </para>
        </callout>
      </calloutlist>
    </example>
  </section>
</section><section role="reference" xml:lang="en" version="5.2" xml:id="systemd-timer-manage"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Managing timers</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        You can manage timers using the <command>systemctl</command> command.
      </para>
    </abstract></info>
  
  <variablelist>
    <varlistentry>
      <term>Starting and stopping timers</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl start <replaceable>TIMER</replaceable>.timer
<prompt>&gt; </prompt><command>sudo</command> systemctl restart <replaceable>TIMER</replaceable>.timer
<prompt>&gt; </prompt><command>sudo</command> systemctl stop <replaceable>TIMER</replaceable>.timer</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Enabling and disabling timers</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl enable <replaceable>TIMER</replaceable>.timer
<prompt>&gt; </prompt><command>sudo</command> systemctl disable <replaceable>TIMER</replaceable>.timer</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Showing the timer file contents</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl cat <replaceable>TIMER</replaceable>.timer</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Checking on a specific timer</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl status <replaceable>TIMER</replaceable>.timer</screen>
        <example>
          <title>Timer Status</title>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl status helloworld.timer
● helloworld.timer - "Run helloworld.service 5min after boot and every 24 hours
relative to activation time"<co xml:id="systemd-timer-status-1"/>
Loaded: loaded (/etc/systemd/system/helloworld.timer; disabled; vendor preset: disabled)<co xml:id="systemd-timer-status-2"/>
Active: active (waiting) since Tue 2022-10-26 18:35:41 CEST; 6s ago<co xml:id="systemd-timer-status-3"/>
Trigger: Wed 2022-10-27 18:35:41 CEST; 23h left<co xml:id="systemd-timer-status-4"/>
Triggers: ● helloworld.service<co xml:id="systemd-timer-status-5"/>
<co xml:id="systemd-timer-status-6"/>
Oct 26 18:35:41 neo systemd[1]: Started "Run helloworld.service 5min after boot and every 24 hours relative to activation time".<co xml:id="systemd-timer-status-7"/></screen>
          <calloutlist>
            <callout arearefs="systemd-timer-status-1">
              <para>
                The timer's file name and description.
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-2">
              <para>
                Lists whether a timer has been successfully parsed and is kept in memory (loaded),
                shows the full path to the timer file, and shows whether the timer is being started
                at boot time (enabled) or not (disabled). The first value shows the current system
                configuration, the second value the vendor preset.
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-3">
              <para>
                Indicates whether the timer is active (waiting to trigger events) or inactive. If
                active, it also shows the time that has passed since the last activation (6 seconds
                in this example).
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-4">
              <para>
                Date and time the timer is triggered next.
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-5">
              <para>
                Name of the service file the timer triggers.
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-6">
              <para>
                Optional line pointing to documentation (for example, man pages). If not available,
                an empty line is shown (as in this example).
              </para>
            </callout>
            <callout arearefs="systemd-timer-status-7">
              <para>
                Latest journal entry created by the timer.
              </para>
            </callout>
          </calloutlist>
        </example>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    To list all timers available on the system, use <command>systemctl list-timers</command>. The
    following options are available:
  </para>
  <variablelist>
    <varlistentry>
      <term>List all active timers:</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl list-timers</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>List all timers including inactive ones:</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl list-timers --all</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>List all timers matching a pattern:</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl list-timers <replaceable>PATTERN</replaceable>
<prompt>&gt; </prompt><command>sudo</command> systemctl list-timers --all<replaceable>PATTERN</replaceable></screen>
        <para>
          <replaceable>PATTERN</replaceable> must be a name or a shell globbing expression. The
          operators <literal>*</literal>, <literal>?</literal>, and <literal>[]</literal> may be
          used. Refer to <command>man 7 glob</command> for more information on globbing patterns.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>List timers matching a certain state:</term>
      <listitem>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl list-timers --state=<replaceable>STATE</replaceable></screen>
        <para>
          <replaceable>STATE</replaceable> takes the following values: <literal>active</literal>,
          <literal>failed</literal>, <literal>load</literal>, <literal>sub</literal>. See
          <command>man systemctl</command> for details.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <example>
    <title>Listing timers</title>
    <para>
      Running any <command>systemctl list-timers</command> results in a table similar to the
      one below. In this example, all active timers matching the pattern
      <literal>snapper*</literal> are listed:
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl list-timers snapper*
NEXT<co xml:id="systemd-timer-list-next"/>                       LEFT<co xml:id="systemd-timer-list-left"/>      LAST<co xml:id="systemd-timer-list-last"/>                        PASSED<co xml:id="systemd-timer-list-passed"/>   UNIT<co xml:id="systemd-timer-list-unit"/>                  ACTIVATES<co xml:id="systemd-timer-list-activates"/>

-----------------------------------------------------------------------------------------------------------------------------
Tue 2022-10-26 19:00:00 CEST 39min left Tue 2022-10-26 18:00:29 CEST 19min ago snapper-timeline.timer snapper-timeline.service
Wed 2022-10-27 08:33:04 CEST 14h   left Tue 2022-10-26 08:33:04 CEST 9h ago    snapper-cleanup.timer  snapper-cleanup.service</screen>
    <calloutlist>
      <callout arearefs="systemd-timer-list-next">
        <para>
          The point in time when the timer runs next.
        </para>
      </callout>
      <callout arearefs="systemd-timer-list-left">
        <para>
          The time left till the next timer run.
        </para>
      </callout>
      <callout arearefs="systemd-timer-list-last">
        <para>
          The point in time when the timer ran last.
        </para>
      </callout>
      <callout arearefs="systemd-timer-list-passed">
        <para>
          Time elapsed since the last timer run.
        </para>
      </callout>
      <callout arearefs="systemd-timer-list-unit">
        <para>
          The name of the timer unit.
        </para>
      </callout>
      <callout arearefs="systemd-timer-list-activates">
        <para>
          The name of the service the timer activates.
        </para>
      </callout>
    </calloutlist>
  </example>
</section><section role="reference" xml:lang="en" version="5.2" xml:id="systemd-timer-types"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Timer types</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        <systemitem class="daemon">systemd</systemitem> supports two types of timers: real-time (based on calendar) and monotonic (based
        on events). Although timers are normally persistent, <systemitem class="daemon">systemd</systemitem> also allows to set up
        transient timers that are only valid for the current session.
      </para>
    </abstract></info>
  
  <variablelist>
    <varlistentry xml:id="systemd-timer-types-realtime">
      <term>Real-time timer</term>
      <listitem>
        <para>
          Real-time timers are triggered by calendar events. They are defined using the option
          <literal>OnCalendar</literal>.
        </para>
        <para>
          You can specify when to trigger an event based on date and time. Use the following
          template:
        </para>
<screen>OnCalendar=DayOfWeek<co xml:id="systemd-timer-real-dayofweek"/> Year-Month-Day<co xml:id="systemd-timer-real-date"/> Hour:Minute:Second<co xml:id="systemd-timer-real-time"/></screen>
        <calloutlist>
          <callout arearefs="systemd-timer-real-dayofweek">
            <para>
              Day of week. Possible values are <literal>Sun</literal>,
              <literal>Mon</literal>,<literal>Tue</literal>,<literal>Wed</literal>,<literal>Thu</literal>,<literal>Sat</literal>.
              Leave out to ignore the day of the week.
            </para>
          </callout>
          <callout arearefs="systemd-timer-real-date">
            <para>
              Date. Specify month and day by two digits, year by four digits. Each value can be
              replaced by the wildcard <literal>*</literal> to match every occurrence.
            </para>
          </callout>
          <callout arearefs="systemd-timer-real-time">
            <para>
              Time. Specify each value by two digits. Each value can be replaced by the wildcard
              <literal>*</literal> to match every occurrence.
            </para>
          </callout>
        </calloutlist>
        <para>
          Applies to all values: Use two dots to define a continuous range
          (<literal>Mon..Fri</literal>). Use a comma to delimit a list of separate values
          (<literal>Mon,Wed,Fri</literal>).
        </para>
        <example>
          <title>Real-time timer examples</title>
          <itemizedlist>
            <listitem>
              <para>
                6 p.m. every Friday:
              </para>
<screen>OnCalendar=Fri *-*-* 18:00:00</screen>
            </listitem>
            <listitem>
              <para>
                5 a.m. every day:
              </para>
<screen>OnCalendar=Mon..Sun *-*-* 5:00:00</screen>
            </listitem>
            <listitem>
              <para>
                1 a.m. and 3 a.m. on Sundays and Tuesdays:
              </para>
<screen>OnCalendar=Tue,Sun *-*-* 01,03:00:00</screen>
            </listitem>
            <listitem>
              <para>
                Single date:
              </para>
<screen>OnCalendar=Mo..Sun 2023-09-23 00:00:01</screen>
            </listitem>
            <listitem>
              <para>
                To specify triggers at different times, you can create more than one OnCalendar
                entry in a single timer file:
              </para>
<screen>OnCalendar=Mon..Fri *-*-* 10:00
OnCalendar=Sat,Sun *-*-* 22:00</screen>
            </listitem>
          </itemizedlist>
        </example>
        <para>
          For a full list of available features and options, refer to <command>man 7
          systemd.time</command> that offers additional information on the following topics:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              shorten the syntax and use abbreviations
            </para>
          </listitem>
          <listitem>
            <para>
              specify repetitions
            </para>
          </listitem>
          <listitem>
            <para>
              find specific days in a month (last day of month, last Sunday, etc.)
            </para>
          </listitem>
          <listitem>
            <para>
              apply time zones
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
    </varlistentry>
    <varlistentry xml:id="systemd-timer-types-monotonic">
      <term>Monotonic timers</term>
      <listitem>
        <para>
          Monotonic timers are triggered at a specified time elapsed from a certain event, such as
          a system boot or system unit activation event. Values are defined as time units (minutes,
          hours, days, months, years, etc.). The following units are supported:
          <literal>usec</literal>, <literal>msec</literal>, <literal>seconds</literal>,
          <literal>minutes</literal>, <literal>hours</literal>, <literal>days</literal>,
          <literal>weeks</literal>, <literal>months</literal>, <literal>years</literal>. There are
          several options for defining monotonic timers:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              <literal>OnActiveSec</literal>: time after unit activation
            </para>
<screen>OnActiveSec=50minutes</screen>
          </listitem>
          <listitem>
            <para>
              <literal>OnBootSec</literal>: time after system boot
            </para>
<screen>OnBootSec=10hours</screen>
          </listitem>
          <listitem>
            <para>
              <literal>OnStartupSec</literal>: time after the service manager is started. For
              system services, this is almost equal to <literal>OnActiveSec</literal>. Use this for
              user services where the service manager is started at user login.
            </para>
<screen>OnStartupSec=5minutes 20seconds</screen>
          </listitem>
          <listitem>
            <para>
              <literal>OnUnitActiveSec</literal>: time after the corresponding service was last
              activated
            </para>
<screen>OnUnitActiveSec=10seconds</screen>
          </listitem>
          <listitem>
            <para>
              <literal>OnUnitInactiveSec</literal>: time after the corresponding service was last
              deactivated
            </para>
<screen>OnUnitInactiveSec=2hours 15minutes 18 seconds</screen>
          </listitem>
        </itemizedlist>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Transient timers</term>
      <listitem>
        <para>
          Transient timers are temporary timers that are only valid for the current session. Using
          these timers, you can either use an existing service file or start a program directly.
          Transient timers are invoked by running <command>systemd-run</command>.
        </para>
        <para>
          The following example runs the <filename>helloworld.service</filename> unit every two
          hours:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemd-run --on-active="2hours" --unit="helloworld.service"</screen>
        <para>
          To run a command directly, use the following syntax. In this example, the script
          <filename>/usr/local/bin/helloworld.sh</filename> is called directly:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemd-run --on-active="2hours" /usr/local/bin/helloworld.sh</screen>
        <para>
          If the command takes parameters, add them separated by space:
        </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemd-run --on-active="2hours" /usr/local/bin/helloworld.sh --language=pt_BR</screen>
        <para>
          Transient timers can be monotonic or real-time. The following switches are supported and
          work as described in <xref linkend="systemd-timer-types-monotonic" xrefstyle="select:title"/>:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              <option>--on-active</option>
            </para>
          </listitem>
          <listitem>
            <para>
              <option>--on-startup</option>
            </para>
          </listitem>
          <listitem>
            <para>
              <option>--on-unit-active</option>
            </para>
          </listitem>
          <listitem>
            <para>
              <option>--on-unit-inactive</option>
            </para>
          </listitem>
          <listitem>
            <para>
              <option>--on-calendar</option>
            </para>
          </listitem>
        </itemizedlist>
        <para>
          For more information, refer to <command>man 1 systemd-run</command>.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
</section><section role="task" xml:lang="en" version="5.2" xml:id="systemd-timer-test"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Testing calendar entries</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        <systemitem class="daemon">systemd</systemitem> provides a tool for testing and creating calendar timer entries for real-time
        timers: <command>systemd-analyze calendar</command>. It accepts the same argument as the
        <literal>OnCalendar</literal> entry required to set up real-time timers.
      </para>
    </abstract></info>
  
  <para>
    You can concatenate several arguments separated by space. If the term to test is correct, the
    output shows you when the timer is triggered next (in local time and UTC). It also
    shows the string in <literal>Normalized form</literal>, and it is recommended to use that string
    in the timer file. Consider the following examples:
  </para>
<screen><prompt>&gt; </prompt>systemd-analyze calendar "Tue,Sun *-*-* 01,03:00:00"
Normalized form: Tue,Sun *-*-* 01,03:00:00
Next elapse: Sun 2021-10-31 01:00:00 CEST
(in UTC): Sat 2021-10-30 23:00:00 UTC
From now: 3 days left

<prompt>&gt; </prompt>systemd-analyze calendar "Mon..Fri *-*-* 10:00" "Sat,Sun *-*-* 22:00"
Original form: Mon..Fri *-*-* 10:00
Normalized form: Mon..Fri *-*-* 10:00:00
Next elapse: Thu 2021-10-28 10:00:00 CEST
(in UTC): Thu 2021-10-28 08:00:00 UTC
From now: 19h left

Original form: Sat,Sun *-*-* 22:00
Normalized form: Sat,Sun *-*-* 22:00:00
Next elapse: Sat 2021-10-30 22:00:00 CEST
(in UTC): Sat 2021-10-30 20:00:00 UTC
From now: 3 days left</screen>
  <para>
    For recurring timers, use the <option>–iterations <replaceable>N</replaceable></option> switch
    to list trigger times, then test whether they work as expected. The argument
    <replaceable>N</replaceable> specifies the number of iterations you would like to test. The
    following example string triggers every 8 hours (starting at 00:00:00) on Sundays:
  </para>
<screen><prompt>&gt; </prompt>systemd-analyze calendar --iterations 5 "Sun *-*-* 0/08:00:00"
Original form: Sun *-*-* 0/08:00:00
Normalized form: Sun *-*-* 00/8:00:00
Next elapse: Sun 2021-10-31 00:00:00 CEST
(in UTC): Sat 2021-10-30 22:00:00 UTC
From now: 3 days left
Iter. #2: Sun 2021-10-31 08:00:00 CET
(in UTC): Sun 2021-10-31 07:00:00 UTC
From now: 3 days left
Iter. #3: Sun 2021-10-31 16:00:00 CET
(in UTC): Sun 2021-10-31 15:00:00 UTC
From now: 4 days left
Iter. #4: Sun 2021-11-07 00:00:00 CET
(in UTC): Sat 2021-11-06 23:00:00 UTC
From now: 1 week 3 days left
Iter. #5: Sun 2021-11-07 08:00:00 CET
(in UTC): Sun 2021-11-07 07:00:00 UTC
From now: 1 week 3 days left</screen>
</section><section role="task" xml:lang="en" version="5.2" xml:id="systemd-mailto"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Getting e-mail notifications when a timer fails</title>
          <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
            <para>
              <systemitem class="daemon">systemd</systemitem> does not offer a feature similar to cron's MAILTO. The
              procedure below describes a workaround to enable e-mail
              notifications when a timer fails.
            </para>

            <para>
              The procedure consists of the following steps:
            </para>
            <orderedlist>
              <listitem>
                <para>
                  Create a script that sends an e-mail.
                </para>
              </listitem>
              <listitem>
                <para>
                  Create a <systemitem class="daemon">systemd</systemitem> service file running the e-mail script.
                </para>
              </listitem>
              <listitem>
                <para>
                  Test the e-mail service file.
                </para>
              </listitem>
              <listitem>
                <para>
                  From the service that the timer controls, call the created
                  e-mail service file via <literal>OnFailure</literal>.
                </para>
              </listitem>
            </orderedlist>
          </abstract>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/></info>
  
  <procedure>
    <para>
      In the following example, we are using the <command>mailx</command> command from package
      <package>mailx</package>. It requires the Postfix e-mail server to be installed and correctly
      configured.
    </para>
    <step>
      <para>
        Create the script <filename class="directory">/usr/local/bin/send_systemd_email</filename>.
      </para>
      <substeps>
        <step>
          <para>
            The script requires two parameters: <literal>$1</literal>, the e-mail
            address, and <literal>$2</literal>, the name of the service file for which the failure
            notification is received. Both parameters are supplied by the unit file running
            the mail script.
          </para>
<screen>#!/bin/sh
systemctl status --full "$2" | mailx -S sendwait\
 -s "Service failure for $2" -r root@$HOSTNAME $1
</screen>
        </step>
        <step>
          <para>
            Make sure the script is executable:
          </para>
<screen><prompt>&gt; </prompt><command>sudo</command> chmod 755 /usr/local/bin/send_systemd_email</screen>
        </step>
      </substeps>
    </step>
    <step>
      <para>
        Create the file
        <filename>/etc/systemd/system/send_email_to_<replaceable>USER</replaceable>@.service</filename>.
      </para>
<screen>[Unit]
Description=Send systemd status information by email for %i to <replaceable>USER</replaceable>

[Service]
Type=oneshot
ExecStart=/usr/local/bin/send_systemd_email <replaceable>EMAIL_ADDRESS</replaceable> %i
User=root
Group=systemd-journal
</screen>
      <para>
        Replace <replaceable>USER</replaceable> and <replaceable>EMAIL_ADDRESS</replaceable> in the
        file with the login and e-mail address of the user that should receive the e-mail.
        <literal>%i</literal> is the name of the service that has failed (it is passed on to
        the e-mail service by the <literal>%n</literal> parameter).
      </para>
    </step>
    <step>
      <para>
        Verify the service file and fix the reported issues:
      </para>
<screen><prompt>&gt; </prompt>systemd-analyze verify /etc/systemd/system/send_email_to_<replaceable>USER</replaceable>@.service</screen>
      <para>
        If the command returns no output, the file has passed the verification successfully.
      </para>
    </step>
    <step>
      <para>
        To verify the complete procedure, start the service using the <literal>dbus</literal>
        instance for testing. (You can use any other service that is currently running. dbus is
        used in this example because the service is guaranteed to run on any installation.)
      </para>
<screen><prompt>&gt; </prompt><command>sudo</command> systemctl start send_email_to_<replaceable>USER</replaceable>@dbus.service</screen>
      <para>
        If successful, <replaceable>EMAIL_ADDRESS</replaceable> receives an e-mail with the
        subject <literal>Service failure for dbus</literal> containing dbus status messages in the
        body. (This is just a test, there is no problem with the dbus service. You can
        safely delete the e-mail, no action is required).
      </para>
      <para>
        If the test e-mail has been successfully sent, proceed by integrating it into your service
        file.
      </para>
    </step>
    <step>
      <para>
        To add an e-mail notification to the service, add an <literal>OnFailure</literal> option to
        the <literal>Unit</literal> section of the service file for which you would like to get
        notified in the event of failure:
      </para>
<screen>[Unit]
Description="Hello World script"
OnFailure<co xml:id="systemd-mailto-onfailure"/>=send_email_to_<replaceable>USER</replaceable><co xml:id="systemd-mailto-user"/>@%n<co xml:id="systemd-mailto-name"/>.service

[Service]
ExecStart=/usr/local/bin/helloworld.sh
</screen>
      <calloutlist>
        <callout arearefs="systemd-mailto-onfailure">
          <para>
            The <literal>OnFailure</literal> option takes a service as an argument.
          </para>
        </callout>
        <callout arearefs="systemd-mailto-user">
          <para>
            Replace the part of the service unit file name with the login name.
          </para>
        </callout>
        <callout arearefs="systemd-mailto-name">
          <para>
            Specifies the name of the service (<literal>helloworld</literal> in this example). This
            name is available in the e-mail service file as %i.
          </para>
        </callout>
      </calloutlist>
    </step>
    <result>
      <para>
        You have successfully set up the failure notification for <systemitem class="daemon">systemd</systemitem> services.
      </para>
    </result>
  </procedure>
  <tip>
    <title>Sending e-mail notifications to multiple users</title>
    <para>
      The e-mail service file has the recipient's e-mail address hard-coded. To send notification
      e-mails to a different user, copy the e-mail service file, and replace the user login in the
      file name and the e-mail address within the copy.
    </para>
    <para>
      To send a failure notification to several recipients simultaneously, add the respective
      service files to the service file (use spaces as a separator):
    </para>
<screen>OnFailure=send_email_to_tux@%n.service send_email_to_wilber@%n.service</screen>
  </tip>
</section><section role="task" xml:lang="en" version="5.2" xml:id="systemd-timer-user"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Using timers as a regular user</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        <systemitem class="daemon">systemd</systemitem> timers can also be used by regular users. It helps you to automate recurring
        tasks like backups, processing images, or moving data to the cloud.
      </para>
    </abstract></info>
  
  <para>
    The same procedures and tasks as for system-wide timers are valid. However, the following
    differences apply:
  </para>
  <itemizedlist>
    <listitem>
      <para>
        Timer and service files must be placed in <filename>~/.config/systemd/user/</filename>.
      </para>
    </listitem>
    <listitem>
      <para>
        All <command>systemctl</command> and <command>journalctl</command> commands must be run
        with the <option>--user</option> switch. <command>systemd-analyze</command> does
        <emphasis>not</emphasis> require this option.
      </para>
      <para>
        As a regular user, you must provide the path to the unit files, as in the examples below.
        Otherwise, if a system-wide timer with the same name exists, it would be executed or listed
        instead.
      </para>
<screen><prompt>&gt; </prompt>systemctl --user start ~/.config/systemd/user/helloworld.timer
<prompt>&gt; </prompt>systemctl --user enable ~/.config/systemd/user/helloworld.timer
<prompt>&gt; </prompt>systemctl --user list-timers
<prompt>&gt; </prompt>journalctl --user -u helloworld.*
<prompt>&gt; </prompt>systemctl-analyze ~/.config/systemd/user/helloworld.timer</screen>
    </listitem>
  </itemizedlist>
  <important>
    <title>User timers only run during an active session</title>
    <para>
      As with other <systemitem class="daemon">systemd</systemitem> services started as a regular user, user timers only run when the
      user is logged in. Instead, to start user timers at boot time and keep them running after
      logout, enable <emphasis>lingering</emphasis> for each affected user:
    </para>
<screen>sudo loginctl enable-linger <replaceable>USER</replaceable></screen>
    <para>
      For more information, refer to <command>man 1 loginctl</command>.
    </para>
  </important>
  <important>
    <title>Environment variables are not inherited</title>
    <para>
      The <systemitem class="daemon">systemd</systemitem> user instance does not inherit environment variables set by scripts like
      <filename>~/.profile</filename> or <filename>~/.bashrc</filename>. To check the <systemitem class="daemon">systemd</systemitem>
      environment, run <command>systemctl --user show-environment</command>.
    </para>
    <para>
      To import any variables missing in the <systemitem class="daemon">systemd</systemitem> environment, specify the following command
      at the end of your <filename>~/.bashrc</filename>:
    </para>
<screen>systemctl --user import-environment <replaceable>VARIABLE1</replaceable> <replaceable>VARIABLE2</replaceable></screen>
  </important>
</section><section role="task" xml:lang="en" version="5.2" xml:id="systemd-timer-catchup"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Migrating from cron to <systemitem class="daemon">systemd</systemitem> timers</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        All cron jobs can be migrated to <systemitem class="daemon">systemd</systemitem> timers. Find
        instructions and an example here.
      </para>
    </abstract></info>
  
  <procedure>
    <step>
      <para>
        Create a service file executing the script. See
        <xref linkend="systemd-timer-example-service"/> for details.
      </para>
    </step>
    <step>
      <para>
        Create a timer file executing the service file. See
        <xref linkend="systemd-timer-example-timer"/> for general instructions.
      </para>
      <substeps>
        <step>
          <para>
            Convert calendar entries. Time is specified differently in cron and <systemitem class="daemon">systemd</systemitem>. Use
            the patterns below as a conversion template:
          </para>
<screen>Cron:               Minute Hour Day Month DayOfWeek
systemd: OnCalendar=DayOfWeek Year-Month-Day Hour:Minute:Second</screen>
          <para>
            To test the converted calendar entry, follow the instructions in
            <xref linkend="systemd-timer-test"/>.
          </para>
        </step>
        <step>
          <para>
            Convert cron nicknames (<literal>@<replaceable>NICK</replaceable></literal>):
          </para>
<screen>Cron     : <systemitem class="daemon">systemd</systemitem> timer
-------- : ----------------------------
@reboot  : OnBootSec=1s
@yearly  : OnCalendar=*-01-01 00:00:00
@annually: OnCalendar=*-01-01 00:00:00
@monthly : OnCalendar=*-*-01 00:00:00
@weekly  : OnCalendar=Sun *-*-* 00:00:00
@daily   : OnCalendar=*-*-* 00:00:00
@hourly  : OnCalendar=*-*-* *:00:00</screen>
        </step>
        <step>
          <para>
            Convert variable assignments. The <systemitem class="daemon">systemd</systemitem> variable assignment must go into the
            <literal>[Service]</literal> section. You cannot convert
            <envar>MAILTO</envar> this way—refer to the next step for this.
          </para>
<screen>cron: <replaceable>VARIABLE</replaceable>=<replaceable>VALUE</replaceable>
systemd: Environment="<replaceable>VARIABLE</replaceable>=<replaceable>VALUE</replaceable>"</screen>
        </step>
        <step>
          <para>
            Set up e-mail notifications to replace cron's MAILTO feature by following the
            instructions in <xref linkend="systemd-mailto"/>.
          </para>
        </step>
      </substeps>
    </step>
  </procedure>
  <example>
    <title>cron to <systemitem class="daemon">systemd</systemitem> timer migration</title>
    <para>
      Here are the crontab entries which call the script <literal>helloworld.sh</literal> 5
      minutes after booting and at 10 o'clock each Monday to Friday:
    </para>
<screen>@reboot sleep 300 &amp;&amp; /usr/local/bin/helloworld.sh
0 10 * * * 1-5 /usr/local/bin/helloworld.sh</screen>
    <para>
      The <systemitem class="daemon">systemd</systemitem> service file (<filename>helloworld.service</filename>) calling the script
      looks like this:
    </para>
<screen>[Unit]
Description="Hello World script"
[Service]
ExecStart=/usr/local/bin/helloworld.sh</screen>
    <para>
      The timer file (<filename>helloworld.timer</filename>) looks like this:
    </para>
<screen>[Unit]
Description="Run helloworld.service 5min after boot and at 10am every Mon-Fri"
[Timer]
OnBootSec=5min
OnCalendar=Mon..Fri *-*-* 10:00:*
Unit=helloworld.service
[Install]
WantedBy=multi-user.target</screen>
  </example>
</section><section role="task" xml:lang="en" version="5.2" xml:id="systemd-timer-troubleshoot"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Troubleshooting and FAQs</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/><abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        Learn how to debug and troubleshoot <systemitem class="daemon">systemd</systemitem> timers that have failed. Find answers to
        frequently asked questions on <systemitem class="daemon">systemd</systemitem> timers.
      </para>
    </abstract></info>
  
  <section>
    <title>Avoiding errors</title>
    <para>
      To avoid errors with <systemitem class="daemon">systemd</systemitem> timers, make sure to follow these best practices:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Verify that the executable you specify in the service with <literal>ExecStart</literal>
          runs correctly.
        </para>
      </listitem>
      <listitem>
        <para>
          Check the syntax of the service and timer files by running <command>systemd-analyze
          verify <replaceable>FILE</replaceable></command>.
        </para>
      </listitem>
      <listitem>
        <para>
          Check execution times of calendar entries by running <command>systemd-analyze calendar
          <replaceable>CALENDER_ENTRY</replaceable></command>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section>
    <title>Event is not triggered</title>
    <para>
      When you activate a timer that contains non-critical errors, <systemitem class="daemon">systemd</systemitem> silently ignores them.
      For example:
    </para>
    <example>
      <title><systemitem class="daemon">systemd</systemitem> timer file cutout containing a non-fatal error</title>
<screen>[Timer]
OnBootSec=5min
OnCalendar=Mon..Fri 10:00
Unit=helloworld.service
</screen>
      <para>
        Line 3 contains a syntax error (<literal>OnCalendar</literal> instead of
        <literal>OnCalendar</literal>). Since the <literal>[Timer]</literal> section contains a
        second timer entry (OnBoot), the error is not critical and is silently ignored. As a
        consequence, the Monday to Friday trigger is not executed. The only way to detect the
        error is to use the command <command>systemd-analyze verify</command>:
      </para>
<screen><prompt role="root"># </prompt> systemd-analyze verify /etc/systemd/system/helloworld.timer
/etc/systemd/system/helloworld.timer:7: Unknown key name 'OnClendar' in section 'Timer', ignoring.</screen>
    </example>
  </section>
  <section>
    <title>Checking the system journal for errors</title>
    <para>
      As with every <systemitem class="daemon">systemd</systemitem> service, events and actions triggered by timers are logged with the
      system journal. If a trigger does not behave as expected, check the log messages with
      <command>journalctl</command>. To filter the journal for relevant information, use the
      <option>-u</option> switch to specify the <systemitem class="daemon">systemd</systemitem> timers and service files. Use this option
      to show the log entries for the timer <emphasis>and</emphasis> the corresponding service
      file:
    </para>
<screen>sudo journalctl -u  helloworld.timer -u helloworld.service</screen>
    <para>
      or shorter (if applicable):
    </para>
<screen>sudo journalctl -u  helloworld.*</screen>
    <para>
      <command>journalctl</command> is a tool that supports many options and filters. Please refer
      to <command>man 1 journalctl</command> for in-depth information. The following options are
      useful for troubleshooting timers:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <option>-b</option>: Only show entries for the current boot.
        </para>
      </listitem>
      <listitem>
        <para>
          <option>-S today</option>: Only show entries from today.
        </para>
      </listitem>
      <listitem>
        <para>
          <option>-x</option>: Show help texts alongside the log entry.
        </para>
      </listitem>
      <listitem>
        <para>
          <option>-f</option>: Start with the most recent entries and continuously print the log as
          new entries get added. Useful to check triggers that occur in short intervals. Exit with
          <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section>
    <title><systemitem class="daemon">systemd</systemitem> timer: catching up on missed runs</title>
    <para>
      If a <systemitem class="daemon">systemd</systemitem> timer was inactive or the system was off during the
      expected execution time, missed events can optionally be triggered
      immediately when the timer is activated again. To enable this, add the
      configuration option <literal>Persistent=true</literal> to the
      <literal>[Timer]</literal> section:
    </para>
    <screen>[Timer]
OnCalendar=Mon..Fri 10:00
Persistent=true
Unit=helloworld.service</screen>
  </section>
  <section>
    <title>How to migrate from cron to <systemitem class="daemon">systemd</systemitem> timers?</title>
    <para>
      All cron jobs can be migrated to <systemitem class="daemon">systemd</systemitem> timers. Here are general instructions on migrating
      a cron job:
    </para>
    <procedure>
      <step>
        <para>
          Create a service file executing the script. See
          <xref linkend="systemd-timer-example-service"/> for details.
        </para>
      </step>
      <step>
        <para>
          Create a timer file executing the service file. See
          <xref linkend="systemd-timer-example-timer"/> for general instructions.
        </para>
        <substeps>
          <step>
            <para>
              Convert calendar entries. Time is specified differently in cron and <systemitem class="daemon">systemd</systemitem>. Use
              the patterns below as a conversion template:
            </para>
<screen>Cron:               Minute Hour Day Month DayOfWeek
systemd: OnCalendar=DayOfWeek Year-Month-Day Hour:Minute:Second</screen>
            <para>
              To test the converted calendar entry, follow the instructions in
              <xref linkend="systemd-timer-test"/>.
            </para>
          </step>
          <step>
            <para>
              Convert cron nicknames (<literal>@<replaceable>NICK</replaceable></literal>):
            </para>
<screen>Cron     : <systemitem class="daemon">systemd</systemitem> timer
-------- : ----------------------------
@reboot  : OnBootSec=1s
@yearly  : OnCalendar=*-01-01 00:00:00
@annually: OnCalendar=*-01-01 00:00:00
@monthly : OnCalendar=*-*-01 00:00:00
@weekly  : OnCalendar=Sun *-*-* 00:00:00
@daily   : OnCalendar=*-*-* 00:00:00
@hourly  : OnCalendar=*-*-* *:00:00</screen>
          </step>
          <step>
            <para>
              Convert variable assignments. The <systemitem class="daemon">systemd</systemitem> variable assignment must go into the
              <literal>[Service]</literal> section. You cannot convert
              <envar>MAILTO</envar> this way—refer to the next step for this.
            </para>
<screen>cron: <replaceable>VARIABLE</replaceable>=<replaceable>VALUE</replaceable>
systemd: Environment="<replaceable>VARIABLE</replaceable>=<replaceable>VALUE</replaceable>"</screen>
          </step>
          <step>
            <para>
              Set up e-mail notifications to replace cron's MAILTO feature by following the
              instructions in <xref linkend="systemd-mailto"/>.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
    <example>
      <title>cron to <systemitem class="daemon">systemd</systemitem> timer migration</title>
      <para>
        Here are the crontab entries which call the script <literal>helloworld.sh</literal> 5
        minutes after booting and at 10 o'clock each Monday to Friday:
      </para>
<screen>@reboot sleep 300 &amp;&amp; /usr/local/bin/helloworld.sh
0 10 * * * 1-5 /usr/local/bin/helloworld.sh</screen>
      <para>
        The <systemitem class="daemon">systemd</systemitem> service file (<filename>helloworld.service</filename>) calling the script
        looks like this:
      </para>
<screen>[Unit]
Description="Hello World script"
[Service]
ExecStart=/usr/local/bin/helloworld.sh</screen>
      <para>
        The timer file (<filename>helloworld.timer</filename>) looks like this:
      </para>
<screen>[Unit]
Description="Run helloworld.service 5min after boot and at 10am every Mon-Fri"
[Timer]
OnBootSec=5min
OnCalendar=Mon..Fri *-*-* 10:00:*
Unit=helloworld.service
[Install]
WantedBy=multi-user.target</screen>
    </example>
  </section>
</section><section role="glue" xml:lang="en" version="5.2" xml:id="systemd-timers-more-info"><info>
          <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:tr="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">For more information</title>
        <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="fs@suse.com" its:translate="no"/></info>
  
  <itemizedlist>
    <listitem>
      <para>
        For a full reference on <systemitem class="daemon">systemd</systemitem> timers including advanced configuration options (like
        delays or handling clock or time zone changes), refer to <command>man 5
        systemd.timer</command>.
      </para>
    </listitem>
    <listitem>
      <para>
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/concept-systemd/concept-systemd.html">Basic
        <systemitem class="daemon">systemd</systemitem> concepts</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/reference-systemctl-start-stop-services/reference-systemctl-start-stop-services.html">Starting
        and stopping <systemitem class="daemon">systemd</systemitem> services</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html">Enabling
        and disabling <systemitem class="daemon">systemd</systemitem> services</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/task-debug-failed-systemd-services/index.html">Debugging
        failed <systemitem class="daemon">systemd</systemitem> services</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://documentation.suse.com/smart/systems-management/html/task-send-termination-signals-systemd/task-send-termination-signals-systemd.html">Sending
        termination signals to <systemitem class="daemon">systemd</systemitem> services</link>
      </para>
    </listitem>
  </itemizedlist>
</section></chapter>
  <appendix version="5.2" xml:id="legal-disclaimer"><info>
    <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">Legal Notice</title>
  </info>
  
  <para> Copyright© 2006– <?dbtimestamp format="Y"?>
 SUSE LLC and contributors.
  All rights reserved. </para>
  <para>
    Permission is granted to copy, distribute and/or modify this document under the terms of the
    GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the Invariant
    Section being this copyright notice and license. A copy of the license version 1.2 is included
    in the section entitled <quote>GNU Free Documentation License</quote>.
  </para>
  <para>
    For SUSE trademarks, see <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.suse.com/company/legal/"/>. All other
    third-party trademarks are the property of their respective owners. Trademark symbols (®, ™
    etc.) denote trademarks of SUSE and its affiliates. Asterisks (*) denote third-party
    trademarks.
  </para>
  <para>
    All information found in this book has been compiled with utmost attention to detail. However,
    this does not guarantee complete accuracy.  Neither SUSE LLC, its affiliates, the authors, nor
    the translators shall be held liable for possible errors or the consequences thereof.
  </para>
</appendix>
  <appendix xmlns:its="http://www.w3.org/2005/11/its" version="5.2" role="legal" its:translate="no" xml:id="doc-gfdl-license"><info>
   <title xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink">GNU Free Documentation License</title>
 </info>

 

 <para>
  Copyright (C) 2000, 2001, 2002 Free Software Foundation, Inc. 51 Franklin St,
  Fifth Floor, Boston, MA 02110-1301 USA. Everyone is permitted to copy and
  distribute verbatim copies of this license document, but changing it is not
  allowed.
 </para>

 <bridgehead renderas="sect4">
    0. PREAMBLE
  </bridgehead>

 <para>
  The purpose of this License is to make a manual, textbook, or other
  functional and useful document "free" in the sense of freedom: to assure
  everyone the effective freedom to copy and redistribute it, with or without
  modifying it, either commercially or non-commercially. Secondarily, this
  License preserves for the author and publisher a way to get credit for their
  work, while not being considered responsible for modifications made by
  others.
 </para>

 <para>
  This License is a kind of "copyleft", which means that derivative works of
  the document must themselves be free in the same sense. It complements the
  GNU General Public License, which is a copyleft license designed for free
  software.
 </para>

 <para>
  We have designed this License to use it for manuals for free software,
  because free software needs free documentation: a free program should come
  with manuals providing the same freedoms that the software does. But this
  License is not limited to software manuals; it can be used for any textual
  work, regardless of subject matter or whether it is published as a printed
  book. We recommend this License principally for works whose purpose is
  instruction or reference.
 </para>

 <bridgehead renderas="sect4">
    1. APPLICABILITY AND DEFINITIONS
  </bridgehead>

 <para>
  This License applies to any manual or other work, in any medium, that
  contains a notice placed by the copyright holder saying it can be distributed
  under the terms of this License. Such a notice grants a world-wide,
  royalty-free license, unlimited in duration, to use that work under the
  conditions stated herein. The "Document", below, refers to any such manual or
  work. Any member of the public is a licensee, and is addressed as "you". You
  accept the license if you copy, modify or distribute the work in a way
  requiring permission under copyright law.
 </para>

 <para>
  A "Modified Version" of the Document means any work containing the Document
  or a portion of it, either copied verbatim, or with modifications and/or
  translated into another language.
 </para>

 <para>
  A "Secondary Section" is a named appendix or a front-matter section of the
  Document that deals exclusively with the relationship of the publishers or
  authors of the Document to the Document's overall subject (or to related
  matters) and contains nothing that could fall directly within that overall
  subject. (Thus, if the Document is in part a textbook of mathematics, a
  Secondary Section may not explain any mathematics.) The relationship could be
  a matter of historical connection with the subject or with related matters,
  or of legal, commercial, philosophical, ethical or political position
  regarding them.
 </para>

 <para>
  The "Invariant Sections" are certain Secondary Sections whose titles are
  designated, as being those of Invariant Sections, in the notice that says
  that the Document is released under this License. If a section does not fit
  the above definition of Secondary then it is not allowed to be designated as
  Invariant. The Document may contain zero Invariant Sections. If the Document
  does not identify any Invariant Sections then there are none.
 </para>

 <para>
  The "Cover Texts" are certain short passages of text that are listed, as
  Front-Cover Texts or Back-Cover Texts, in the notice that says that the
  Document is released under this License. A Front-Cover Text may be at most 5
  words, and a Back-Cover Text may be at most 25 words.
 </para>

 <para>
  A "Transparent" copy of the Document means a machine-readable copy,
  represented in a format whose specification is available to the general
  public, that is suitable for revising the document straightforwardly with
  generic text editors or (for images composed of pixels) generic paint
  programs or (for drawings) some widely available drawing editor, and that is
  suitable for input to text formatters or for automatic translation to a
  variety of formats suitable for input to text formatters. A copy made in an
  otherwise Transparent file format whose markup, or absence of markup, has
  been arranged to thwart or discourage subsequent modification by readers is
  not Transparent. An image format is not Transparent if used for any
  substantial amount of text. A copy that is not "Transparent" is called
  "Opaque".
 </para>

 <para>
  Examples of suitable formats for Transparent copies include plain ASCII
  without markup, Texinfo input format, LaTeX input format, SGML or XML using a
  publicly available DTD, and standard-conforming simple HTML, PostScript or
  PDF designed for human modification. Examples of transparent image formats
  include PNG, XCF and JPG. Opaque formats include proprietary formats that can
  be read and edited only by proprietary word processors, SGML or XML for which
  the DTD and/or processing tools are not generally available, and the
  machine-generated HTML, PostScript or PDF produced by some word processors
  for output purposes only.
 </para>

 <para>
  The "Title Page" means, for a printed book, the title page itself, plus such
  following pages as are needed to hold, legibly, the material this License
  requires to appear in the title page. For works in formats which do not have
  any title page as such, "Title Page" means the text near the most prominent
  appearance of the work's title, preceding the beginning of the body of the
  text.
 </para>

 <para>
  A section "Entitled XYZ" means a named subunit of the Document whose title
  either is precisely XYZ or contains XYZ in parentheses following text that
  translates XYZ in another language. (Here XYZ stands for a specific section
  name mentioned below, such as "Acknowledgements", "Dedications",
  "Endorsements", or "History".) To "Preserve the Title" of such a section when
  you modify the Document means that it remains a section "Entitled XYZ"
  according to this definition.
 </para>

 <para>
  The Document may include Warranty Disclaimers next to the notice which states
  that this License applies to the Document. These Warranty Disclaimers are
  considered to be included by reference in this License, but only as regards
  disclaiming warranties: any other implication that these Warranty Disclaimers
  may have is void and has no effect on the meaning of this License.
 </para>

 <bridgehead renderas="sect4">
    2. VERBATIM COPYING
  </bridgehead>

 <para>
  You may copy and distribute the Document in any medium, either commercially
  or non-commercially, provided that this License, the copyright notices, and
  the license notice saying this License applies to the Document are reproduced
  in all copies, and that you add no other conditions whatsoever to those of
  this License. You may not use technical measures to obstruct or control the
  reading or further copying of the copies you make or distribute. However, you
  may accept compensation in exchange for copies. If you distribute a large
  enough number of copies you must also follow the conditions in section 3.
 </para>

 <para>
  You may also lend copies, under the same conditions stated above, and you may
  publicly display copies.
 </para>

 <bridgehead renderas="sect4">
    3. COPYING IN QUANTITY
  </bridgehead>

 <para>
  If you publish printed copies (or copies in media that commonly have printed
  covers) of the Document, numbering more than 100, and the Document's license
  notice requires Cover Texts, you must enclose the copies in covers that
  carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the
  front cover, and Back-Cover Texts on the back cover. Both covers must also
  clearly and legibly identify you as the publisher of these copies. The front
  cover must present the full title with all words of the title equally
  prominent and visible. You may add other material on the covers in addition.
  Copying with changes limited to the covers, as long as they preserve the
  title of the Document and satisfy these conditions, can be treated as
  verbatim copying in other respects.
 </para>

 <para>
  If the required texts for either cover are too voluminous to fit legibly, you
  should put the first ones listed (as many as fit reasonably) on the actual
  cover, and continue the rest onto adjacent pages.
 </para>

 <para>
  If you publish or distribute Opaque copies of the Document numbering more
  than 100, you must either include a machine-readable Transparent copy along
  with each Opaque copy, or state in or with each Opaque copy a
  computer-network location from which the general network-using public has
  access to download using public-standard network protocols a complete
  Transparent copy of the Document, free of added material. If you use the
  latter option, you must take reasonably prudent steps, when you begin
  distribution of Opaque copies in quantity, to ensure that this Transparent
  copy will remain thus accessible at the stated location until at least one
  year after the last time you distribute an Opaque copy (directly or through
  your agents or retailers) of that edition to the public.
 </para>

 <para>
  It is requested, but not required, that you contact the authors of the
  Document well before redistributing any large number of copies, to give them
  a chance to provide you with an updated version of the Document.
 </para>

 <bridgehead renderas="sect4">
    4. MODIFICATIONS
  </bridgehead>

 <para>
  You may copy and distribute a Modified Version of the Document under the
  conditions of sections 2 and 3 above, provided that you release the Modified
  Version under precisely this License, with the Modified Version filling the
  role of the Document, thus licensing distribution and modification of the
  Modified Version to whoever possesses a copy of it. In addition, you must do
  these things in the Modified Version:
 </para>

 <orderedlist numeration="upperalpha" spacing="normal">
  <listitem>
   <para>
    Use in the Title Page (and on the covers, if any) a title distinct from
    that of the Document, and from those of previous versions (which should, if
    there were any, be listed in the History section of the Document). You may
    use the same title as a previous version if the original publisher of that
    version gives permission.
   </para>
  </listitem>
  <listitem>
   <para>
    List on the Title Page, as authors, one or more persons or entities
    responsible for authorship of the modifications in the Modified Version,
    together with at least five of the principal authors of the Document (all
    of its principal authors, if it has fewer than five), unless they release
    you from this requirement.
   </para>
  </listitem>
  <listitem>
   <para>
    State on the Title page the name of the publisher of the Modified Version,
    as the publisher.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve all the copyright notices of the Document.
   </para>
  </listitem>
  <listitem>
   <para>
    Add an appropriate copyright notice for your modifications adjacent to the
    other copyright notices.
   </para>
  </listitem>
  <listitem>
   <para>
    Include, immediately after the copyright notices, a license notice giving
    the public permission to use the Modified Version under the terms of this
    License, in the form shown in the Addendum below.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve in that license notice the full lists of Invariant Sections and
    required Cover Texts given in the Document's license notice.
   </para>
  </listitem>
  <listitem>
   <para>
    Include an unaltered copy of this License.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve the section Entitled "History", Preserve its Title, and add to it
    an item stating at least the title, year, new authors, and publisher of the
    Modified Version as given on the Title Page. If there is no section
    Entitled "History" in the Document, create one stating the title, year,
    authors, and publisher of the Document as given on its Title Page, then add
    an item describing the Modified Version as stated in the previous sentence.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve the network location, if any, given in the Document for public
    access to a Transparent copy of the Document, and likewise the network
    locations given in the Document for previous versions it was based on.
    These may be placed in the "History" section. You may omit a network
    location for a work that was published at least four years before the
    Document itself, or if the original publisher of the version it refers to
    gives permission.
   </para>
  </listitem>
  <listitem>
   <para>
    For any section Entitled "Acknowledgements" or "Dedications", Preserve the
    Title of the section, and preserve in the section all the substance and
    tone of each of the contributor acknowledgements and/or dedications given
    therein.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve all the Invariant Sections of the Document, unaltered in their
    text and in their titles. Section numbers or the equivalent are not
    considered part of the section titles.
   </para>
  </listitem>
  <listitem>
   <para>
    Delete any section Entitled "Endorsements". Such a section may not be
    included in the Modified Version.
   </para>
  </listitem>
  <listitem>
   <para>
    Do not retitle any existing section to be Entitled "Endorsements" or to
    conflict in title with any Invariant Section.
   </para>
  </listitem>
  <listitem>
   <para>
    Preserve any Warranty Disclaimers.
   </para>
  </listitem>
 </orderedlist>

 <para>
  If the Modified Version includes new front-matter sections or appendices that
  qualify as Secondary Sections and contain no material copied from the
  Document, you may at your option designate some or all of these sections as
  invariant. To do this, add their titles to the list of Invariant Sections in
  the Modified Version's license notice. These titles must be distinct from any
  other section titles.
 </para>

 <para>
  You may add a section Entitled "Endorsements", provided it contains nothing
  but endorsements of your Modified Version by various parties--for example,
  statements of peer review or that the text has been approved by an
  organization as the authoritative definition of a standard.
 </para>

 <para>
  You may add a passage of up to five words as a Front-Cover Text, and a
  passage of up to 25 words as a Back-Cover Text, to the end of the list of
  Cover Texts in the Modified Version. Only one passage of Front-Cover Text and
  one of Back-Cover Text may be added by (or through arrangements made by) any
  one entity. If the Document already includes a cover text for the same cover,
  previously added by you or by arrangement made by the same entity you are
  acting on behalf of, you may not add another; but you may replace the old
  one, on explicit permission from the previous publisher that added the old
  one.
 </para>

 <para>
  The author(s) and publisher(s) of the Document do not by this License give
  permission to use their names for publicity for or to assert or imply
  endorsement of any Modified Version.
 </para>

 <bridgehead renderas="sect4">
    5. COMBINING DOCUMENTS
  </bridgehead>

 <para>
  You may combine the Document with other documents released under this
  License, under the terms defined in section 4 above for modified versions,
  provided that you include in the combination all of the Invariant Sections of
  all of the original documents, unmodified, and list them all as Invariant
  Sections of your combined work in its license notice, and that you preserve
  all their Warranty Disclaimers.
 </para>

 <para>
  The combined work need only contain one copy of this License, and multiple
  identical Invariant Sections may be replaced with a single copy. If there are
  multiple Invariant Sections with the same name but different contents, make
  the title of each such section unique by adding at the end of it, in
  parentheses, the name of the original author or publisher of that section if
  known, or else a unique number. Make the same adjustment to the section
  titles in the list of Invariant Sections in the license notice of the
  combined work.
 </para>

 <para>
  In the combination, you must combine any sections Entitled "History" in the
  various original documents, forming one section Entitled "History"; likewise
  combine any sections Entitled "Acknowledgements", and any sections Entitled
  "Dedications". You must delete all sections Entitled "Endorsements".
 </para>

 <bridgehead renderas="sect4">
    6. COLLECTIONS OF DOCUMENTS
  </bridgehead>

 <para>
  You may make a collection consisting of the Document and other documents
  released under this License, and replace the individual copies of this
  License in the various documents with a single copy that is included in the
  collection, provided that you follow the rules of this License for verbatim
  copying of each of the documents in all other respects.
 </para>

 <para>
  You may extract a single document from such a collection, and distribute it
  individually under this License, provided you insert a copy of this License
  into the extracted document, and follow this License in all other respects
  regarding verbatim copying of that document.
 </para>

 <bridgehead renderas="sect4">
    7. AGGREGATION WITH INDEPENDENT WORKS
  </bridgehead>

 <para>
  A compilation of the Document or its derivatives with other separate and
  independent documents or works, in or on a volume of a storage or
  distribution medium, is called an "aggregate" if the copyright resulting from
  the compilation is not used to limit the legal rights of the compilation's
  users beyond what the individual works permit. When the Document is included
  in an aggregate, this License does not apply to the other works in the
  aggregate which are not themselves derivative works of the Document.
 </para>

 <para>
  If the Cover Text requirement of section 3 is applicable to these copies of
  the Document, then if the Document is less than one half of the entire
  aggregate, the Document's Cover Texts may be placed on covers that bracket
  the Document within the aggregate, or the electronic equivalent of covers if
  the Document is in electronic form. Otherwise they must appear on printed
  covers that bracket the whole aggregate.
 </para>

 <bridgehead renderas="sect4">
    8. TRANSLATION
  </bridgehead>

 <para>
  Translation is considered a kind of modification, so you may distribute
  translations of the Document under the terms of section 4. Replacing
  Invariant Sections with translations requires special permission from their
  copyright holders, but you may include translations of some or all Invariant
  Sections in addition to the original versions of these Invariant Sections.
  You may include a translation of this License, and all the license notices in
  the Document, and any Warranty Disclaimers, provided that you also include
  the original English version of this License and the original versions of
  those notices and disclaimers. In case of a disagreement between the
  translation and the original version of this License or a notice or
  disclaimer, the original version will prevail.
 </para>

 <para>
  If a section in the Document is Entitled "Acknowledgements", "Dedications",
  or "History", the requirement (section 4) to Preserve its Title (section 1)
  will typically require changing the actual title.
 </para>

 <bridgehead renderas="sect4">
    9. TERMINATION
  </bridgehead>

 <para>
  You may not copy, modify, sublicense, or distribute the Document except as
  expressly provided for under this License. Any other attempt to copy, modify,
  sublicense or distribute the Document is void, and will automatically
  terminate your rights under this License. However, parties who have received
  copies, or rights, from you under this License will not have their licenses
  terminated so long as such parties remain in full compliance.
 </para>

 <bridgehead renderas="sect4">
    10. FUTURE REVISIONS OF THIS LICENSE
  </bridgehead>

 <para>
  The Free Software Foundation may publish new, revised versions of the GNU
  Free Documentation License from time to time. Such new versions will be
  similar in spirit to the present version, but may differ in detail to address
  new problems or concerns. See
  <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.gnu.org/copyleft/">http://www.gnu.org/copyleft/</link>.
 </para>

 <para>
  Each version of the License is given a distinguishing version number. If the
  Document specifies that a particular numbered version of this License "or any
  later version" applies to it, you have the option of following the terms and
  conditions either of that specified version or of any later version that has
  been published (not as a draft) by the Free Software Foundation. If the
  Document does not specify a version number of this License, you may choose
  any version ever published (not as a draft) by the Free Software Foundation.
 </para>

 <bridgehead renderas="sect4">
    ADDENDUM: How to use this License for your documents
  </bridgehead>

<screen>Copyright (c) YEAR YOUR NAME.
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled “GNU
Free Documentation License”.</screen>

 <para>
  If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts,
  replace the “with...Texts.” line with this:
 </para>

<screen>with the Invariant Sections being LIST THEIR TITLES, with the
Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</screen>

 <para>
  If you have Invariant Sections without Cover Texts, or some other combination
  of the three, merge those two alternatives to suit the situation.
 </para>

 <para>
  If your document contains nontrivial examples of program code, we recommend
  releasing these examples in parallel under your choice of free software
  license, such as the GNU General Public License, to permit their use in free
  software.
</para>
</appendix>
</book>
