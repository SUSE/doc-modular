<?xml version="1.0"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="orig-DC-assembly-alp.xml" version="5.2" xml:lang="en"><info>
 <title>The Adaptable Linux Platform Guide</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Documentation</dm:component>
    <dm:product>ALP</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <!-- 2022-11-07 toba: removing for now
   <dm:editurl>https://github.com/SUSE/doc-modular/edit/main/xml/</dm:editurl>
   -->
   <dm:translation>no</dm:translation>
  </dm:docmanager>
  <!-- can we try to use the same date format here as in other docs? how can we use the    
   <date><?dbtimestamp format="B d, Y"?></date>
   format here? -->
  <pubdate>
   <!-- enter the original publishing date -->
  </pubdate>
  <meta name="updated" content="enter ISO date of last update as YYYY-MM-DD"/>
  <meta name="time-to-read" content="enter time to read in minutes"/>
  <meta name="bugtracker"><phrase role="url">https://bugzilla.suse.com/enter_bug.cgi</phrase><phrase role="component">Non-product-specific documentation</phrase><phrase role="product">Smart Docs</phrase><phrase role="assignee">tbazant@suse.com</phrase>
  </meta>
  <meta name="translation"><phrase role="trans">no</phrase><phrase role="language">
    <!-- comma-separated list of languages, for example en-us,de-de,cs-cz --></phrase>
  </meta>
  <meta name="refers-to" content="enter id of parent chapter or article"/>
  <meta name="architecture" content=""/>
  <meta name="Adaptable Linux Platform"><productname version="1">Adaptable Linux Platform</productname>
  </meta>
  <meta name="title">Adaptable Linux Platform</meta>
  <meta name="description">The Adaptable Linux Platform (ALP) is a lightweight operating system. Instead of applications distributed in traditional software packages, it runs containerized and virtualized workloads.</meta>
  <meta name="social-descr"/>
  <meta name="targetgroup" content=""/>
  <meta name="function" content=""/>
  <meta name="task" content=""/>
  <meta name="category" content=""/>
  <!--        <xi:include href="common_copyright_gfdl.xml"/>-->
  <abstract>
   <para>
    This guide introduces the Adaptable Linux Platform (ALP)—its deployment, system
    management and software installation as well as running of containerized
    workloads. To enhance this ALP documentation, find its sources at
    <link xlink:href="https://github.com/SUSE/doc-modular/edit/main/xml/"/>.
   </para>
   <variablelist>
    <varlistentry>
     <term>WHAT?</term>
     <listitem>
      <para>
       ALP is a lightweight operating system. Instead of
       applications distributed in traditional software packages, it runs
       containerized and virtualized workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WHY?</term>
     <listitem>
      <para>
       This guide introduces an overview of what ALP is and how it is
       different from traditional operating systems. It also describes how to
       administer ALP and install and manage individual workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>EFFORT?</term>
     <listitem>
      <para>
       To understand the concepts and perform tasks described in this guide,
       you need to have good knowledge and practice with the Linux
       operating system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>GOAL!</term>
     <listitem>
      <para>
       After having read this guide, you will be able to deploy ALP,
       modify its file system in a transactional way, and install and run
       specific workloads on top of it.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </abstract>
 </info>
 
<chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp"><info><title xmlns:its="http://www.w3.org/2005/11/its">General description</title></info>
  
  <section xml:id="what-is-alp">
    <title>What is ALP?</title>
    <para>
     The Adaptable Linux Platform (ALP) is a lightweight operating system. Its stability
     and software consistence is ensured by a read-only root file system
     modified by transactional updates. Instead of applications distributed in
     traditional software packages, it runs containerized and virtualized
     workloads.
    </para>
  </section>
  <section xml:id="how-it-works-alp">
    <title>Core components of ALP</title>
    <para>
      The Adaptable Linux Platform (ALP) consists of the following components:
    </para>
    <variablelist>
      <varlistentry>
        <term>Base operating system</term>
        <listitem>
          <para>
            The core of ALP which runs all required services. It is an
            immutable operating system with a read-only root file system. The
            file system is modified by transactional updates which utilize the
            snapshotting feature of BTRFS.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Transactional updates</term>
        <listitem>
          <para>
           The <command>transactional-update</command> command performs changes on the file system. You can
           use it, for example, to install new software or apply software
           patches. Because it uses file system snapshots, applied changes can
           be easily rolled back.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Container orchestration</term>
        <listitem>
          <para>
            ALP runs containerized workloads instead of applications
            packed in software packages. The default container orchestrator in
            ALP is Podman which is responsible for managing containers
            and container images.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Containerized workloads</term>
        <listitem>
          <para>
            Workloads replace traditional applications. A containerized
            workload contains all software dependencies required to run a
            specific application or tool.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Cockpit</term>
        <listitem>
          <para>
            A Web-based graphical interface to administer single or multiple
            ALP workloads from one place. It helps you manage, for
            example, user accounts, network settings, or container
            orchestration.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="benefits-alp">
    <title>Benefits of ALP</title>
    <para>
      The Adaptable Linux Platform offers the following customer benefits:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          High security of running workloads.
        </para>
      </listitem>
      <listitem>
        <para>
          Minimal maintenance with keeping the workloads up to date.
        </para>
      </listitem>
      <listitem>
        <para>
          Stable immutable base operating system that utilizes transactions
          when modifying the file system.
        </para>
      </listitem>
      <listitem>
        <para>
          Ability to roll back modifications on the file system in case the
          transaction result is undesirable.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-alp">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Find more details about ALP deployment in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Transactional updates are detailed in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman is introduced in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Available workloads are described in
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp-deployment"><info><title xmlns:its="http://www.w3.org/2005/11/its">Deployment</title></info>
  
  <section xml:id="what-is-alp-deployment">
    <title>Introduction</title>
    <para>
      The Adaptable Linux Platform (ALP) is distributed either as a disk image of the
      ALP installer named <emphasis>D-Installer</emphasis>, or as a
      pre-built ALP raw disk image.
    </para>
    <section xml:id="alp-deployment-dinstaller">
      <title>D-Installer</title>
      <para>
        While D-Installer handles both bare-metal and virtualized deployments,
        it is a preferred method for bare-metal deployments. ALP
        deployment using D-Installer is similar to a traditional operating
        system setup. After booting the D-Installer image, the installer uses
        a graphical user-friendly interface to walk you through the system
        configuration and deployment.
      </para>
    </section>
    <section xml:id="alp-deployment-raw-image">
      <title>Raw disk image</title>
      <para>
        This method handles both bare-metal and virtualized deployment as well.
        It is different from the D-Installer deployment in that you do not
        boot an installer but the actual ALP image itself. On first
        boot, you can configure basic system options using an
        <emphasis>ncurses</emphasis> user interface. Using a raw disk image, you
        can fine-tune the deployment setup with Combustion and Ignition
        tools.
      </para>
    </section>
  </section>
  <section xml:id="requirements-deploy-alp-raw-image">
    <title>Hardware requirements</title>
    <para>
      The minimum supported hardware requirements for deploying ALP
      follow:
    </para>
    <variablelist>
      <varlistentry>
        <term>CPU</term>
        <listitem>
          <para>
            AMD64/Intel 64 and AArch64 CPU architectures are supported.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Maximum number of CPUs</term>
        <listitem>
          <para>
            The maximum number of CPUs supported by software design is 8192.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Memory</term>
        <listitem>
          <para>
            ALP requires at least 1 GB RAM. Bear in mind that this
            is a minimal value for the operation system, the actual memory size
            depends on the workload.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Hard disk</term>
        <listitem>
          <para>
            The minimum hard disk space is 12 GB, while the recommended
            value is 20 GB of hard disk space. Adjust the value according to
            the workloads of your containers.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="related-alp-deployment">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          For deploying ALP with D-Installer, refer to
          <xref linkend="task-deploy-alp-dinstaller"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For deploying the raw disk image with minimal configuration, refer to
          <xref linkend="task-deploy-alp-raw-image"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For adjusting the deployment with Ignition, refer to
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For adjusting the deployment with Combustion, refer to
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          List
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-prepare-alp-vm"><info><title xmlns:its="http://www.w3.org/2005/11/its">Preparing an ALP virtual machine</title></info>
  
  <section xml:id="introduction-prepare-alp-vm">
    <title>Introduction</title>
    <para>
      This article describes how to configure a new virtual machine suitable
      for the ALP deployment by using the Virtual Machine Manager.
    </para>
  </section>
  <section xml:id="requirements-prepare-alp-vm">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          A VM Host Server with KVM hypervisor.
        </para>
      </listitem>
      <listitem>
        <para>
          Depending on the installation method, download either the ALP
          raw disk image from https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images or the D-Installer image from
          https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/iso on the VM Host Server where you will run
          virtualized ALP.
        </para>
        <note>
          <para>
            Note that for the raw disk image deployment, there are two types of
            images, depending on whether you intend to run ALP on an
            encrypted disk or an unencrypted disk.
          </para>
        </note>
      </listitem>
    </itemizedlist>
    <important>
      <title>Encrypted image does not expand to the full disk capacity</title>
      <para>
        As of now, the encrypted raw disk image does not expand to the full
        disk capacity automatically. As a workaround, the following steps are
        required:
      </para>
      <procedure>
        <step>
          <para>
            Use the <command>qemu-img</command> command to increase the disk
            image to the desired size.
          </para>
        </step>
        <step>
          <para>
            Set up the virtual machine and boot it. When the JeOS Firstboot
            wizard asks you which method to use for encryption, select
            <guimenu>passphrase</guimenu>.
          </para>
        </step>
        <step>
          <para>
            When the system is ready, use the <command>parted</command> command
            to resize the partition where the LUKS device resides (for example,
            partition number 3) to the desired size.
          </para>
        </step>
        <step>
          <para>
            Run the <command>cryptsetup resize luks</command> command. When
            asked, enter the passphrase to resize the encrypted device.
          </para>
        </step>
        <step>
          <para>
            Run the <command>transactional-update shell</command> command to
            open a read-write shell in the current disk snapshot. Then resize
            the BTRFS file system to the desired size, for example:
          </para>
<screen><prompt role="root"># </prompt>btrfs fi resize max /</screen>
        </step>
        <step>
          <para>
            Leave the shell with <command>exit</command> and reboot the system
            with <command>reboot</command>.
          </para>
        </step>
      </procedure>
    </important>
  </section>
  <section xml:id="requirements-prepare-alp-vm-">
    <title>Configuring a virtual machine for ALP deployment</title>
    <procedure>
      <step>
        <para>
          Start Virtual Machine Manager and select
          <menuchoice><guimenu>File</guimenu><guimenu>New Virtual
          Machine</guimenu></menuchoice>.
        </para>
        <substeps>
          <step>
            <para>
              For deployment using D-Installer, select <guimenu>Local install
              media</guimenu>.
            </para>
          </step>
          <step>
            <para>
              For the raw disk deployment, select <guimenu>Import existing disk
              image</guimenu>.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          Confirm with <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the path to the ALP disk image that you previously
          downloaded and the type of linux OS you are deploying, for example,
          <literal>Generic Linux 2020</literal>. Confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the amount of memory and number of processors that you want
          to assign to the ALP virtual machine and confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          For deployment using D-Installer, enable storage for the virtual
          machine and specify the size of the disk image.
        </para>
      </step>
      <step>
        <para>
          Specify the name for the virtual machine and the network to be used.
        </para>
      </step>
      <step>
        <para>
          If you are deploying an encrypted ALP image, perform these
          additional steps:
        </para>
        <substeps>
          <step>
            <para>
              Enable <guimenu>Customize configuration before install</guimenu>
              and confirm with <guimenu>Finish</guimenu>.
            </para>
          </step>
          <step>
            <para>
              Click <guimenu>Overview</guimenu> from the left menu and change
              the boot method from BIOS to UEFI for secure boot. Confirm with
              <guimenu>Apply</guimenu>.
            </para>
            <figure>
              <title>Set UEFI firmware for the encrypted ALP image</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Set UEFI firmware for the encrypted ALP image</phrase>
                </textobject>
              </mediaobject>
            </figure>
          </step>
          <step>
            <para>
              Add a Trusted Platform Module (TPM) device. Click <guimenu>Add
              Hardware</guimenu>, select <guimenu>TPM</guimenu> from the left
              menu, and select the <guimenu>Emulated</guimenu> type.
            </para>
            <figure>
              <title>Add an emulated TPM device</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Add an emulated
                TPM device</phrase>
                </textobject>
              </mediaobject>
            </figure>
            <para>
              Confirm with <guimenu>Finish</guimenu> and start the ALP
              deployment by clicking <guimenu>Begin Installation</guimenu> from
              the top menu.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <substeps>
          <step>
            <para>
              For the raw disk image deployment, to deploy ALP with only
              minimal setup options, confirm with <guimenu>Finish</guimenu>.
              The disk image will be booted and JeOS Firstboot will take care
              of the deployment. Refer to
              <xref linkend="deploy-alp-jeos-firstboot"/> for next
              steps.
            </para>
            <tip>
              <para>
                You can detail the deployment setup by using the Ignition or
                Combustion tools. For more details, refer to
                <xref linkend="concept-configure-ignition"/> and
                <xref linkend="concept-configure-combustion"/>.
              </para>
            </tip>
          </step>
          <step>
            <para>
              To continue the deployment by using D-Installer, confirm with
              <guimenu>finish</guimenu> and continue with
              <xref linkend="task-deploy-alp-dinstaller"/>.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
  </section>
  <section xml:id="summary-prepare-alp-vm">
    <title>Summary</title>
    <para>
      Now your virtualization environment is ready for the ALP
      deployment process.
    </para>
  </section>
  <section xml:id="next-prepare-alp-vm">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          For deployment using D-Installer, refer to
          <xref linkend="task-deploy-alp-dinstaller"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For raw disk deployment, continue with
          <xref linkend="deploy-alp-jeos-firstboot"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-prepare-alp-vm">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Advanced configuration of ALP deployment is described in
          <xref linkend="concept-configure-ignition"/> and
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find detailed information about using the Virtual Machine Manager in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
          and
          <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp-dinstaller"><info><title xmlns:its="http://www.w3.org/2005/11/its">Deploying ALP using D-Installer</title></info>
  
  <section xml:id="introduction-deploy-alp-dinstaller">
    <title>Introduction</title>
    <para>
      This article describes how to deploy the Adaptable Linux Platform (ALP) using
      D-Installer.
    </para>
  </section>
  <section xml:id="deploying-alp-using-dinstaller">
    <title>Deploying ALP using D-Installer</title>
    <procedure>
      <step>
        <para>
          Download the ALP D-Installer image from
          https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/iso. For example:
        </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/isod-installer-live.x86_64-0.6-ALP-Build3.6.iso</screen>
      </step>
      <step>
        <para>
          If you are deploying ALP as a VM Guest, you need to first
          prepare the virtual machine. To do this, follow the steps in
          <xref linkend="task-prepare-alp-vm"/>.
        </para>
      </step>
      <step>
        <para>
          After booting the D-Installer image, select
          <guimenu>d-installer-live</guimenu> from the boot menu. A graphical
          installer will appear with configuration options affecting the
          ALP deployment.
        </para>
        <figure>
          <title>The D-Installer GUI</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-welcome.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-welcome.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>The D-Installer GUI</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Click the default language and select your preferred language from
          the drop-down list.
        </para>
      </step>
      <step>
        <para>
          Configure network settings. Click the default wired connection and
          configure the network according to your needs. You can, for example,
          change the networking mode to <guimenu>Manual</guimenu>, add IP
          addresses and related prefixes or netmasks, gateway, and DNS servers.
        </para>
        <figure>
          <title>Configuring a wired connection</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-network-wired.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-network-wired.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Configuring a wired
            connection</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <para>
          By clicking <guimenu>Connect to a Wi-Fi network</guimenu> you can
          utilize your local wireless network.
        </para>
      </step>
      <step>
        <para>
          Configure storage by clicking <guimenu>Edit storage
          settings</guimenu>.
        </para>
        <figure>
          <title>Configuring ALP storage</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-storage.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-storage.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Configuring ALP storage</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <para>
          You can select a device for ALP installation or specify
          advanced storage settings, for example, whether to use LVM or
          encrypted devices.
        </para>
        <figure>
          <title>Advanced storage settings</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-storage-advanced.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-storage-advanced.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Advanced
            storage settings</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <tip>
         <para>
          If you enable disk encryption, you will be asked for a decryption
          password on each reboot. Because the GRUB 2 boot loader does not
          enable switching keyboard layouts, select a password made of
          alphanumeric characters and be aware of national keyboard layout
          differences. For extended post-deployment information about
          disk encryption, refer to <xref linkend="alp-post-deploy-full-disk-encryption"/>.
         </para>
        </tip>
      </step>
      <step>
        <para>
          In the <guimenu>Users</guimenu> section, specify a <systemitem class="username">root</systemitem>
          password, upload a <guimenu>Root SSH public key</guimenu>, or create
          an additional user account and enable auto-login for it.
        </para>
        <figure>
          <title>Creating a user account</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-user.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-user.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>Creating a user account</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          To begin the installation, click <guimenu>Install</guimenu> and
          confirm with <guimenu>Continue</guimenu>. The installation process
          will start.
        </para>
        <figure>
          <title>ALP installation in progress</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-dinstaller-installation.png" width="50%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-dinstaller-installation.png" width="50%"/>
            </imageobject>
            <textobject role="description"><phrase>ALP installation in progress</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <para>
          After the installation is finished, click <guimenu>Reboot</guimenu>
          and select <guimenu>ALP</guimenu> from the boot menu after reboot.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="summary-deploy-alp-dinstaller">
    <title>Summary</title>
    <para>
      After the deployment of ALP is finished, you are presented with
      the login prompt. Log in as <systemitem class="username">root</systemitem>, and you are ready to set up the
      system and install additional workloads.
    </para>
  </section>
  <section xml:id="next-deploy-alp-dinstaller">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Install additional software with <command>transactional-update</command>. Refer to
          <xref linkend="concept-transactional-update"/> for more details.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. Refer to
          <xref linkend="reference-available-alp-workloads"/> for more
          details.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-deploy-alp-dinstaller">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          General description of ALP is included in
          <xref linkend="concept-alp"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Advanced configuration of ALP deployment is described in
          <xref linkend="concept-configure-ignition"/> and
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find detailed information about using the Virtual Machine Manager in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
          and
          <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp-raw-image"><info><title xmlns:its="http://www.w3.org/2005/11/its">Deploying ALP using a raw disk image</title></info>
  
  <section xml:id="introduction-deploy-alp-raw-image">
    <title>Introduction</title>
    <para>
      This article describes how to deploy the Adaptable Linux Platform (ALP) raw disk
      image. It applies to ALP running both on encrypted and unencrypted
      disk.
    </para>
    <section xml:id="alp-deployment-firstboot-detection">
      <title>First boot detection</title>
      <para>
        The deployment configuration runs on the first boot only. To
        distinguish between the first and subsequent boots, the flag file
        <filename>/boot/writable/firstboot_happened</filename> is created after
        the first boot finishes. If the file is not present in the file system,
        the attribute <literal>ignition.firstboot</literal> is passed to the
        kernel command line and thus both Ignition and Combustion are
        triggered to run (in the initrd). After completing the first boot, the
        <filename>/boot/writable/firstboot_happened</filename> flag file is
        created.
      </para>
      <note>
        <title>The flag file is always created</title>
        <para>
          Even though the configuration may not be successful due to improper
          or missing configuration files, the
          <filename>/boot/writable/firstboot_happened</filename> flag file is
          created.
        </para>
      </note>
      <tip>
        <para>
          You may force the first boot configuration on subsequent boot by
          passing the <literal>ignition.firstboot</literal> attribute to the
          kernel command line or by deleting the
          <filename>/boot/writable/firstboot_happened</filename> flag file.
        </para>
      </tip>
    </section>
    <section xml:id="alp-deployment-default-partitioning">
      <title>Default partitioning</title>
      <para>
        The pre-built images are delivered with a default partitioning scheme.
        You can change it during the first boot by using Ignition or
        Combustion.
      </para>
      <important>
        <title>BTRFS is mandatory for the root file system</title>
        <para>
          If you intend to perform any changes to the default partitioning
          scheme, the root file system must be BTRFS.
        </para>
      </important>
      <para>
        Each image has the following subvolumes:
      </para>
<screen>
 /home
 /root
 /opt
 /srv
 /usr/local
 /var
 </screen>
      <para>
        The <literal>/etc</literal> directory is mounted as overlayfs, where
        the upper directory is mounted to
        <filename>/var/lib/overlay/1/etc/</filename>.
      </para>
      <para>
        You can recognize the subvolumes mounted by default by the option
        <literal>x-initrd.mount</literal> in <filename>/etc/fstab</filename>.
        Other subvolumes or partitions must be configured either by Ignition
        or Combustion.
      </para>
    </section>
  </section>
  <section xml:id="deploy-alp-jeos-firstboot">
    <title>Deploying ALP with JeOS Firstboot</title>
    <tip>
      <para>
        When booting the ALP raw image for the first time,
        <emphasis>JeOS Firstboot</emphasis> enables you to perform a minimal
        configuration of your system. If you need more control over the
        deployment process, find more information in
        <xref linkend="concept-configure-ignition"/> and
        <xref linkend="concept-configure-combustion"/>.
      </para>
    </tip>
    <procedure>
      <step>
        <para>
          Download the ALP raw disk image from https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images. There are
          two types of images, depending on whether you intend to run ALP
          on an encrypted disk or an unencrypted disk.
        </para>
        <para>
          For example, for the unencrypted image:
        </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/imagesALP-VM.x86_64-0.0.1-kvm-Build15.17.qcow2</screen>
        <para>
          And for the encrypted image:
        </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/imagesALP-VM.x86_64-0.0.1-kvm_encrypted-Build15.18.qcow2</screen>
      </step>
      <step>
        <para>
          If you are deploying ALP as a VM Guest, you need to first
          prepare the virtual machine by following
          <xref linkend="task-prepare-alp-vm"/>.
        </para>
      </step>
      <step>
        <para>
          After booting the ALP disk image, you will be presented with a
          boot loader screen. Select <guimenu>ALP</guimenu> and confirm
          with <keycap function="enter"/>.
        </para>
        <figure>
          <title>ALP boot screen</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-1.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-1.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>ALP boot screen</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          <guimenu>JeOS Firstboot</guimenu> displays a welcome screen. Confirm
          with <keycap function="enter"/>.
        </para>
        <figure>
          <title>JeOS Firstboot screen</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>JeOS Firstboot screen</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          On the next screens, select keyboard, confirm the license agreement
          and select the time zone.
        </para>
      </step>
      <step>
        <para>
          In the <guimenu>Enter root password</guimenu> dialog window, enter a
          password for the <systemitem class="username">root</systemitem> and confirm it.
        </para>
        <figure>
          <title>Enter root password</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Enter root password</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          When deploying with an encrypted disk, follow these additional steps:
        </para>
        <substeps>
          <step>
            <para>
              Select the desired protection method and confirm with
              <guimenu>OK</guimenu>.
            </para>
            <figure>
              <title>Select method for encryption</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-passkey.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-passkey.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Select method for encryption</phrase>
                </textobject>
              </mediaobject>
            </figure>
          </step>
          <step>
            <para>
              Enter a recovery password for LUKS encryption and retype it. The
              root file system re-encryption will begin.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          ALP is successfully deployed using a minimal initial
          configuration.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="summary-deploy-alp-raw-image">
    <title>Summary</title>
    <para>
      After the deployment of ALP is finished, you are presented with
      the login prompt. Log in as <systemitem class="username">root</systemitem>, and you are ready to set up the
      system and install additional workloads.
    </para>
  </section>
  <section xml:id="next-deploy-alp-raw-image">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Install additional software with <command>transactional-update</command>. Refer to
          <xref linkend="concept-transactional-update"/> for more details.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. Refer to
          <xref linkend="reference-available-alp-workloads"/> for more
          details.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-deploy-alp-raw-image">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          General description of ALP is included in
          <xref linkend="concept-alp"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Advanced configuration of ALP deployment is described in
          <xref linkend="concept-configure-ignition"/> and
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find detailed information about using the Virtual Machine Manager in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
          and
          <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-ignition"><info><title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Ignition</title></info>
  
  <!-- highly inspired by sle/xml/deployment_images_ignition.xml -->
  <section xml:id="what-is-ignition">
    <title>What is Ignition?</title>
    <para>
      <link xlink:href="https://coreos.github.io/ignition/">Ignition</link>
      is a provisioning tool that enables you to configure a system according
      to your specification on the first boot.
    </para>
  </section>
  <section xml:id="how-it-works-ignition">
    <title>How does Ignition work?</title>
    <para>
      When the system is booted for the first time, Ignition is loaded as
      part of an <filename>initramfs</filename> and searches for a
      configuration file within a specific directory (on a USB flash disk, or
      you can provide a URL). All changes are performed before the kernel
      switches from the temporary file system to the real root file system
      (before the <literal>switch_root</literal> command is issued).
    </para>
    <para>
      Ignition uses a configuration file in the JSON format named
      <filename>config.ign</filename>. You can either write the configuration
      manually or use the Fuel Ignition Web application at
      <link xlink:href="https://opensuse.github.io/fuel-ignition"/> to generate it.
    </para>
    <important>
      <para>
        Note that Fuel Ignition does not cover the complete Ignition
        vocabulary yet, and the resulting JSON file may need some manual
        tweaking.
      </para>
    </important>
    <tip>
      <para>
        If you decide to write the Ignition configuration manually and prefer
        the YAML format over JSON, you can create a YAML file and convert this
        file to JSON using a <literal>Butane</literal> tool. For details, refer
        to <xref linkend="task-convert-yaml-to-json"/>.
      </para>
    </tip>
    <section xml:id="sec-ignition-configuration">
      <title><filename>config.ign</filename></title>
      <para>
        When installing on bare metal, the configuration file
        <filename>config.ign</filename> must reside in the
        <filename>ignition</filename> subdirectory on the configuration media,
        for example, a USB stick labeled <literal>ignition</literal>. The
        directory structure must look as follows:
      </para>
<screen>
&lt;root directory&gt;
└── ignition
    └── config.ign

 </screen>
      <tip>
        <para>
          To create a disk image with the Ignition configuration, you can use
          the Fuel Ignition Web application at https://opensuse.github.io/fuel-ignition.
        </para>
      </tip>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>config.ign</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/com.coreos/config" file="/location/to/config.ign"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <para>
        The <filename>config.ign</filename> contains various data types:
        objects, strings, integers, booleans and lists of objects. For a
        complete specification, refer to
        <link xlink:href="https://coreos.github.io/ignition/configuration-v3_3/">Ignition
        specification v3.3.0</link>.
      </para>
      <para>
        The <literal>version</literal> attribute is mandatory and in case of
        ALP, its value must be set either to <literal>3.3.0</literal> or
        to any lower version. Otherwise, Ignition will fail.
      </para>
      <para>
        If you want to log in to your system as <systemitem class="username">root</systemitem>, you must at least
        include a password for <systemitem class="username">root</systemitem>. However, it is recommended to
        establish access via SSH keys. To configure a password, make sure to
        use a secure one. If you use a randomly generated password, use at
        least 10 characters. If you create your password manually, use even
        more than 10 characters and combine uppercase and lowercase letters and
        numbers.
      </para>
    </section>
  </section>
  <section xml:id="related-configure-ignition">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Converting Ignition YAML to JSON is described in
          <xref linkend="task-convert-yaml-to-json"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Examples of Ignition configuration are introduced in
          <xref linkend="reference-ignition-configuration"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Configuring the JeOS Firstboot with Combustion is described in
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-convert-yaml-to-json"><info><title xmlns:its="http://www.w3.org/2005/11/its">Converting YAML formatted files into JSON</title></info>
  
  <section xml:id="introduction-convert-yaml-to-json">
    <title>Introduction</title>
    <para>
      JSON is a universal file format for storing structured data.
      Applications, for example, Ignition, use it to store and retrieve their
      configuration. Because JSON's syntax is complex and hard to read for
      human beings, you can write the configuration in a more friendly format
      called YAML and then convert it into JSON.
    </para>
  </section>
  <section xml:id="sec-converting-config">
    <title>Converting YAML files into JSON format</title>
    <para>
      The tool that converts Ignition-specific vocabularies in YAML files
      into JSON format is <literal>butane</literal>. It also verifies the
      syntax of the YAML file to catch potential errors in the structure. For
      the latest version of <literal>butane</literal>, add the following
      repository:
    </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command>  zypper ar -f \
  https://download.opensuse.org/repositories/devel:/kubic:/ignition/openSUSE_Tumbleweed/ \
  devel_kubic_ignition
</screen>
    <para>
      Replace <literal>openSUSE_Tumbleweed</literal> with one of the following
      (depending on your distribution):
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>'openSUSE_Leap_$releasever'</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>15.3</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Now you can install the <literal>butane</literal> tool:
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command>  zypper ref &amp;&amp; zypper in butane</screen>
    <para>
      After the installation is complete, you can invoke
      <literal>butane</literal> by running:
    </para>
<screen><prompt>&gt; </prompt> butane -p -o config.ign config.fcc</screen>
    <itemizedlist>
      <listitem>
        <para>
          <filename>config.fcc</filename> is the path to the YAML configuration
          file.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>config.ign</filename> is the path to the output JSON
          configuration file.
        </para>
      </listitem>
      <listitem>
        <para>
          The <option>-p</option> command option adds line breaks to the output
          file and thus makes it more readable.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="summary-convert-yaml-to-json">
    <title>Summary</title>
    <para>
      After you completed the described steps, you can write and store
      configuration files in YAML format while providing them in JSON format if
      applications, for example, Ignition, require it.
    </para>
  </section>
  <section xml:id="related-convert-yaml-to-json">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Configuration using Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Examples of the Ignition configuration are included in
          <xref linkend="reference-ignition-configuration"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-ignition-configuration"><info><title xmlns:its="http://www.w3.org/2005/11/its">Ignition configuration examples</title></info>
  
  <section xml:id="sec-ignition-examples">
    <title>Configuration examples</title>
    <para>
      This section will provide you with some common examples of the Ignition
      configuration in both the native JSON format and the YAML format in
      addition. Note that Ignition does not accept configuration in the YAML
      format, and you need to convert it to the JSON format. To do so, you can
      use the <literal>butane</literal> tool as described in
      <xref linkend="task-convert-yaml-to-json"/>.
    </para>
    <important>
      <para>
        <xref linkend="alp-deployment-default-partitioning"/> lists subvolumes
        that are mounted by default when running the pre-built image. If you
        want to add a new user or modify any of the files on a subvolume that
        is not mounted by default, you need to declare such subvolume first so
        that it is mounted as well. Find more details about mounting file
        systems in <xref linkend="sec-storage-filesystem"/>.
      </para>
    </important>
    <note>
      <title>The <literal>version</literal> attribute is mandatory</title>
      <para>
        Each <filename>config.fcc</filename> must include version 1.4.0 or
        lower that is then converted to the corresponding Ignition
        specification.
      </para>
    </note>
    <section xml:id="sec-ignition-storage">
      <title>Storage configuration</title>
      <para>
        The <literal>storage</literal> attribute is used to configure
        partitions, RAID, define file systems, create files, etc. To define
        partitions, use the <literal>disks</literal> attribute. The
        <literal>filesystems</literal> attribute is used to format partitions
        and define mount points of particular partitions. The
        <literal>files</literal> attribute can be used to create files in the
        file system. Each of the mentioned attributes is described in the
        following sections.
      </para>
      <section xml:id="sec-storage-disks">
        <title>The <literal>disks</literal> attribute</title>
        <para>
          The <literal>disks</literal> attribute is a list of devices that
          enables you to define partitions on these devices. The
          <literal>disks</literal> attribute must contain at least one
          <literal>device</literal>, other attributes are optional. The
          following example will use a single virtual device and divide the
          disk into four partitions:
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "disks": [
      {
        "device": "/dev/vda",
        "partitions": [
          {
            "label": "root",
            "number": 1,
            "typeGuid": "4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709"
          },
          {
            "label": "boot",
            "number": 2,
            "typeGuid": "BC13C2FF-59E6-4262-A352-B275FD6F7172"
          },
          {
            "label": "swap",
            "number": 3,
            "typeGuid": "0657FD6D-A4AB-43C4-84E5-0933C84B4F4F"
          },
          {
            "label": "home",
            "number": 4,
            "typeGuid": "933AC7E1-2EB4-4F13-B844-0E14E2AEF915"
          }
        ],
        "wipeTable": true
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  disks:
    - device: "/dev/vda"
      wipe_table: true
      partitions:
       - label: root
         number: 1
         type_guid: 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709
       - label: boot
         number: 2
         type_guid: BC13C2FF-59E6-4262-A352-B275FD6F7172
       - label: swap
         number: 3
         type_guid: 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
       - label: home
         number: 4
         type_guid: 933AC7E1-2EB4-4F13-B844-0E14E2AEF915
 </screen>
      </section>
      <section xml:id="sec-storage-raid">
        <title>The <literal>raid</literal> attribute</title>
        <para>
          The <literal>raid</literal> is a list of RAID arrays. The following
          attributes of <literal>raid</literal> are mandatory:
        </para>
        <variablelist>
          <varlistentry>
            <term>level</term>
            <listitem>
              <para>
                a level of the particular RAID array (linear, raid0, raid1,
                raid2, raid3, raid4, raid5, raid6)
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>devices</term>
            <listitem>
              <para>
                a list of devices in the array referenced by their absolute
                paths
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>name</term>
            <listitem>
              <para>
                a name that will be used for the md device
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "raid": [
      {
        "devices": [
          "/dev/sda",
          "/dev/sdb"
        ],
        "level": "raid1",
        "name": "system"
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  raid:
    - name: system
      level: raid1
      devices:
        - "/dev/sda"
        - "/dev/sdb"
 </screen>
      </section>
      <section xml:id="sec-storage-filesystem">
        <title>The <literal>filesystems</literal> attribute</title>
        <para>
          <literal>filesystems</literal> must contain the following attributes:
        </para>
        <variablelist>
          <varlistentry>
            <term>device</term>
            <listitem>
              <para>
                the absolute path to the device, typically
                <literal>/dev/sda</literal> in case of physical disk
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>format</term>
            <listitem>
              <para>
                the file system format (btrfs, ext4, xfs, vfat or swap)
              </para>
              <note>
                <para>
                  In case of ALP, the <literal>root</literal> file
                  system must be formatted to btrfs.
                </para>
              </note>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>
          The following example demonstrates using the
          <literal>filesystems</literal> attribute. The
          <filename>/opt</filename> directory will be mounted to the
          <literal>/dev/sda1</literal> partition, which is formatted to btrfs.
          The device will not be erased.
        </para>
        <para>
          JSON
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "filesystems": [
      {
        "device": "/dev/sda1",
        "format": "btrfs",
        "path": "/opt",
        "wipeFilesystem": false
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  filesystems:
    - path: /opt
      device: "/dev/sda1"
      format: btrfs
      wipe_filesystem: false
 </screen>
        <para>
          Normally, a regular user's home directory is located in the
          <filename>/home/<replaceable>USER_NAME</replaceable></filename>
          directory. Since <filename>/home</filename> is not mounted by default
          in the initrd, the mount has to be explicitly defined for the user
          creation to succeed:
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.1.0"
  },
  "passwd": {
    "users": [
      {
        "name": "root",
        "passwordHash": "$6$Dxkc092R4JdlFeLE$bfO3TPV1n3a4I1to1/2EkfvU2GiSKpR...",
        "sshAuthorizedKeys": [
          "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCpNOU+nwWRnZYoMV3biUgCC..."
        ]
      }
    ]
  },
  "storage": {
    "filesystems": [
      {
        "device": "/dev/sda3",
        "format": "btrfs",
        "mountOptions": [
          "subvol=/@/home"
        ],
        "path": "/home",
        "wipeFilesystem": false
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.1.0
storage:
  filesystems:
    - path: /home
      device: /dev/sda3
      format: btrfs
      wipe_filesystem: false
      mount_options:
       - "subvol=/@/home"
passwd:
  users:
   - name: root
     password_hash: $6$Dxkc092R4JdlFeLE$bfO3TPV1n3a4I1to1/2EkfvU2GiSKpR...
     ssh_authorized_keys:
       - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCpNOU+nwWRnZYoMV3biUgCC...
</screen>
      </section>
      <section xml:id="sec-storage-files">
        <title>The <literal>files</literal> attribute</title>
        <para>
          You can use the <literal>files</literal> attribute to create any
          files on your machine. Bear in mind that if you want to create files
          outside the default partitioning schema, you need to define the
          directories by using the <literal>filesystems</literal> attribute.
        </para>
        <para>
          In the following example, a host name is created by using the
          <literal>files</literal> attribute. The file
          <filename>/etc/hostname</filename> will be created with the
          <emphasis>alp-1</emphasis> host name:
        </para>
        <important>
          <para>
            Note that the file mode specification is different for JSON and
            YAML. While JSON accepts file modes in decimal numbers, for
            example, <literal>420</literal>, YAML accepts octal numbers
            (<literal>0644</literal>).
          </para>
        </important>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "files": [
      {
        "overwrite": true,
        "path": "/etc/hostname",
        "contents": {
          "source": "data:,alp-1"
        },
        "mode": 420
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      overwrite: true
      contents:
        inline: "alp-1"
 </screen>
      </section>
      <section xml:id="sec-storage-directories">
        <title>The <literal>directories</literal> attribute</title>
        <para>
          The <literal>directories</literal> attribute is a list of directories
          that will be created in the file system. The
          <literal>directories</literal> attribute must contain at least one
          <literal>path</literal> attribute.
        </para>
        <para>
          JSON:
        </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "storage": {
    "directories": [
      {
        "path": "/home/tux",
        "user": {
          "name": "tux"
        }
      }
    ]
  }
}
</screen>
        <para>
          YAML:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  directories:
    - path: /home/tux
      user:
        name: tux
 </screen>
      </section>
    </section>
    <section xml:id="sec-ignition-users">
      <title>Users administration</title>
      <para>
        The <literal>passwd</literal> attribute is used to add users. If you
        intend to log in to your system, create <systemitem class="username">root</systemitem> and set the
        <systemitem class="username">root</systemitem>'s password and/or add the SSH key to the Ignition
        configuration. You need to hash the <systemitem class="username">root</systemitem> password, for example by
        using the <command>openssl</command> command:
      </para>
<screen>
 openssl passwd -6
 </screen>
      <para>
        The command creates a hash of the password you chose. Use this hash as
        the value of the <literal>password_hash</literal> attribute.
      </para>
      <para>
        JSON:
      </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "passwd": {
    "users": [
      {
        "name": "root",
        "passwordHash": "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1",
        "sshAuthorizedKeys": [
          "ssh-rsa long...key user@host"
        ]
      }
    ]
  }
}
</screen>
      <para>
        YAML:
      </para>
<screen>
variant: fcos
version: 1.0.0
passwd:
  users:
   - name: root
     password_hash: "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1"
     ssh_authorized_keys:
       - ssh-rsa long...key user@host
 </screen>
      <para>
        The <literal>users</literal> attribute must contain at least one
        <literal>name</literal> attribute.
        <literal>ssh_authorized_keys</literal> is a list of ssh keys for the
        user.
      </para>
    </section>
    <section xml:id="sec-ignition-systemd">
      <title>Enabling <literal>systemd</literal> services</title>
      <para>
        You can enable <systemitem class="daemon">systemd</systemitem> services by specifying them in the
        <literal>systemd</literal> attribute.
      </para>
      <para>
        JSON:
      </para>
<screen>
{
  "ignition": {
    "version": "3.0.0"
  },
  "systemd": {
    "units": [
      {
        "enabled": true,
        "name": "sshd.service"
      }
    ]
  }
}
</screen>
      <para>
        YAML:
      </para>
<screen>
variant: fcos
version: 1.0.0
systemd:
  units:
  - name: sshd.service
    enabled: true
 </screen>
      <para>
        The <literal>name</literal> must be the exact name of a service to be
        enabled (including the suffix).
      </para>
    </section>
  </section>
  <section xml:id="related-ignition-configuration">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Configuring the ALP deployment with Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Converting YAML files to the JSON format is described in
          <xref linkend="task-convert-yaml-to-json"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Configuring the ALP deployment with Combustion is described
          in <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-combustion"><info><title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Combustion</title></info>
  
  <!-- highly inspired by sle/xml/deployment_images_combustion.xml -->
  <section xml:id="what-is-combustion">
    <title>What is Combustion?</title>
    <para>
      Combustion is a dracut module that enables you to configure your system
      on the first boot. You can use Combustion, for example, to change the
      default partitions, set user passwords, create files, or install
      packages.
    </para>
  </section>
  <section xml:id="how-it-works-combustion">
    <title>How does Combustion work?</title>
    <para>
      Combustion is invoked after the <literal>ignition.firstboot</literal>
      argument is passed to the kernel command line. Combustion reads a
      provided file named <filename>script</filename>, executes included
      commands, and thus performs changes to the file system. If
      <filename>script</filename> includes the network flag, Combustion tries
      to configure the network. After <literal>/sysroot</literal> is mounted,
      Combustion tries to activate all mount points in
      <filename>/etc/fstab</filename> and then calls
      <command>transactional-update</command> to apply other changes, for
      example, setting <systemitem class="username">root</systemitem> password or installing packages.
    </para>
    <section xml:id="sec-combustion-configuration">
      <title>The <filename>script</filename> file</title>
      <para>
        When installing on bare metal, the configuration file
        <filename>script</filename> must reside in the
        <filename>combustion</filename> subdirectory on the configuration media
        labeled <literal>combustion</literal>. The directory structure must
        look as follows:
      </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
</screen>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>script</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/org.opensuse.combustion/script" file="/location/to/script"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <tip>
        <title>Using Combustion together with Ignition</title>
        <para>
          Combustion can be used along with Ignition. If you intend to do
          so, label your configuration medium <literal>ignition</literal> and
          include the <filename>ignition</filename> directory with the
          <filename>config.ign</filename> to your directory structure as shown
          below:
        </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
└── ignition
    └── config.ign
</screen>
        <para>
          In this scenario, Ignition runs before Combustion.
        </para>
      </tip>
    </section>
  </section>
  <section xml:id="related-configure-combustion">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Examples of Combustion configuration are detailed in
          <xref linkend="reference-combustion-configuration"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          General information about ALP deployment is included in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For more details about configuring the ALP deployment with
          Ignition, refer to <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Transactional updates are described in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-combustion-configuration"><info><title xmlns:its="http://www.w3.org/2005/11/its">Combustion configuration examples</title></info>
  
  <section xml:id="configuring-combustion-script">
    <title>The <filename>script</filename> configuration file</title>
    <para>
      The <filename>script</filename> configuration file is a set of commands
      that are parsed and executed by Combustion in a <command>transactional-update</command> shell. This
      article provides examples of configuration tasks performed by
      Combustion.
    </para>
    <important>
      <title>Include interpreter declaration</title>
      <para>
        As the <filename>script</filename> file is interpreted by Bash, always
        start the file with the interpreter declaration at its first line:
      </para>
<screen>#!/usr/bin/bash</screen>
    </important>
    <para>
      To log in to your system, include at least the <systemitem class="username">root</systemitem> password.
      However, it is recommended to establish the authentication using SSH
      keys. If you need to use a <systemitem class="username">root</systemitem> password, make sure to configure a
      secure password. If you use a randomly generated password, use at least
      10 characters. If you create your password manually, use even more than
      10 characters and combine uppercase and lowercase letters and numbers.
    </para>
    <section xml:id="sec-script-network">
      <title>Network configuration</title>
      <para>
        To configure and use the network connection during the first boot, add
        the following statement to <filename>script</filename>:
      </para>
<screen># combustion: network</screen>
      <para>
        Using this statement will pass the <literal>rd.neednet=1</literal>
        argument to dracut. If you do not use the statement, the system will be
        configured without any network connection.
      </para>
    </section>
    <section xml:id="combustion-script-partitioning">
      <title>Partitioning</title>
      <para>
        ALP raw images are delivered with a default partitioning scheme
        as described in <xref linkend="alp-deployment-default-partitioning"/>.
        You might want to use a different partitioning. The following set of
        example snippets moves the <filename>/home</filename> to a different
        partition.
      </para>
      <note>
        <title>Performing changes outside of directories included in snapshots</title>
        <para>
          The following script performs changes that are not included in
          snapshots. If the script fails and the snapshot is discarded, some
          changes remain visible and cannot be reverted, for example, the
          changes to the <literal>/dev/vdb</literal> device.
        </para>
      </note>
      <para>
        The following snippet creates a GPT partitioning schema with a single
        partition on the <literal>/dev/vdb</literal> device:
      </para>
<screen>
sfdisk /dev/vdb &lt;&lt;EOF
label: gpt
type=linux
EOF

partition=/dev/vdb1
</screen>
      <para>
        The partition is formatted to BTRFS:
      </para>
<screen>
wipefs --all ${partition}
mkfs.btrfs ${partition}
</screen>
      <para>
        Possible content of <filename>/home</filename> is moved to the new
        <filename>/home</filename> folder location by the following snippet:
      </para>
<screen>
mount /home
mount ${partition} /mnt
rsync -aAXP /home/ /mnt/
umount /home /mnt
</screen>
      <para>
        The snippet below removes an old entry in
        <filename>/etc/fstab</filename> and creates a new entry:
      </para>
<screen>
awk -i inplace '$2 != "/home"' /etc/fstab
echo "$(blkid -o export ${partition} | grep ^UUID=) /home btrfs defaults 0 0" &gt;&gt;/etc/fstab
</screen>
    </section>
    <section xml:id="combustion-script-security">
      <title>Setting a password for <systemitem class="username">root</systemitem></title>
      <para>
        Before you set the <systemitem class="username">root</systemitem> password, generate a hash of the
        password, for example, by using the <command>openssl passwd
        -6</command>. To set the password, add the following to the
        <filename>script</filename>:
      </para>
<screen>echo 'root:$5$.wn2BZHlEJ5R3B1C$TAHEchlU.h2tvfOpOki54NaHpGYKwdNhjaBuSpDotD7' | chpasswd -e</screen>
    </section>
    <section xml:id="combustion-script-sshkeys">
      <title>Adding SSH keys</title>
      <para>
        The following snippet creates a directory to store the <systemitem class="username">root</systemitem>'s SSH
        key and then copies the public SSH key located on the configuration
        device to the <filename>authorized_keys</filename> file.
      </para>
<screen>
mkdir -pm700 /root/.ssh/
cat id_rsa_new.pub &gt;&gt; /root/.ssh/authorized_keys
</screen>
      <note>
        <para>
          The SSH service must be enabled in case you need to use remote login
          via SSH. For details, refer to
          <xref linkend="combustion-script-services"/>.
        </para>
      </note>
    </section>
    <section xml:id="combustion-script-services">
      <title>Enabling services</title>
      <para>
        To enable system services, for example, the SSH service, add the
        following line to <filename>script</filename>:
      </para>
<screen>systemctl enable sshd.service</screen>
    </section>
    <section xml:id="combustion-script-install">
      <title>Installing packages</title>
      <important>
        <title>Network connection and registering your system may be necessary</title>
        <para>
          As some packages may require additional subscription, you may need to
          register your system beforehand. An available network connection may
          also be needed to install additional packages.
        </para>
      </important>
      <para>
        During the first boot configuration, you can install additional
        packages to your system. For example, you can install the
        <literal>vim</literal> editor by adding:
      </para>
<screen>zypper --non-interactive install vim-small</screen>
      <note>
        <para>
          Bear in mind that you will not be able to use
          <command>zypper</command> after the configuration is complete and you
          boot to the configured system. To perform changes later, you must use
          the <command>transactional-update</command> command to create a
          changed snapshot.
        </para>
      </note>
    </section>
  </section>
  <section xml:id="related-combustion-configuration">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          The ALP deployment if generally described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The concept of Combustion is outlined in
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          First boot configuration using Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-post-deployment-considerations"><info><title xmlns:its="http://www.w3.org/2005/11/its">Post-deployment considerations</title></info>
  
  <section xml:id="introduction-post-deployment-considerations">
    <title>Introduction</title>
    <para>
      This article includes important information and tasks that you need to
      consider after you successfully deploy the Adaptable Linux Platform (ALP).
    </para>
  </section>
  <section xml:id="alp-post-deploy-full-disk-encryption">
    <title>Full disk encryption</title>
    <section xml:id="alp-post-deploy-full-disk-encryption-password">
      <title>Change encryption password</title>
      <para>
        During the ALP deployment, you entered a password that is used
        for disk encryption. If you want to change the password, run the
        following command:
      </para>
<screen><prompt role="root"># </prompt>fdectl passwd</screen>
    </section>
    <section xml:id="alp-post-deploy-full-disk-encryption-tpm">
      <title>TPM device</title>
      <para>
        Without a TPM chip, you need to enter the encryption password to
        decrypt the disk on each ALP boot. On systems that have a TPM
        2.0 chip, ALP deployed with D-Installer supports the automatic
        protection of the LUKS volume with a TPM device. The requirement is
        that the machine must use the UEFI Secure Boot enabled.
      </para>
      <para>
        If the D-Installer detects a TPM 2.0 chip and UEFI Secure Boot, it will
        create a secondary LUKS key. On the first boot, ALP will use the
        TPM to protect this key and configure the GRUB 2 boot loader to
        automatically unwrap the key. Be aware that you must remove the ISO
        after the installer has finished and before the system boots for the
        first time. This is because we use the TPM to ensure that the system
        comes up with exactly the same configuration before unlocking the LUKS
        partition.
      </para>
      <para>
        This allows you to use the full disk encryption without having to type
        the disk password on each reboot. However, the disk password is still
        there and can be used for recovery. For example, after updating the
        GRUB 2 boot loader, or the SHIM loader, the TPM will no longer be able
        to unseal the secondary key correctly, and GRUB 2 will have to fall
        back to the password.
      </para>
    </section>
  </section>
  <section xml:id="alp-post-deploy-selinux">
    <title>SELinux</title>
    <para>
      Security-Enhanced Linux (SELinux) is a security framework that increases
      system security by defining access controls for applications, processes
      and files on the file system.
    </para>
    <para>
      ALP ships with SELinux enabled and set to the restrictive
      <emphasis>enforce</emphasis> mode for increased security. The enforce
      mode can lead to processes or workloads not behaving correctly because
      the default policy may be too strict. If you observe such unexpected
      issues, set SELinux to the <emphasis>permissive</emphasis> mode that does
      not enforce SELinux policies but still logs offences against them called
      <emphasis>Access Vector Rules</emphasis> (AVCs).
    </para>
    <para>
      To set SELinux to the permissive mode temporarily, run:
    </para>
<screen><prompt role="root"># </prompt>setenforce 0</screen>
    <tip>
      <para>
        To set SELinux to the permissive mode permanently, edit
        <filename>/etc/selinux/config</filename> and update it to include the
        following line:
      </para>
<screen>SELINUX=permissive</screen>
    </tip>
    <important>
      <para>
        If you entered an SELinux permissive mode, you need to relabel your
        system until it is back in a good state. The reason is that the
        permissive mode allows you to reach states that are not reachable
        otherwise. To relabel the system, run the following command and reboot
        the system:
      </para>
<screen>
<prompt role="root"># </prompt>touch /etc/selinux/.autorelabel
<prompt role="root"># </prompt>reboot
</screen>
    </important>
    <para>
      To monitor AVCs, search the Audit log and <systemitem class="daemon">systemd</systemitem> journal for log
      messages similar to the following one:
    </para>
<screen>
type=AVC msg=audit(1669971354.731:25): avc:  denied  { create } \
for pid=1264 comm="ModemManager" scontext=system_u:system_r:modemmanager_t:s0 \
tcontext=system_u:system_r:modemmanager_t:s0 tclass=qipcrtr_socket permissive=0
</screen>
    <para>
      To filter such messages, you can use the following commands:
    </para>
<screen><prompt role="root"># </prompt>tail -f /var/log/audit/audit.log | grep -i AVC</screen>
    <para>
      and
    </para>
<screen><prompt role="root"># </prompt>journalctl -f | grep -i AVC</screen>
    <para>
      For more advanced search, use the following command:
    </para>
<screen><prompt role="root"># </prompt>ausearch -m avc,user_avc,selinux_err -i</screen>
    <para>
      If such messages appear while using the application that did not behave
      correctly when SELinux was set to the enforce mode, the policies are
      probably too restrictive and need updating. You can help to fine-tune
      SELinux policies by creating a bug report at
      <link xlink:href="https://bugzilla.suse.com/enter_bug.cgi?classification=SUSE%20ALP%20-%20SUSE%20Adaptable%20Linux%20Platform"/>.
      Specify <literal>Basesystem</literal> as a component, include the word
      <literal>SELinux</literal> in the bug subject, and attach the gathered
      unique lines that include AVCs together with reproduction steps.
    </para>
  </section>
  <section xml:id="related-post-deployment-considerations">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Installing software packages and patterns is detailed in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The SELinux framework is detailed in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-selinux.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-transactional-update"><info><title xmlns:its="http://www.w3.org/2005/11/its">Transactional updates</title></info>
  
  <section xml:id="what-is-transactional-update">
    <title>What are transactional updates?</title>
    <para>
      To keep the base operating system stable and consistent, the Adaptable Linux Platform
      (ALP) uses a read-only root file system. Therefore, you cannot
      perform direct changes to the root file system, for example, by
      using the <command>zypper</command> command. Instead, ALP
      introduces <emphasis>transactional updates</emphasis> that allow you to
      apply one or more changes to the root file system.
    </para>
  </section>
  <section xml:id="how-it-works-transactional-update">
    <title>How do transactional updates work?</title>
    <para>
      Each time you call the <command>transactional-update</command> command to change your system—either
      to install a package, perform an update or apply a patch—the
      following actions take place:
    </para>
    <procedure>
      <title>Modifying the root file system</title>
      <step>
        <para>
          A new read-write snapshot is created from your current root file
          system, or from a snapshot that you specified.
        </para>
      </step>
      <step>
        <para>
          All changes are applied (updates, patches or package installation).
        </para>
      </step>
      <step>
        <para>
          The snapshot is switched back to read-only mode.
        </para>
      </step>
      <step>
        <para>
          The new root file system snapshot is prepared, so that it will be
          active after you reboot.
        </para>
      </step>
      <step>
        <para>
          After rebooting, the new root file system is set as the default
          snapshot.
        </para>
      </step>
    </procedure>
    <tip>
      <title>Multiple changes to the root file system without reboots</title>
      <para>
        Normally, each transactional update requires the system reboot before
        the changes are applied. If you want to perform multiple transactional
        updates in one snapshot, use the <option>--continue</option> option.
        This way you will need to reboot the system only once after you perform
        all the required updates.
      </para>
    </tip>
  </section>
  <section xml:id="how-it-works-transactional-update-etc">
    <title><filename>/etc</filename> on a read-only file system</title>
    <para>
      Even though <filename>/etc</filename> is part of the read-only file
      system, using an <literal>OverlayFS</literal> layer on this directory
      enables you to write to this directory. All modifications that you
      performed on the content of <filename>/etc</filename> are written to the
      <filename>/var/lib/overlay/<replaceable>SNAPSHOT_NUMBER</replaceable>/etc</filename>.
      Each snapshot has one associated <literal>OverlayFS</literal> directory.
    </para>
    <para>
      Whenever a new snapshot is created (for example, as a result of a system
      update), the content of <filename>/etc</filename> is synchronized and
      used as a base in the new snapshot. In the <literal>OverlayFS</literal>
      terminology, the current snapshot's <filename>/etc</filename> is mounted
      as <literal>lowerdir</literal>. The new snapshot's
      <filename>/etc</filename> is mounted as <literal>upperdir</literal>. If
      there were no changes in the <literal>upperdir</literal>
      <filename>/etc</filename>, any changes performed to the
      <literal>lowerdir</literal> are visible to the
      <literal>upperdir</literal>. Therefore, the new snapshot also contains
      the changes from the current snapshot's <filename>/etc</filename>.
    </para>
    <important>
      <title>Concurrent modification of <literal>lowerdir</literal> and <literal>upperdir</literal></title>
      <para>
        If <filename>/etc</filename> in both snapshots is modified, only the
        changes in the new snapshot (<literal>upperdir</literal>) persist.
        Changes made to the current snapshot (<literal>lowerdir</literal>) are
        not synchronized to the new snapshot. Therefore, we do not recommend
        changing <filename>/etc</filename> after a new snapshot has been
        created and the system has not been rebooted. However, you can still
        find the changes in the <filename>/var/lib/overlay/</filename>
        directory for the snapshot in which the changes were performed.
      </para>
    </important>
    <note>
      <title>Using the <literal>--continue</literal> option of the <command>transactional-update</command> command</title>
      <para>
        When using the <option>--continue</option> option
        and the new snapshot is a descendant of the current snapshot,
        then the <filename>/etc</filename> overlays of all the snapshots
         in between will be added as additional directories to the
          <filename>lowerdir</filename> (the <filename>lowerdir</filename>
          can have several mount points).
      </para>
    </note>
  </section>
  <section xml:id="how-it-works-transactional-update-repositories">
    <title>Software repositories</title>
    <para>
      The current ALP image points to the following two software
      repositories:
    </para>
    <variablelist>
      <varlistentry>
        <term>ALP</term>
        <listitem>
          <para>
            <literal>https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/repo/ALP-0.1-x86_64-Media1/</literal>
          </para>
          <para>
            This repository is enabled. It is a subset of the build repository
            and an equivalent of the <literal>POOL</literal> repository known
            from other SUSE software products. It will remain unchanged until
            the release of the next ALP prototype.
          </para>
          <tip>
            <para>
              If you need a package which is not included in the
              <literal>ALP</literal> repository, you may find it in the
              <literal>ALP-Build</literal> repository. To enable it, run:
            </para>
<screen><prompt role="root"># </prompt>zypper mr -e ALP-Build</screen>
          </tip>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>ALP-Build</term>
        <listitem>
          <para>
            <literal>https://download.opensuse.org/repositories/SUSE:/ALP/standard/</literal>
          </para>
          <para>
            This repository is disabled by default. It is used for building the
            project. It includes all packages built in the
            <literal>SUSE:ALP</literal> project in the build service and will
            be moving forward over the time with future development.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="benefits-transactional-update">
    <title>Benefits of transactional updates</title>
    <itemizedlist>
      <listitem>
        <para>
          They are atomic—the update is applied only if it completes
          successfully.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes are applied in a separate snapshot and so do not influence
          the running system.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes can easily be rolled back.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-transactional-update">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Usage of the <command>transactional-update</command> command is detailed in
          <xref linkend="reference-transactional-update-usage"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-transactional-update-usage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>transactional-update</command> command</title></info>
  
  <para>
    The <command>transactional-update</command> command enables the atomic installation or removal of updates.
    Updates are applied only if all of them can be successfully installed.
    <command>transactional-update</command> creates a snapshot of your system and uses it to update the system.
    Later you can restore this snapshot. All changes become active only after
    reboot.
  </para>
  <section xml:id="transactional-update-shell">
    <title>The <command>transactional-update</command> shell</title>
    <para>
      The <command>transactional-update shell</command> command opens a shell in the transactional-update
      environment. In the shell, you can enter almost any linux command to make
      changes to the file system, for example, install multiple packages with
      the <command>zypper</command>. Or, you can verify that the changes you
      previously made with the <command>transactional-update</command> command are correct.
    </para>
    <important>
      <para>
        The transactional shell has several limitations. For example, you
        cannot operate <systemitem class="daemon">systemd</systemitem> commands, or modify the
        <filename>/var</filename> partition because it is not mounted.
      </para>
    </important>
    <para>
      All changes that you make to the file system are part of a single
      snapshot. After you finish making changes to the file system and leave
      the shell with the <command>exit</command> command, you need to reboot
      the host to apply the changes.
    </para>
    <tip>
      <para>
        Another approach to making multiple changes to the file system without
        rebooting the host is to use the <option>--continue</option> option.
        For more details, refer to
        <xref linkend="transactional-update-continue"/>.
      </para>
    </tip>
  </section>
  <section xml:id="sec-command-list">
    <title><command>transactional-update</command> usage</title>
    <para>
      The <command>transactional-update</command> command syntax is as follows:
    </para>
<screen>
transactional-update <option>[option]</option> <replaceable>[general_command]</replaceable> <replaceable>[package_command]</replaceable> <replaceable>standalone_command</replaceable>
</screen>
    <note>
      <title>Running <command>transactional-update</command> without arguments</title>
      <para>
        If you do not specify any command or option while running the <command>transactional-update</command>
        command, the system updates itself.
      </para>
    </note>
    <para>
      Possible command parameters are described further.
    </para>
    <variablelist>
      <title><command>transactional-update</command> options</title>
      <varlistentry>
        <term><option>--interactive, -i</option></term>
        <listitem>
          <para>
            Can be used along with a package command to turn on interactive
            mode.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--non-interactive, -n</option></term>
        <listitem>
          <para>
            Can be used along with a package command to turn on non-interactive
            mode.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry xml:id="transactional-update-continue">
        <term><option>--continue [<replaceable>number</replaceable>], -c</option></term>
        <listitem>
          <para>
            The <option>--continue</option> option is for making multiple
            changes to an existing snapshot without rebooting.
          </para>
          <para>
            The default <command>transactional-update</command> behavior is to create a new snapshot from the
            current root file system. If you forget something, such as
            installing a new package, you have to reboot to apply your previous
            changes, run <command>transactional-update</command> again to install the forgotten package, and
            reboot again. You cannot run the <command>transactional-update</command> command multiple times
            without rebooting to add more changes to the snapshot, because this
            will create separate independent snapshots that do not include
            changes from the previous snapshots.
          </para>
          <para>
            Use the <option>--continue</option> option to make as many changes
            as you want without rebooting. A separate snapshot is made each
            time, and each snapshot contains all the changes you made in the
            previous snapshots, plus your new changes. Repeat this process as
            many times as you want, and when the final snapshot includes
            everything you want, reboot the system, and your final snapshot
            becomes the new root file system.
          </para>
          <para>
            Another useful feature of the <option>--continue</option> option is
            that you may select any existing snapshot as the base for your new
            snapshot. The following example demonstrates running <command>transactional-update</command> to
            install a new package in a snapshot based on snapshot 13, and then
            running it again to install another package:
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_1</replaceable></command></screen>
<screen><prompt role="root"># </prompt><command>transactional-update --continue 13 pkg install <replaceable>package_2</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--no-selfupdate</option></term>
        <listitem>
          <para>
            Disables self-updating of <command>transactional-update</command>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--drop-if-no-change, -d</option></term>
        <listitem>
          <para>
            Discards the snapshot created by <command>transactional-update</command> if there were no changes
            to the root file system. If there are some changes to the
            <filename> /etc</filename> directory, those changes merged back to
            the current file system.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--quiet</option></term>
        <listitem>
          <para>
            The <command>transactional-update</command> command will not output to <literal>stdout</literal> .
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--help, -h</option></term>
        <listitem>
          <para>
            Prints help for the <command>transactional-update</command> command.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--version</option></term>
        <listitem>
          <para>
            Displays the version of the <command>transactional-update</command> command.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The general commands are the following:
    </para>
    <variablelist>
      <title>General commands</title>
      <varlistentry>
        <term><command>cleanup-snapshots</command></term>
        <listitem>
          <para>
            The command marks all unused snapshots that are intended to be
            removed.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>cleanup-overlays</command></term>
        <listitem>
          <para>
            The command removes all unused overlay layers of <filename>
            /etc</filename>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>cleanup</command></term>
        <listitem>
          <para>
            The command combines the <command>cleanup-snapshots</command> and
            <command>cleanup-overlays</command> commands.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>grub.cfg</command></term>
        <listitem>
          <para>
            Use this command to rebuild the GRUB boot loader configuration
            file.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>bootloader</command></term>
        <listitem>
          <para>
            The command reinstalls the boot loader.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>initrd</command></term>
        <listitem>
          <para>
            Use the command to rebuild <literal>initrd</literal>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>kdump</command></term>
        <listitem>
          <para>
            In case you perform changes to your hardware or storage, you may
            need to rebuild the kdump initrd.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>shell</command></term>
        <listitem>
          <para>
            Opens a read-write shell in the new snapshot before exiting. The
            command is typically used for debugging purposes.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>reboot</command></term>
        <listitem>
          <para>
            The system reboots after the <command>transactional-update</command> command is complete.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>run <replaceable>&lt;command&gt;</replaceable></command></term>
        <listitem>
          <para>
            Runs the provided command in a new snapshot.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>setup-selinux</command></term>
        <listitem>
          <para>
            Installs and enables targeted SELinux policy.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The package commands are the following:
    </para>
    <variablelist>
      <title>Package commands</title>
      <varlistentry>
        <term><command>dup</command></term>
        <listitem>
          <para>
            Performs upgrade of your system. The default option for this
            command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>migration</command></term>
        <listitem>
          <para>
            The command migrates your system to a selected target. Typically,
            it is used to upgrade your system if it has been registered via
            SUSE Customer Center.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>patch</command></term>
        <listitem>
          <para>
            Checks for available patches and installs them. The default option
            for this command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg install</command></term>
        <listitem>
          <para>
            Installs individual packages from the available channels using the
            <command>zypper install</command> command. This command can also be
            used to install Program Temporary Fix (PTF) RPM files. The default
            option for this command is <option>--interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_name</replaceable></command></screen>
          <para>
            or
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>rpm1 rpm2</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg remove</command></term>
        <listitem>
          <para>
            Removes individual packages from the active snapshot using the
            <command>zypper remove</command> command. This command can also be
            used to remove PTF RPM files. The default option for this command
            is <option> --interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg remove <replaceable>package_name</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg update</command></term>
        <listitem>
          <para>
            Updates individual packages from the active snapshot using the
            <command>zypper update</command> command. Only packages that are
            part of the snapshot of the base file system can be updated. The
            default option for this command is <option>--interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg update <replaceable>package_name</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>register</command></term>
        <listitem>
          <para>
            Registers or deregisters your system. For a complete usage
            description, refer to
            <xref linkend="sec-register-command"/>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>up</command></term>
        <listitem>
          <para>
            Updates installed packages to newer versions. The default option
            for this command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The standalone commands are the following:
    </para>
    <variablelist>
      <title>Standalone commands</title>
      <varlistentry>
        <term><command>rollback <replaceable>&lt;snapshot number&gt;</replaceable></command></term>
        <listitem>
          <para>
            This sets the default subvolume. The current system is set as the
            new default root file system. If you specify a number, that
            snapshot is used as the default root file system. On a read-only
            file system, it does not create any additional snapshots.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update rollback <replaceable>snapshot_number</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>rollback last</command></term>
        <listitem>
          <para>
            This command sets the last known to be working snapshot as the
            default.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>status</command></term>
        <listitem>
          <para>
            This prints a list of available snapshots. The currently booted one
            is marked with an asterisk, the default snapshot is marked with a
            plus sign.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <section xml:id="sec-register-command">
      <title>The <command>register</command> command</title>
      <para>
        The <command>register</command> command enables you to handle all tasks
        regarding registration and subscription management. You can supply the
        following options:
      </para>
      <variablelist>
        <varlistentry>
          <term><option>--list-extensions</option></term>
          <listitem>
            <para>
              With this option, the command will list available extensions for
              your system. You can use the output to find a product identifier
              for product activation.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-p, --product</option></term>
          <listitem>
            <para>
              Use this option to specify a product for activation. The product
              identifier has the following format: <emphasis>
              &lt;name&gt;/&lt;version&gt;/&lt;architecture&gt;</emphasis>, for
              example, <literal>sle-module-live-patching/15.3/x86_64</literal>.
              The appropriate command will then be the following:
            </para>
<screen><prompt role="root"># </prompt>transactional-update register -p sle-module-live-patching/15.3/x86_64</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-r, --regcode</option></term>
          <listitem>
            <para>
              Register your system with the registration code provided. The
              command will register the subscription and enable software
              repositories.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-d, --de-register</option></term>
          <listitem>
            <para>
              The option deregisters the system, or when used along with the
              <literal>-p</literal> option, deregisters an extension.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-e, --email</option></term>
          <listitem>
            <para>
              Specify an email address that will be used in SUSE Customer Center for
              registration.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--url</option></term>
          <listitem>
            <para>
              Specify the URL of your registration server. The URL is stored in
              the configuration and will be used in subsequent command
              invocations. For example:
            </para>
<screen><prompt role="root"># </prompt>transactional-update register --url https://scc.suse.com</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-s, --status</option></term>
          <listitem>
            <para>
              Displays the current registration status in JSON format.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--write-config</option></term>
          <listitem>
            <para>
              Writes the provided options value to the <filename>
              /etc/SUSEConnect</filename> configuration file.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--cleanup</option></term>
          <listitem>
            <para>
              Removes old system credentials.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--version</option></term>
          <listitem>
            <para>
              Prints the version.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--help</option></term>
          <listitem>
            <para>
              Displays the usage of the command.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
  </section>
  <section xml:id="related-transactional-update-usage">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          General description of transactional updates is described in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-containers-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Containers and Podman</title></info>
  
  <!-- highly inspired by https://susedoc.github.io/doc-sle/main/html/SLE-Micro-podman/article-podman.html -->
  <section xml:id="what-is-containers-podman">
    <title>What are containers and Podman?</title>
    <para>
      Containers offer a lightweight virtualization method to run multiple
      virtual environments (containers) simultaneously on a single host. Unlike
      technologies such as Xen or KVM, where the processor simulates a
      complete hardware environment and a hypervisor controls virtual machines,
      containers provide virtualization on the operating system level, where
      the kernel controls the isolated containers.
    </para>
    <para>
      <emphasis>Podman</emphasis> is a short name for Pod Manager Tool. It is
      a daemonless container engine that enables you to run and deploy
      applications using containers and container images. Podman provides a
      command line interface to manage containers.
    </para>
  </section>
  <section xml:id="how-it-works-podman">
    <title>How does Podman work?</title>
    <para>
      Podman provides integration with <systemitem class="daemon">systemd</systemitem>. This way you can control
      containers via <systemitem class="daemon">systemd</systemitem> units. You can create these units for existing
      containers as well as generate units that can start containers if they do
      not exist in the system. Moreover, Podman can run <systemitem class="daemon">systemd</systemitem> inside
      containers.
    </para>
    <para>
      Podman enables you to organize your containers into pods. Pods share
      the same network interface and resources. A typical use case for
      organizing a group of containers into a pod is a container that runs a
      database and a container with a client that accesses the database.
    </para>
    <section xml:id="pod-architecture">
      <title>Pods architecture</title>
      <para>
        A pod is a group of containers that share the same namespace, ports and
        network connection. Usually, containers within one pod can communicate
        directly with each other. Each pod contains an infrastructure container
        (<literal>INFRA</literal>), whose purpose is to hold the namespace.
        <literal>INFRA</literal> also enables Podman to add other containers
        to the pod. Port bindings, cgroup-parent values, and kernel namespaces
        are all assigned to the infrastructure container. Therefore, later
        changes of these values are not possible.
      </para>
      <figure xml:id="fig-pod-architecture">
        <title>Pods architecture</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <textobject role="description"><phrase>Pods architecture</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        Each container in a pod has its own instance of a monitoring program.
        The monitoring program watches the container's process and if the
        container dies, the monitoring program saves its exit code. The program
        also holds open the tty interface for the particular container. The
        monitoring program enables you to run containers in the detached mode
        when Podman exits, because this program continues to run and enables
        you to attach tty later.
      </para>
    </section>
  </section>
  <section xml:id="benefits-containers-podman">
    <title>Benefits of containers</title>
    <itemizedlist mark="bullet" spacing="normal">
      <listitem>
        <para>
          Containers make it possible to isolate applications in self-contained
          units.
        </para>
      </listitem>
      <listitem>
        <para>
          Containers provide near-native performance. Depending on the runtime,
          a container can use the host kernel directly, thus minimizing
          overhead.
        </para>
      </listitem>
      <listitem>
        <para>
          It is possible to control network interfaces and apply resources
          inside containers through kernel control groups.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-containers-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Usage of the <command>podman</command> command is detailed in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          List of available workloads with links to their installation using
          Podman is included in
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-enable-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Enabling Podman</title></info>
  
  <section xml:id="introduction-enable-podman">
    <title>Introduction</title>
    <para>
      This article helps you verify that Podman is installed on the
      ALP system and provides guidelines to enable its <systemitem class="daemon">systemd</systemitem> service
      when Cockpit requires it.
    </para>
  </section>
  <section xml:id="requirements-enable-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="install-podman">
    <title>Installing Podman</title>
    <procedure>
      <step>
        <para>
          Verify that Podman is installed on your system by running the
          following command:
        </para>
<screen><prompt role="root"># </prompt>zypper se -i podman</screen>
      </step>
      <step>
        <para>
          If Podman is not listed in the output, install it by running:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install podman*</screen>
      </step>
      <step>
        <para>
          Reboot the ALP host for the changes to take effect.
        </para>
      </step>
      <step>
        <para>
          Optionally, enable and start the
          <systemitem class="daemon">podman.service</systemitem> service for
          applications that require it, such as Cockpit. You can enable it
          either in Cockpit by clicking <menuchoice><guimenu>Podman
          containers</guimenu><guimenu>Start podman</guimenu></menuchoice>, or
          by running the following command:
        </para>
<screen><prompt role="root"># </prompt>systemctl enable --now podman.service</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="install-podman-rootless">
    <title>Enabling rootless mode</title>
    <para>
      By default, Podman requires <systemitem class="username">root</systemitem> privileges. To enable rootless
      mode for the current user, run the following command:
    </para>
<screen>
<prompt>&gt; </prompt>sudo usermod --add-subuids 100000-165535 \
  --add-subgids 100000-165535 <replaceable>USER</replaceable>
</screen>
    <para>
      Reboot the machine to enable the change. The command above defines a
      range of local UIDs to which the UIDs allocated to users inside the
      container are mapped on the host. Note that the ranges defined for
      different users must not overlap. It is also important that the ranges do
      not reuse the UID of an existing local user or group. By default, adding
      a user with the <command>useradd</command> command automatically
      allocates subUID and subGID ranges.
    </para>
    <note>
      <title>Limitations of rootless containers</title>
      <para>
        Running a container with Podman in rootless mode on SLE Micro may fail,
        because the container might need access to directories or files that
        require <systemitem class="username">root</systemitem> privileges.
      </para>
    </note>
  </section>
  <section xml:id="next-enable-podman">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Run containerized workloads. For details, refer to
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-enable-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Containers and Podman are outlined in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Usage of the <command>podman</command> command is listed in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Available workloads and links to their installation using Podman
          are listed in <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-podman-usage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Podman usage</title></info>
  
    <para>
      This article introduces basic Podman usage that you may need when
      running containerized workloads.
    </para>
  <section xml:id="sec-getting-images">
    <title>Getting container images</title>
    <para>
      To run a container, you need an image. An image includes all dependencies
      needed to run an application. You can obtain images from an image
      registry. Available registries are defined in the
      <filename>/etc/containers/registries.conf</filename> configuration file.
      If you have a local image registry or want to use other registries, add
      the registries into the configuration file.
    </para>
    <important>
      <title>No tools for building images in ALP</title>
      <para>
        ALP does not provide tools for building custom images.
        Therefore, the only way to get an image is to pull it from an image
        registry.
      </para>
    </important>
    <para>
      The <command>podman pull</command> command pulls an image from an image
      registry. The syntax is as follows:
    </para>
<screen><prompt role="root"># </prompt>podman pull <replaceable>[OPTIONS]</replaceable> <replaceable>SOURCE</replaceable></screen>
    <para>
      The <replaceable>source</replaceable> can be an image without the
      registry name. In that case, Podman tries to pull the image from all
      registries configured in the
      <filename>/etc/containers/registries.conf</filename> file. The default
      image tag is <literal>latest</literal>. The default location of pulled
      images is
      <filename>/var/lib/containers/storage/overlay-images/</filename>.
    </para>
    <para>
      To view all possible options of the <command>podman pull</command>
      command, run:
    </para>
<screen><prompt role="root"># </prompt>podman pull --help</screen>
    <note>
      <title>Getting images using Cockpit</title>
      <para>
        If you are using Cockpit, you can also pull images from an image
        registry in the <guimenu>Podman containers</guimenu> menu by clicking
        <guimenu>+ Get new image</guimenu>.
      </para>
    </note>
    <para>
      Podman enables you to search for images in an image registry or a list
      of registries using the command:
    </para>
<screen><prompt role="root"># </prompt>podman search <replaceable>IMAGE_NAME</replaceable></screen>
  </section>
  <section xml:id="sec-working-containers">
    <title>Working with containers</title>
    <para>
      The following section covers common container management tasks. This
      includes creating, starting, and modifying containers.
    </para>
    <warning>
      <para>
        The current version of ALP does not provide tools for building
        custom images. Therefore, the only way to get a container image is to
        pull it from an image registry.
      </para>
    </warning>
    <section xml:id="sec-podman-run">
      <title>Running containers</title>
      <tip>
        <para>
          For specific details on running ALP containers, refer to links
          in the <xref linkend="reference-available-alp-workloads"/> article.
        </para>
      </tip>
      <para>
        After you have pulled your container image, you can create containers
        based on it. You can run an instance of the image using the
        <command>podman run</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman run [<replaceable>OPTIONS</replaceable>] <replaceable>IMAGE</replaceable> [<replaceable>CONTAINER_NAME</replaceable>]</screen>
      <para>
        <replaceable>IMAGE</replaceable> is specified in format
        <emphasis>transport:path</emphasis>. If <emphasis>transport</emphasis>
        is omitted, the default <literal>docker</literal> is used. The
        <emphasis>path</emphasis> can reference to a specific image registry.
        If omitted, Podman searches for the image in registries defined in
        the <filename>/etc/containers/registries.conf</filename> file. An
        example that runs a container called <literal>sles15</literal> based on
        the <literal>sle15</literal> image follows:
      </para>
<screen><prompt role="root"># </prompt>podman run registry.opensuse.org/suse/templates/images/sle-15-sp3/base/images/suse/sle15 sles15</screen>
      <para>
        Below is a list of frequently used options. For a complete list of
        available options, run the command: <command>podman run
        --help</command>.
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--detach, -d</literal></term>
          <listitem>
            <para>
              The container will run in the background.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--env, -e=env</literal></term>
          <listitem>
            <para>
              This option allows arbitrary environment variables that are
              available for the process to be launched inside of the container.
              If an environment variable is specified without a value, Podman
              will check the host environment for a value and set the variable
              only if it is set on the host.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--help</literal></term>
          <listitem>
            <para>
              Prints help for the <command>podman run</command> command.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--hostname=</literal><emphasis>name</emphasis>,<literal> -h</literal></term>
          <listitem>
            <para>
              Sets the container host name that is available inside the
              container.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--pod=</literal><emphasis>name</emphasis></term>
          <listitem>
            <para>
              Runs the container in an existing pod. To create a pod, prefix
              the pod name with <literal>new:</literal>.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--read-only</literal></term>
          <listitem>
            <para>
              Mounts the container’s root file system as read-only.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--systemd=true|false|always</literal></term>
          <listitem>
            <para>
              Runs the container in systemd mode. The default is true.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="sec-podman-stop">
      <title>Stopping containers</title>
      <para>
        If the <command>podman run</command> command finished successfully, a
        new container has been started. You can stop the container by running:
      </para>
<screen><prompt role="root"># </prompt>podman stop <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        You can specify a single container name or ID or a space-separated list
        of containers. The command takes the following options:
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--all, -a</literal></term>
          <listitem>
            <para>
              Stops all running containers.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--latest, -l</literal></term>
          <listitem>
            <para>
              Instead of providing a container name, the last created container
              will be stopped.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--time, -t=</literal><emphasis>seconds</emphasis></term>
          <listitem>
            <para>
              Seconds to wait before forcibly stopping the container.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        To view all possible options of the <command>podman stop</command>
        command, run the following:
      </para>
<screen><prompt role="root"># </prompt>podman stop --help</screen>
    </section>
    <section xml:id="sec-podman-start">
      <title>Starting containers</title>
      <para>
        To start already created but stopped containers, use the
        <command>podman start</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman start <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        For a complete list of possible options of <command>podman
        start</command>, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman start --help</screen>
    </section>
    <section xml:id="podman-update-containers">
      <title>Updating containers</title>
      <para>
        To update an existing container, follow these steps:
      </para>
      <procedure>
        <step>
          <para>
            Identify the image of the container that you want to update, for
            example, <literal>yast-mgmt-qt</literal>:
          </para>
<screen>
<prompt>&gt; </prompt>podman image ls
REPOSITORY                                                                                                  TAG         IMAGE ID      CREATED      SIZE
[...]
registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt  latest      f349194a439d  13 days ago  674 MB
</screen>
        </step>
        <step>
          <para>
            Pull the image from the registry to find out if there is a newer
            version. If you do not specify a version tag, the
            <literal>latest</literal> tag is used:
          </para>
<screen>
<prompt role="root"># </prompt>podman pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
Trying to pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest...
Getting image source signatures
Copying blob 6bfbcdeee2ec done
[...]
Writing manifest to image destination
Storing signatures
f349194a439da249587fbd8baffc5659b390aa14c8db1d597e95be703490ffb1
</screen>
        </step>
        <step>
          <para>
            If the container is running, identify its ID and stop it:
          </para>
<screen>
<prompt role="root"># </prompt>podman ps
CONTAINER ID  IMAGE                                                                             COMMAND     CREATED         STATUS
[...]
28fef404417b /workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest               2 weeks ago     Up 24 seconds ago
<prompt role="root"># </prompt>podman stop 28fef404417b
</screen>
        </step>
        <step>
          <para>
            Run the container following specific instructions at
            <xref linkend="reference-available-alp-workloads"/>, for
            example:
          </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="sec-podman-commit">
      <title>Committing modified containers</title>
      <para>
        You can run a new container with specific attributes that are not part
        of the original image. To save the container with these attributes as a
        new image, you can use the <command>podman commit</command> command:
      </para>
<screen><prompt role="root"># </prompt>podman commit <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> is a container name or a container
        ID. <replaceable>IMAGE</replaceable> is the new image name. If the
        image name does not start with a registry name, the value
        <literal>localhost</literal> is used.
      </para>
      <para>
        When using Cockpit, you can perform the <command>commit</command>
        operation directly from a container's <guimenu>Details</guimenu>, by
        clicking <guimenu>Commit</guimenu>. A dialog box opens. Specify all
        required details as shown below and click <guimenu>Commit</guimenu>:
      </para>
      <figure>
        <title>Committing a container in Cockpit</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
    <section xml:id="sec-podman-ps">
      <title>Listing containers</title>
      <para>
        Podman enables you to list all running containers using the
        <command>podman ps</command> command. The generic syntax of the command
        is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman  ps <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        Command options can change the displayed information. For example,
        using the <literal>--all</literal> option will output all containers
        created by Podman (not only the running containers).
      </para>
      <para>
        For a complete list of <command>podman ps</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman ps --help</screen>
    </section>
    <section xml:id="sec-podman-rm">
      <title>Removing containers</title>
      <para>
        To remove one or more unused containers from the host, use the
        <command>podman rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman rm <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        The command does not remove the specified container if the container is
        running. To remove a running container, use the <literal>-f</literal>
        option.
      </para>
      <para>
        For a complete list of <command>podman rm</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman rm --help</screen>
      <note>
        <title>Deleting all stopped containers</title>
        <para>
          You can delete all stopped containers from your host with a single
          command:
        </para>
<screen><prompt role="root"># </prompt>podman container prune</screen>
        <para>
          Make sure that each stopped container is intended to be removed
          before you run the command, otherwise you might remove containers
          that are still in use and were stopped only temporarily.
        </para>
      </note>
    </section>
  </section>
  <section xml:id="sec-working-pods">
    <title>Working with pods</title>
    <para>
      Containers can be grouped into a pod. The containers in the pod then
      share network, pid, and IPC namespace. Pods can be managed by
      <command>podman pod</command> commands. This section provides an overview
      of the commands for managing pods.
    </para>
    <section xml:id="sec-creating-pods">
      <title>Creating pods</title>
      <para>
        The command <command>podman pod create</command> is used to create a
        pod. The syntax of the command is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod create <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        The command outputs the pod ID. By default, the pods are created
        without being started. You can start a pod by running a container in
        the pod, or by starting the pod as described in
        <xref linkend="sec-starting-pods"/>.
      </para>
      <note>
        <title>Default pod names</title>
        <para>
          If you do not specify a pod name with the <literal>--name</literal>
          option, Podman will assign a default name for the pod.
        </para>
      </note>
      <para>
        For a complete list of possible options, run the following command:
      </para>
<screen><prompt role="root"># </prompt>podman pod create --help</screen>
    </section>
    <section xml:id="sec-listing-pods">
      <title>Listing pods</title>
      <para>
        You can list all pods by running the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod list</screen>
      <para>
        The output looks as follows:
      </para>
<screen>
POD ID        NAME               STATUS   CREATED       # OF CONTAINERS  INFRA ID
30fba506fecb  upbeat_mcclintock  Created  19 hours ago  1                4324f40c9651
976a83b4d88b  nervous_feynman    Running  19 hours ago  2                daa5732ecd02
</screen>
      <para>
        As each pod includes the <literal>INFRA</literal> container, the number
        of containers in a pod is always larger than zero.
      </para>
    </section>
    <section xml:id="sec-starting-pods">
      <title>Starting/stopping/restarting pods</title>
      <para>
        After a pod is created, you must start it, as it is not in the state
        <literal>running</literal> by default. In the commands below,
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To start a pod, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod start <replaceable>[OPTIONS]</replaceable> <replaceable>POD</replaceable></screen>
      <para>
        For a complete list of possible options, run:
      </para>
<screen><prompt role="root"># </prompt>podman pod start --help</screen>
      <para>
        To stop a pod, use the <command>podman pod stop</command> as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod stop <replaceable>POD</replaceable></screen>
      <para>
        To restart a pod, use the <command>podman pod restart</command> command
        as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod restart <replaceable>POD</replaceable></screen>
    </section>
    <section xml:id="sec-adding-pods">
      <title>Managing containers in a pod</title>
      <para>
        To add a new container to a pod, use the <command>podman run</command>
        command with the option <literal>--pod</literal>. A general syntax of
        the command follows:
      </para>
<screen><prompt role="root"># </prompt>podman run <replaceable>[OPTIONS]</replaceable> --pod <replaceable>POD_NAME</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        For details about the <command>podman run</command> command, refer to
        <xref linkend="sec-podman-run"/>.
      </para>
      <note>
        <title>Only new containers can be added to a pod</title>
        <para>
          The <command>podman start</command> command does not allow for
          starting a container in a pod if the container was not added to the
          pod during the container's initial running.
        </para>
      </note>
      <para>
        You cannot remove a container from a pod and keep the container
        running, because the container itself is removed from the host.
      </para>
      <para>
        Other actions like start, restart and stop can be performed on specific
        containers without affecting the status of the pod.
      </para>
    </section>
    <section xml:id="sec-removing-pods">
      <title>Removing pods</title>
      <para>
        There are two ways to remove pods. You can use the <command>podman pod
        rm</command> command to remove one or more pods. Alternatively, you can
        remove all stopped pods using the <command>podman pod prune</command>
        command.
      </para>
      <para>
        To remove a pod or several pods, run the <command>podman pod
        rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod rm <replaceable>POD</replaceable></screen>
      <para>
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To remove all currently stopped pods, use the <command>podman pod
        prune</command> command. Make sure that all stopped pods are intended
        to be removed before you run the <command>podman pod prune</command>
        command, otherwise you might remove pods that are still in use.
      </para>
    </section>
    <section xml:id="sec-monitoring-pods">
      <title>Monitoring processes in pods</title>
      <para>
        To view all containers in all pods, use the following command:
      </para>
<screen><prompt role="root"># </prompt>podman ps -a --pod</screen>
      <para>
        The output of the command will be similar to the following one:
      </para>
      <!-- Decreased text size and removed a few columns to make this fit on the PDF. -->
<screen>
<?dbsuse-fo font-size="0.70em"?>
CONTAINER ID  IMAGE                       COMMAND    CREATED       STATUS                 [...]
4324f40c9651  k8s.gcr.io/pause:3.2                   21 hours ago  Created
daa5732ecd02  k8s.gcr.io/pause:3.2                   22 hours ago  Up 3 hours ago
e5c8e360c54b  localhost/test:latest       /bin/bash  3 days ago    Exited (137) 3 days ago
82dad15828f7  localhost/opensuse/toolbox  /bin/bash  3 days ago    Exited (137) 3 days ago
1a23da456b6f  docker.io/i386/ubuntu       /bin/bash  4 days ago    Exited (0) 6 hours ago
df890193f651  localhost/opensuse/toolbox  /bin/bash  4 days ago    Created
  </screen>
      <para>
        The first two records are the <literal>INFRA</literal> containers of
        each pod, based on the <literal>k8s.gcr.io/pause:3.2</literal> image.
        Other containers in the output are stand-alone containers that do not
        belong to any pod.
      </para>
    </section>
    <section xml:id="related-podman-usage">
      <title>Related topics</title>
      <itemizedlist>
        <listitem>
          <para>
            Enabling Podman is described in
            <xref linkend="task-enable-podman"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            Containers and Podman are outlined in
            <xref linkend="concept-containers-podman"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            Available workloads and links to their installation are listed in
            <xref linkend="reference-available-alp-workloads"/>.
          </para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
</section></chapter><chapter xml:lang="en" role="reference" version="5.1" xml:id="reference-available-alp-workloads"><info><title xmlns:its="http://www.w3.org/2005/11/its">Workloads</title></info>
  
  <section xml:id="introduction-available-alp-workloads">
    <title>Introduction</title>
    <para>
      The Adaptable Linux Platform (ALP) runs containerized workloads instead of
      traditional applications. Images of these containers are stored in image
      registries online. ALP can run any containerized workload that is
      supported by the default container manager Podman. This article lists
      and describes workloads securely distributed and supported by SUSE. You
      can find the source files of the workloads at
      <link xlink:href="https://build.opensuse.org/project/show/SUSE:ALP:Workloads"/>.
    </para>
  </section>
  <section xml:id="alp-workload-yast">
    <title>YaST</title>
    <para>
      The following YaST container images are available:
    </para>
    <variablelist>
      <varlistentry>
        <term>yast-mgmt-ncurses</term>
        <listitem>
          <para>
            The base YaST workload. It contains the text version of YaST
            (ncurses).
          </para>
          <para>
            For more details, refer to
            <xref linkend="task-run-yast-with-podman"/>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-qt</term>
        <listitem>
          <para>
            This workload adds the Qt-based graphical user interface.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-web</term>
        <listitem>
          <para>
            This workload exposes the standard graphical interface via a VNC
            server and uses a JavaScript VNC client to render the screen in a
            Web browser.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="alp-workload-kvm">
    <title>KVM</title>
    <para>
      This workload adds virtualization capability to ALP so that you
      can use it as a VM Host Server. It uses the KVM hypervisor supported by the
      <systemitem class="library">libvirt</systemitem> toolkit.
    </para>
    <para>
      For more details, refer to <xref linkend="task-run-kvm-with-podman"/>.
    </para>
  </section>
  <section xml:id="alp-workload-cockpit">
    <title>Cockpit Web server</title>
    <para>
      This workload adds the Cockpit Web server to ALP so that you can
      administer the system and container via a user-friendly interface in your
      Web browser.
    </para>
    <para>
      For more details, refer to
      <xref linkend="task-run-cockpit-with-podman"/>.
    </para>
  </section>
  <section xml:id="alp-workload-gdm">
   <title>GDM</title>
   <para>
    This workload runs GDM and basic GNOME environment. For more
    details, refer to <xref linkend="task-run-gdm-with-podman"/>.
   </para>
  </section>
  <section xml:id="alp-workload-fiewalld">
    <title><systemitem class="daemon">firewalld</systemitem></title>
    <para>
      This workload adds firewall capability to ALP to define the trust
      level of network connections or interfaces.
    </para>
    <para>
      For more details, refer to
      <xref linkend="task-run-firewalld-with-podman"/>.
    </para>
  </section>
  <section xml:id="alp-workload-grafana">
   <title>Grafana</title>
   <para>
    This workload adds a Web-based dashboard to the ALP host that lets
    you query, monitor, visualize and better understand existing data residing
    on any client host.
   </para>
    <para>
      For more details, refer to
      <xref linkend="task-run-grafana-with-podman"/>.
    </para>
  </section>
  <section xml:id="related-run-workloads-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          <systemitem class="library">libvirt</systemitem> virtualization is described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/part-virt-libvirt.html"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The general concept of Podman is described in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-run-yast-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the YaST workload using Podman</title></info>
  
  <section xml:id="introduction-run-yast-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to start the YaST workload on the Adaptable Linux Platform
      (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-yast-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-yast-text-mode">
    <title>Starting YaST in text mode</title>
    <para>
      To start the text version (ncurses) of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-ncurses
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest
</screen>
        <figure>
          <title>YaST running in text mode on ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>YaST running in text mode on ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-starting-yast-qt">
    <title>Starting graphical YaST</title>
    <para>
      To start the graphical Qt version of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          To view the graphical YaST on your local X server, you need to use
          SSH X forwarding. It requires the <package>xauth</package> package
          installed, applied by the host reboot:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install xauth &amp;&amp; reboot</screen>
      </step>
      <step>
        <para>
          Connect to the ALP host using <command>ssh</command> with the
          X forwarding enabled:
        </para>
<screen><prompt>&gt; </prompt>ssh -X <replaceable>ALP_HOST</replaceable></screen>
      </step>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-qt
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
[...]
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest
</screen>
        <figure>
          <title>Running graphical YaST on top of ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Running graphical YaST on top of ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="related-run-yast-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          YaST is generally described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-yast-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-kvm-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the KVM virtualization workload using Podman</title></info>
  
  <section xml:id="introduction-run-kvm-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run KVM VM Host Server on the Adaptable Linux Platform
      (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-kvm-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          When running ALP in a virtualized environment, you need to
          enable the nested KVM virtualization on the bare-metal host
          operating system and use <literal>kernel-default</literal> kernel
          instead of the default <literal>kernel-default-base</literal> in
          ALP.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-kvm">
    <title>Starting the KVM workload</title>
    <para>
      ALP can serve as a host running virtual machines. The following
      procedure describes steps to prepare the ALP host to run
      containerized KVM VM Host Server and run an example VM Guest on top of it.
    </para>
    <procedure>
      <step>
        <para>
          Identify the KVM workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search kvm
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and install all the wrapper scripts:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm:latest</screen>
      </step>
      <step>
        <para>
          Create the <literal>libvirtd</literal> container from the downloaded
          image:
        </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh create</screen>
      </step>
      <step>
        <para>
          Start the container:
        </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh start</screen>
      </step>
      <step>
        <para>
          Optionally, run a VM Guest on top of the started KVM VM Guest
          using the <command>virt-install.sh</command> script.
        </para>
        <tip>
          <para>
            <command>virt-install.sh</command> uses the
            <filename>openSUSE-Tumbleweed-JeOS.x86_64-OpenStack-Cloud.qcow2</filename>
            image by default. To specify another VM image, modify the
            <option>APPLIANCE_MIRROR</option> and <option>APPLIANCE</option>
            options in the <filename>/etc/kvm-container.conf</filename> file.
          </para>
        </tip>
        <tip>
          <para>
            <command>virsh.sh</command> is a wrapper script to launch the
            <command>virsh</command> command inside the container (the default
            container name is <command>libvirtd</command>).
          </para>
        </tip>
<screen>
<prompt>&gt; </prompt>virt-install.sh
[...]
Starting install...
Password for first root login is: OPjQok1nlfKp5DRZ
Allocating 'Tumbleweed-JeOS_5221fd7860.qcow2'            |    0 B  00:00:00 ...
Creating domain...                                       |    0 B  00:00:00
Running text console command: virsh --connect qemu:///system console Tumbleweed-JeOS_5221fd7860
Connected to domain 'Tumbleweed-JeOS_5221fd7860'
Escape character is ^] (Ctrl + ])

Welcome to openSUSE Tumbleweed 20220919 - Kernel 5.19.8-1-default (hvc0).

eth0: 192.168.10.67 fe80::5054:ff:fe5a:c416

localhost login:
</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="related-run-kvm-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Details about the usage of the
          <command>kvm-container-manage.sh</command> script are described in
          <xref linkend="reference-kvm-container-manage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling KVM nested virtualization is described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-vt-installation.html#sec-vt-installation-nested-vms"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find details about <systemitem class="library">libvirt</systemitem> in
          <link xlink:href="https://susedoc.github.io/doc-sle/main/html/SLES-virtualization/part-virt-libvirt.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-kvm-container-manage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>kvm-container-manage.sh</command> script</title></info>
  
  <para>
    The <command>kvm-container-manage.sh</command> script is used to manage the
    KVM server container on the Adaptable Linux Platform (ALP). This article lists each
    subcommand of the script and describes its purpose.
  </para>
  <variablelist>
    <varlistentry>
      <term><command>kvm-container-manage.sh create</command></term>
      <listitem>
        <para>
          Creates a KVM server container from a previously downloaded
          container image. To download the images, use
          <command>podman</command>, for example:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm:latest</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh start</command></term>
      <listitem>
        <para>
          Starts the KVM server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh virsh list</command></term>
      <listitem>
        <para>
          Lists all running VM Guests. Append the <option>--all</option>
          option to get the list of all—running and
          stopped—VM Guests.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh stop</command></term>
      <listitem>
        <para>
          Stops the running KVM server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh uninstall</command></term>
      <listitem>
        <para>
          Cleans the host environment by uninstalling all files that were
          required to run the KVM server container.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="related-kvm-container-manage">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Basic introduction to Podman is in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is explained in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find details on running the KVM workload in
          <xref linkend="task-run-kvm-with-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-cockpit-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the Cockpit Web server using Podman</title></info>
  
  <section xml:id="introduction-run-cockpit-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run a containerized Cockpit Web server on
      the Adaptable Linux Platform (ALP) using Podman.
    </para>
    <note>
      <para>
        An alternative way of installing and enabling the Cockpit Web server
        is described in
        <link xlink:href="https://en.opensuse.org/openSUSE:ALP/Workgroups/SysMngmnt/Cockpit#Install_the_Web_Server_Via_Packages"/>.
      </para>
    </note>
  </section>
  <section xml:id="requirements-run-cockpit-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed the <package>alp_cockpit</package> pattern.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-cockpit">
    <title>Starting the Cockpit workload</title>
    <para>
      Cockpit is a tool to administer one or more hosts from one place via a
      Web user interface. Its default functionality is extended by plug-ins
      that you can install additionally. You do not need the Cockpit Web user
      interface installed on every ALP host. One instance of the Web
      interface can connect to multiple hosts if they have the
      <package>alp_cockpit</package> pattern installed.
    </para>
    <para>
      ALP has the base part of the Cockpit component installed by
      default. It is included in the <package>alp_cockpit</package> pattern. To
      install and run Cockpit's Web interface, follow these steps:
    </para>
    <procedure>
      <step>
        <para>
          Identify the Cockpit Web server workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search cockpit-ws
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
      </step>
      <step>
        <para>
          Run the Cockpit's containerized Web server:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name cockpit-ws run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
      </step>
      <step>
        <para>
          To run the Cockpit's Web server on each ALP boot, enable its
          service:
        </para>
<screen><prompt role="root"># </prompt>systemctl enable cockpit.service</screen>
      </step>
      <step>
        <para>
          To view the Cockpit Web user interface, point your Web browser to
          the following address and accept the self-signed certificate:
        </para>
<screen>https://<replaceable>HOSTNAME_OR_IP_OF_ALP_HOST:9090</replaceable></screen>
        <figure>
          <title>Cockpit running on ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-cockpit.png" width="100%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-cockpit.png" width="100%"/>
            </imageobject>
            <textobject role="description"><phrase>Cockpit running on ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="next-run-cockpit-with-podman">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Administer the system using Cockpit.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. For their list and description,
          refer to <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-run-cockpit-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Installing software packages and patterns is detailed in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is listed in <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Managing <systemitem class="daemon">systemd</systemitem> services is described in
          <link xlink:href="https://documentation.suse.com/smart/linux/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-cockpit-adding-functionality"><info><title xmlns:its="http://www.w3.org/2005/11/its">Adding more functionality to Cockpit</title></info>
  
  <section xml:id="introduction-cockpit-adding-functionality">
    <title>Introduction</title>
    <para>
      After you deploy Cockpit on the Adaptable Linux Platform (ALP), it already provides
      a default functionality. The following sections describe how to extend it
      by installing additional Cockpit extensions. Note that you need to
      reboot ALP to apply the changes.
    </para>
    <important>
      <para>
        Some packages described in this article are available from the
        <literal>ALP-Build</literal> repository which may be disabled by
        default. To make sure the repository is enabled, run the following
        command:
      </para>
<screen><prompt role="root"># </prompt>zypper mr -e ALP-Build &amp;&amp; refresh</screen>
    </important>
  </section>
  <section xml:id="cockpit-metrics">
    <title>Metrics</title>
    <para>
      To enable the visualization of some current metrics, install the PCP
      extension:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-pcp
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Metrics and history in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-metrics.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-metrics.png" width="100%"/>
        </imageobject>
        <textobject role="description"><phrase>Metrics and history in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="cockpit-software-updates">
    <title>Software updates</title>
    <para>
      To be able to perform transactional software updates from Cockpit,
      install the <package>cockpit-tukit</package> package:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-tukit
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Software updates in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <textobject role="description"><phrase><guimenu>Software Updates</guimenu> in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="cockpit-storage-devices">
    <title>Storage devices</title>
    <para>
      To manage local storage devices and their associated technologies,
      install the <package>cockpit-storaged</package> package:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-storaged
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Storage in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-storage.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-storage.png" width="75%"/>
        </imageobject>
        <textobject role="description"><phrase><guimenu>Storage</guimenu> in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="related-cockpit-adding-functionality">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          The process of running the Cockpit container is described in
          <xref linkend="task-run-cockpit-with-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-gdm-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the GNOME Display Manager workload using Podman</title></info>
  
  <section xml:id="introduction-run-gdm-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to deploy and run the GNOME Display Manager (GDM) on
      the Adaptable Linux Platform (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-gdm-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-gdm">
    <title>Starting the GDM workload</title>
    <procedure>
      <step>
        <para>
          On the ALP host system, install
          <package>accountsservice</package> and
          <package>systemd-experimental</package> packages:
        </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install accountsservice systemd-experimental
<prompt role="root"># </prompt>reboot
</screen>
      </step>
      <step>
        <para>
          Verify that SELinux is configured in the
          <emphasis>permissive</emphasis> mode and enable the
          <emphasis>permissive</emphasis> mode if required:
        </para>
<screen><prompt role="root"># </prompt>setenforce 0</screen>
      </step>
      <step>
        <para>
          Identify the GDM container:
        </para>
<screen>
<prompt>&gt; </prompt>podman search gdm
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm
</screen>
      </step>
      <step>
        <para>
          Download and recreate the GDM container locally:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
      </step>
      <step>
        <para>
          Reload the affected <systemitem class="daemon">systemd</systemitem> services:
        </para>
<screen>
<prompt role="root"># </prompt>systemctl daemon-reload
<prompt role="root"># </prompt>systemctl reload dbus
<prompt role="root"># </prompt>systemctl restart accounts-daemon
</screen>
      </step>
      <step>
        <para>
          Run the GDM container.
        </para>
        <substeps>
          <step>
            <para>
              For a standalone process in a container, run:
            </para>
<screen><prompt role="root"># </prompt>systemctl start gdm.service</screen>
            <para>
              Alternatively, run the command manually:
            </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name gdm run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
          </step>
          <step>
            <para>
              For systems with <systemitem class="daemon">systemd</systemitem> running in a container, run:
            </para>
<screen><prompt role="root"># </prompt>systemctl start gdm-systemd.service</screen>
            <para>
              Alternatively, run the command manually:
            </para>
<screen><prompt role="root"># </prompt>podman container runlabel run-systemd --name gdm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
          </step>
        </substeps>
      </step>
      <step>
       <para>
        The GDM starts. After you log in, a basic GNOME environment
        opens.
       </para>
       <figure>
        <title>GNOME Settings on top of ALP</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="alp-gdm-workload.png" width="100%"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="alp-gdm-workload.png" width="100%"/>
         </imageobject>
         <textobject role="description">
          <phrase>GNOME Settings on top of ALP</phrase>
         </textobject>
        </mediaobject>
       </figure>
      </step>
    </procedure>
    <tip>
      <title>Uninstalling deployed files</title>
      <para>
        If you need to clean the environment from all deployed files, run the
        following command:
      </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/gdm:latest</screen>
    </tip>
  </section>
  <section xml:id="related-run-gdm-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The list of SUSE-provided ALP workloads is in
          <xref linkend="introduction-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-firewalld-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running <systemitem class="daemon">firewalld</systemitem> using Podman</title></info>
  
  <section xml:id="introduction-run-firewalld-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run a containerized <systemitem class="daemon">firewalld</systemitem> on the
      Adaptable Linux Platform (ALP) using Podman.
    </para>
    <important>
      <para>
        The <systemitem class="daemon">firewalld</systemitem> container needs access to the host network and needs to
        run as a privileged container. The container image uses the system dbus
        instance. Therefore, you need to install <package>dbus</package> and
        <package>polkit</package> configuration files first.
      </para>
    </important>
  </section>
  <section xml:id="requirements-run-firewalld-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman
        </para>
      </listitem>
      <listitem>
        <para>
          Installed <package>alp_cockpit</package> pattern
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-running-firewalld">
    <title>Running the <systemitem class="daemon">firewalld</systemitem> workload</title>
    <procedure>
      <step>
        <para>
          Identify the <systemitem class="daemon">firewalld</systemitem> workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search firewalld
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld
</screen>
      </step>
      <step>
        <para>
          Verify that <package>firewalld</package> is not installed in the host
          system. Remove it, if necessary, and reboot the ALP host:
        </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg remove firewalld
reboot
</screen>
      </step>
      <step>
        <para>
          Initialize the environment:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
        <para>
          The command prepares the system and creates the following files on
          the host system:
        </para>
<screen>
/etc/dbus-1/system.d/FirewallD.conf
/etc/polkit-1/actions/org.fedoraproject.FirewallD1.policy <co xml:id="alp-firewalld-polkit-policy"/>
/etc/systemd/system/firewalld.service <co xml:id="alp-firewalld-systemd"/>
/etc/default/container-firewalld
/usr/local/bin/firewall-cmd <co xml:id="alp-firewalld-config"/>
</screen>
        <calloutlist>
          <callout arearefs="alp-firewalld-polkit-policy">
            <para>
              The <package>polkit</package> policy file will only be installed
              if <package>polkit</package> itself is installed. It may be
              necessary to restart the
              <systemitem class="daemon">dbus</systemitem> and
              <systemitem class="daemon">polkit</systemitem> daemon afterwards.
            </para>
          </callout>
          <callout arearefs="alp-firewalld-systemd">
            <para>
              The <systemitem class="daemon">systemd</systemitem> service and the corresponding configuration file
              <filename>/etc/default/container-firewalld</filename> allow to
              start and stop the container using <systemitem class="daemon">systemd</systemitem> if Podman is used
              as a container manager.
            </para>
          </callout>
          <callout arearefs="alp-firewalld-config">
            <para>
              <command>/usr/local/bin/firewall-cmd</command> is a wrapper to
              call the <command>firewall-cmd</command> inside the container.
              Docker and Podman are supported.
            </para>
          </callout>
        </calloutlist>
      </step>
      <step>
        <para>
          Run the container:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
        <para>
          The command will run the container as a privileged container with the
          host network. Additionally, <filename>/etc/firewalld</filename> and
          the <systemitem class="daemon">dbus</systemitem> socket are mounted
          into the container.
        </para>
        <tip>
          <para>
            If your container manager is Podman, you can operate <systemitem class="daemon">firewalld</systemitem>
            by using its <systemitem class="daemon">systemd</systemitem> unit files, for example:
          </para>
<screen><prompt role="root"># </prompt>systemctl start firewalld</screen>
        </tip>
      </step>
      <step>
        <para>
          Optionally, you can remove the <systemitem class="daemon">firewalld</systemitem> workload and clean the
          environment from all related files. Configuration files are left on
          the system.
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel uninstall \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld</screen>
      </step>
    </procedure>
    <section xml:id="alp-manage-firewalld-instance">
      <title>Managing the <systemitem class="daemon">firewalld</systemitem> instance</title>
      <para>
        After the <systemitem class="daemon">firewalld</systemitem> container is started, you can manage its
        instance in two ways. You can manually call its client application via
        the <command>podman exec</command> command, for example:
      </para>
<screen>podman exec firewalld firewall-cmd <replaceable>OPTIONS</replaceable></screen>
      <para>
        Alternatively, you can use a shorter syntax by running the
        <command>firewall-cmd</command> wrapper script.
      </para>
    </section>
    <section xml:id="alp-firewalld-documentation">
      <title><systemitem class="daemon">firewalld</systemitem> manual pages</title>
      <para>
        To read the <systemitem class="daemon">firewalld</systemitem> manual page, run the following command:
      </para>
<screen><prompt>&gt; </prompt>podman run -i --rm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld \
 man firewalld</screen>
      <para>
        To read the <command>firewall-cmd</command> manual page, run the
        following command:
      </para>
<screen><prompt>&gt; </prompt>podman run -i --rm \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_images/suse/alp/workloads/firewalld \
 man firewall-cmd</screen>
    </section>
  </section>
  <section xml:id="related-run-firewalld-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is listed in <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Managing <systemitem class="daemon">systemd</systemitem> services is described in
          <link xlink:href="https://documentation.suse.com/smart/linux/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-grafana-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the Grafana workload using Podman</title></info>
  
  <section xml:id="introduction-run-grafana-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run the Grafana visualization tool on the
      Adaptable Linux Platform (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-grafana-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-grafana">
    <title>Starting the Grafana workload</title>
    <para>
      This section describes how to start the Grafana workload, set up a
      client so that we can test it with real data, and configure the Grafana Web
     application to visualize the client's data.
    </para>
    <procedure>
      <step>
        <para>
          Identify the Grafana workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search grafana
[...]
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/grafana
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and prepare the environment:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/grafana:latest
</screen>
      </step>
      <step>
        <para>
          Create the <literal>grafana</literal> container from the downloaded
          image:
        </para>
<screen><prompt role="root"># </prompt>grafana-container-manage.sh create</screen>
      </step>
      <step>
        <para>
          Start the container with the Grafana server:
        </para>
<screen><prompt role="root"># </prompt>grafana-container-manage.sh start</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-setting-grafana-client">
    <title>Setting up a Grafana client</title>
    <para>
      To test Grafana, you need to set up a client that will provide real
      data to the Grafana server.
    </para>
    <procedure>
      <step>
        <para>
          Log in to the client host and install the
          <package>golang-github-prometheus-node_exporter</package> and
          <package>golang-github-prometheus-prometheus</package> packages:
        </para>
<screen><prompt role="root"># </prompt>zypper in golang-github-prometheus-node_exporter golang-github-prometheus-prometheus</screen>
        <note>
          <para>
            If your Grafana server and client hosts are virtualized by a
            KVM containerized workload, use the <option>--network</option>
            option while creating the POD because the
            <option>--publish</option> option does not work in this scenario.
            To get the IP of the VM Host Server default network, run the following
            command on the VM Host Server:
          </para>
<screen><prompt>&gt; </prompt>virsh net-dhcp-leases default</screen>
        </note>
      </step>
      <step>
        <para>
          Restart the Prometheus services on the client host:
        </para>
<screen>
<prompt role="root"># </prompt>systemctl restart prometheus-node_exporter.service
<prompt role="root"># </prompt>systemctl restart prometheus
</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-setting-grafana-web">
    <title>Configuring the Grafana Web application</title>
    <para>
      To configure a data source for the Grafana Web dashboard, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          Open the Grafana Web page that is running on port 3000 on the
          ALP host where the Grafana workload is running, for example:
        </para>
<screen><prompt>&gt; </prompt>firefox http://<replaceable>ALP_HOST_IP_ADDRESS</replaceable>:3000</screen>
      </step>
      <step>
        <para>
          Log in to Grafana. The default user name and password are both set
          to <literal>admin</literal>. After logging in, enter a new password.
        </para>
      </step>
      <step>
        <para>
          Add the Prometheus data source provided by the client. In the left
          panel, hover your mouse over the gear icon and select <guimenu>Data
          sources</guimenu>.
        </para>
        <figure>
          <title>Grafana data sources</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-data-sources.png" width="25%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-data-sources.png" width="25%"/>
            </imageobject>
            <textobject role="description"><phrase>Grafana data sources</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Click <guimenu>Add data source</guimenu> and select
          <guimenu>Prometheus</guimenu>. Fill the <guimenu>URL</guimenu>
          field with the URL of the client where the Prometheus service runs
          on port 9090, for example:
        </para>
        <figure>
          <title>Prometheus URL configuration in Grafana</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-prometheus-url.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-prometheus-url.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Prometheus URL configuration in Grafana</phrase>
            </textobject>
          </mediaobject>
        </figure>
        <para>
          Confirm with <guimenu>Save &amp; test</guimenu>
        </para>
      </step>
      <step>
        <para>
          Create a dashboard based on Prometheus data. Hover your mouse over
          the plus sign in the left panel and select <guimenu>Import</guimenu>.
        </para>
        <figure>
          <title>Creating a Grafana dashboard</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-pimport-dashboard.png" width="25%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-pimport-dashboard.png" width="25%"/>
            </imageobject>
            <textobject role="description"><phrase>Creating Grafana dashboard</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          Enter <literal>405</literal> as the dashboard ID and confirm with
          <guimenu>Load</guimenu>.
        </para>
      </step>
      <step>
        <para>
          From the <guimenu>Prometheus</guimenu> drop-down list at the bottom,
          select the data source you have already created. Confirm with
          <guimenu>Import</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Grafana shows your newly created dashboard.
        </para>
        <figure>
          <title>New Grafana dashboard</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-grafana-dashboard.png" width="100%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-grafana-dashboard.png" width="100%"/>
            </imageobject>
            <textobject role="description"><phrase>New Grafana dashboard</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="related-run-grafana-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
       <para>
        Find more details about Grafana at its home page <link xlink:href="https://grafana.com/grafana/"/>.
       </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-grafana-container-manage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>grafana-container-manage.sh</command> script</title></info>
  
  <para>
    The <command>grafana-container-manage.sh</command> script is used to manage
    the Grafana container on the Adaptable Linux Platform (ALP). This article lists each
    subcommand of the script and describes their purpose.
  </para>
  <variablelist>
    <varlistentry>
      <term><command>grafana-container-manage.sh create</command></term>
      <listitem>
        <para>
          Pulls the Grafana image and creates the corresponding container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh install</command></term>
      <listitem>
        <para>
          Installs additional files that are required to manage the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh start</command></term>
      <listitem>
        <para>
          Starts the container called <literal>grafana</literal>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh uninstall</command></term>
      <listitem>
        <para>
          Uninstalls all files on the host that were required to manage the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh stop</command></term>
      <listitem>
        <para>
          Stops the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh rm</command></term>
      <listitem>
        <para>
          Deletes the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh rmcache</command></term>
      <listitem>
        <para>
          Removes the container image in cache.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh </command></term>
      <listitem>
        <para>
          Runs the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh bash</command></term>
      <listitem>
        <para>
          Runs the <command>bash</command> shell inside the
          <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>grafana-container-manage.sh logs</command></term>
      <listitem>
        <para>
          Displays log messages of the <literal>grafana</literal> container.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="related-grafana-container-manage">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          The basic introduction to Podman is in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is explained in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find details on running the Grafana workload in
          <xref linkend="task-run-grafana-with-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section></chapter></book>
