<?xml version="1.0"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="orig-DC-assembly-alp.xml" version="5.2" xml:lang="en"><info><title>The Adaptable Linux Platform Guide</title>
  <!-- can we try to use the same date format here as in other docs? how can we use the    
   <date><?dbtimestamp format="B d, Y"?></date>
   format here? -->
  <pubdate>
   <!-- enter the original publishing date -->
  </pubdate>
  <meta name="updated" content="enter ISO date of last update as YYYY-MM-DD"/>
  <meta name="time-to-read" content="enter time to read in minutes"/>
  <meta name="bugtracker"><phrase role="url">https://bugzilla.suse.com/enter_bug.cgi</phrase><phrase role="component">Non-product-specific documentation</phrase><phrase role="product">Smart Docs</phrase><phrase role="assignee">tbazant@suse.com</phrase>
  </meta>
  <meta name="translation"><phrase role="trans">no</phrase><phrase role="language">
    <!-- comma-separated list of languages, for example en-us,de-de,cs-cz --></phrase>
  </meta>
  <meta name="refers-to" content="enter id of parent chapter or article"/>
  <meta name="architecture" content=""/>
  <meta name="Adaptable Linux Platform"><productname version="1">Adaptable Linux Platform</productname>
  </meta>
  <meta name="title">Adaptable Linux Platform</meta>
  <meta name="description">The Adaptable Linux Platform (ALP) is a lightweight operating system. Instead of applications distributed in traditional software packages, it runs containerized and virtualized workloads.</meta>
  <meta name="social-descr"/>
  <meta name="targetgroup" content=""/>
  <meta name="function" content=""/>
  <meta name="task" content=""/>
  <meta name="category" content=""/>
  <!--        <xi:include href="common_copyright_gfdl.xml"/>-->
  <abstract>
   <para>
    This guide introduces the Adaptable Linux Platform (ALP)â€”its deployment, system
    management and installation as well as running of containerized workloads.
   </para>
   <variablelist>
    <varlistentry>
     <term>WHAT?</term>
     <listitem>
      <para>
       ALP is a lightweight operating system. Instead of
       applications distributed in traditional software packages, it runs
       containerized and virtualized workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WHY?</term>
     <listitem>
      <para>
       This guide introduces an overview of what ALP is and how it is
       different from traditional operating systems. It also describes how to
       administer ALP and install and manage individual workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>EFFORT?</term>
     <listitem>
      <para>
       To understand the concepts and perform tasks described in this guide,
       you need to have good knowledge and practice with the Linux
       operating system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>GOAL!</term>
     <listitem>
      <para>
       After having read this guide, you will be able to deploy ALP,
       modify its file system in a transactional way, and install and run
       specific workloads on top of it.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </abstract>
 </info>
 
 
<chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Concepts of the Adaptable Linux Platform</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="what-is-alp">
  <title>What is the Adaptable Linux Platform?</title>
  <para>
   The Adaptable Linux Platform (ALP) is a lightweight operating system. Instead of
   applications distributed in traditional software packages, it runs
   containerized and virtualized workloads.
  </para>
 </section>
 <section xml:id="how-it-works-alp">
  <title>What are core components of the Adaptable Linux Platform?</title>
  <para>
   The Adaptable Linux Platform (ALP) consists of the following components:
  </para>
  <variablelist>
   <varlistentry>
    <term>Base operating system</term>
    <listitem>
     <para>
      The core of ALP which runs all required services. It is an
      immutable operating system with a read-only root file system. The file
      system is modified by transactional updates which utilize the
      snapshotting feature of BTRFS.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Transactional updates</term>
    <listitem>
     <para>
      The <command>transactional-update</command> command performs changes on the file system. You can use it
      to install software, update existing workloads, or apply software
      patches. Because it uses file system snapshots, applied changes can be
      easily rolled back.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Container orchestration</term>
    <listitem>
     <para>
      ALP runs containerized workloads instead of applications packed in
      software packages. The default container orchestrator in ALP is
      Podman which is responsible for managing containers and container images.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Containerized workloads</term>
    <listitem>
     <para>
      Workloads replace traditional applications. A containerized workload
      contains all software dependencies required to run a specific application
      or tool.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cockpit</term>
    <listitem>
     <para>
      A Web-based graphical interface to administer single or multiple
      ALP workloads from one place. It helps you manage, for example,
      user accounts, network settings, or container orchestration.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="benefits-alp">
  <title>Benefits of the Adaptable Linux Platform</title>
  <para>
   The Adaptable Linux Platform offers the following customer benefits:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     High security of running workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     Minimal maintenance with keeping the workloads up to date.
    </para>
   </listitem>
   <listitem>
    <para>
     Stable immutable base operating system that utilizes transactions when
     modifying the file system.
    </para>
   </listitem>
   <listitem>
    <para>
     Ability to roll back modifications on the file system in case the
     transaction result is undesirable.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-alp">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Find more details about ALP deployment in <xref linkend="concept-alp-deployment"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Transactional updates are detailed in <xref linkend="concept-transactional-update"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Podman is introduced in <xref linkend="concept-containers-podman"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Available workloads are described in <xref linkend="reference-available-alp-workloads"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-available-alp-workloads"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Available workloads for the Adaptable Linux Platform</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-available-alp-workloads">
  <title>Introduction</title>
  <para>
   This article lists and describes workloads that are available for the Adaptable Linux Platform
   (ALP).
  </para>
 </section>
 <section xml:id="alp-workload-yast">
  <title>YaST</title>
  <para>
   The following YaST container images are available:
  </para>
  <variablelist>
   <varlistentry>
    <term>yast-mgmt-ncurses</term>
    <listitem>
     <para>
      The base YaST workload. It contains the text version of YaST
      (ncurses).
     </para>
     <para>
      For more details, refer to <xref linkend="task-run-yast-with-podman"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>yast-mgmt-qt</term>
    <listitem>
     <para>
      This workload adds the Qt-based graphical user interface.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>yast-mgmt-web</term>
    <listitem>
     <para>
      This workload exposes the standard graphical interface via a VNC server
      and uses a JavaScript VNC client to render the screen in a Web browser.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="alp-workload-kvm">
  <title>KVM</title>
  <para>
   This workload adds virtualization capability to ALP so that you can
   use it as a VM Host Server. It uses the KVM hypervisor supported by the
   <systemitem class="library">libvirt</systemitem> toolkit.
  </para>
  <para>
   For more details, refer to <xref linkend="task-run-kvm-with-podman"/>.
  </para>
 </section>
 <section xml:id="alp-workload-cockpit">
  <title>Cockpit Web server</title>
  <para>
   This workload adds the Cockpit Web server to Adaptable Linux Platform so that you can
   administer the system and container via a user-friendly interface in your
   Web browser.
  </para>
  <para>
   For more details, refer to <xref linkend="task-run-cockpit-with-podman"/>.
  </para>
 </section>
 <section xml:id="related-run-workloads-with-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     <systemitem class="library">libvirt</systemitem> virtualization is described in
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/part-virt-libvirt.html"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The general concept of Podman is described in
     <xref linkend="concept-containers-podman"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp-deployment"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Deployment of the Adaptable Linux Platform</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="what-is-alp-deployment">
  <title>Introduction</title>
  <para>
   The Adaptable Linux Platform (ALP) is distributed as a pre-built raw disk image. You can
   download the latest image from <link xlink:href="https://download.opensuse.org/repositories/SUSE:/ALP/images/"/>
   and deploy it either with a minimal initial configuration (JeOS
   Firstboot), or use additional toolsâ€”Combustion and
   Ignitionâ€”to specify a detailed system setup.
  </para>
 </section>
 <section xml:id="alp-deployment-firstboot-detection">
  <title>First boot detection</title>
  <para>
   The deployment configuration runs on the first boot only. To distinguish
   between the first and subsequent boots, the flag file
   <filename>/boot/writable/firstboot_happened</filename> is created after the
   first boot finishes. If the file is not present in the file system, the
   attribute <literal>ignition.firstboot</literal> is passed to the kernel
   command line, which triggers the creation of <filename>initramfs</filename>
   (Ignition) or running a specific dracut module (Combustion). After
   completing the first boot, the
   <filename>/boot/writable/firstboot_happened</filename> flag file is created.
  </para>
  <note>
   <title>The flag file is always created</title>
   <para>
    Even though the configuration may not be successful, due to improper or
    missing configuration files, the
    <filename>/boot/writable/firstboot_happened</filename> flag file is
    created.
   </para>
  </note>
  <tip>
   <para>
    You may force the first boot configuration on subsequent boot by passing
    the <literal>ignition.firstboot</literal> attribute to the kernel command
    line or by deleting the
    <filename>/boot/writable/firstboot_happened</filename> flag file.
   </para>
  </tip>
 </section>
 <section xml:id="alp-deployment-default-partitioning">
  <title>Default partitioning</title>
  <para>
   The pre-built images are delivered with a default partitioning scheme. You
   can change it during the first boot by using Ignition or Combustion.
  </para>
  <important>
   <title>BTRFS is mandatory for the root file system</title>
   <para>
    If you intend to perform any changes to the default partitioning scheme,
    the root file system must be BTRFS.
   </para>
  </important>
  <para>
   Each image has the following subvolumes:
  </para>
<screen>
 /home
 /root
 /opt
 /srv
 /usr/local
 /var
 </screen>
  <para>
   The <literal>/etc</literal> directory is mounted as overlayfs, where the
   upper directory is mounted to <filename>/var/lib/overlay/1/etc/</filename>.
  </para>
  <para>
   You can recognize the subvolumes mounted by default by the option
   <literal>x-initrd.mount</literal> in <filename>/etc/fstab</filename>. Other
   subvolumes or partitions must be configured either by Ignition or
   Combustion.
  </para>
 </section>
 <section xml:id="related-alp-deployment">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     For deploying with minimal configuration, refer to
     <xref linkend="task-deploy-alp"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For adjusting the deployment with Ignition, refer to
     <xref linkend="concept-configure-ignition"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For adjusting the deployment with Combustion, refer to
     <xref linkend="concept-configure-combustion"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     List
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Deploying the Adaptable Linux Platform</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-deploy-alp">
  <title>Introduction</title>
  <para>
   This article describes how to deploy the Adaptable Linux Platform (ALP) raw disk image.
  </para>
 </section>
 <section xml:id="requirements-deploy-alp">
  <title>Hardware requirements</title>
  <para>
   The minimum supported hardware requirements for deploying ALP follow:
  </para>
  <variablelist>
   <varlistentry>
    <term>CPU</term>
    <listitem>
     <para>
      AMD64/IntelÂ 64 CPU architecture is supported
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Maximum number of CPUs</term>
    <listitem>
     <para>
      The maximum number of CPUs supported by software design is 8192.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Memory</term>
    <listitem>
     <para>
      ALP requires at least 1Â GB RAM. Bear in mind that this is a
      minimal value for the operation system, the actual memory size depends on
      the workload.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hard disk</term>
    <listitem>
     <para>
      The minimum hard disk space is 12GB, while the recommended value is 20GB
      of hard disk space. Adjust the value according to the workloads of your
      containers.
     </para>
     <important>
      <title>Encrypted image does not expand to the full disk capacity</title>
      <para>
       As of now, the encrypted image does not expand to the full disk capacity
       automatically. As a workaround, the following steps are required:
      </para>
      <procedure>
       <step>
        <para>
         Use the <command>qemu-img</command> command to increase the disk image
         to the desired size.
        </para>
       </step>
       <step>
        <para>
         Setup the virtual machine and boot it. When the JeOS Firstboot wizard
         asks you which method to use for encryption, select
         <guimenu>passphrase</guimenu>.
        </para>
       </step>
       <step>
        <para>
         When the system is ready, use the <command>parted</command> command to
         resize the partition where the LUKS device resides (for example,
         partition number 3) to the desired size.
        </para>
       </step>
       <step>
        <para>
         Run the <command>cryptsetup resize luks</command> command. When asked,
         enter the passphrase to resize the encrypted device.
        </para>
       </step>
       <step>
        <para>
         Run the <command>transactional-update shell</command> command to open
         a read-write shell in the current disk snapshot. Then resize the BTRFS
         file system to the desired size, for example:
        </para>
<screen><prompt role="root"># </prompt>btrfs fi resize max /</screen>
       </step>
       <step>
        <para>
         Leave the shell with <command>exit</command> and reboot the system
         with <command>reboot</command>.
        </para>
       </step>
      </procedure>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="deploy-alp-kvm">
  <title>Deploying ALP on a KVM VM Host Server</title>
  <para>
   This procedure describes steps to deploy ALP as a KVM virtual
   machine using the Virtual Machine Manager.
  </para>
  <procedure>
   <step>
    <para>
     Download ALP virtual machine image on the host where you will run
     ALP. Go to https://download.opensuse.org/repositories/SUSE:/ALP/images/, and download the latest disk image of
     ALP. For example:
    </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP/images/ALP-VM.x86_64-0.0.1-kvm-Build9.1.qcow2</screen>
   </step>
   <step>
    <para>
     Start Virtual Machine Manager, select <menuchoice><guimenu>File</guimenu><guimenu>New
     VM</guimenu></menuchoice> and <guimenu>Import existing disk
     image</guimenu>. Confirm with <guimenu>Forward</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Specify the path to the ALP disk image that you previously
     downloaded and the type of linux OS you are deploying, for example,
     <literal>Generic Linux 2020</literal>. Confirm with
     <guimenu>Forward</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Specify the amount of memory and number of processors that you want to
     assign to the ALP virtual machine and confirm with
     <guimenu>Forward</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Specify the name for the virtual machine and the network to be used.
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you want to deploy ALP with only minimal setup options,
       confirm with <guimenu>Finish</guimenu>. The ALP disk image will
       be booted and JeOS Firstboot will take care of the deployment. Refer to
       <xref linkend="deploy-alp-jeos-firstboot"/> for next steps.
      </para>
     </step>
     <step>
      <para>
       If you want to specify detailed deployment options, you need to use
       Ignition or Combustion tools to supply your setup during the disk
       image boot process. For more details, refer to
       <xref linkend="concept-configure-ignition"/> and
       <xref linkend="concept-configure-combustion"/>.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
 </section>
 <section xml:id="deploy-alp-jeos-firstboot">
  <title>Deploying ALP with JeOS Firstboot</title>
  <para>
   When booting the ALP raw image for the first time,
   <emphasis>JeOS Firstboot</emphasis> enables you to perform a minimal
   configuration of your system:
  </para>
  <procedure>
   <step>
    <para>
     After booting the ALP disk image, you will be presented with a
     bootloader screen. Select <guimenu>ALP</guimenu> and confirm with
     <keycap function="enter"/>.
    </para>
    <figure>
     <title>ALP boot screen</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp-deploy-1.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp-deploy-1.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase>ALP boot screen</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     <guimenu>JeOS Firstboot</guimenu> displays a welcome screen. Confirm with
     <keycap function="enter"/>.
    </para>
    <figure>
     <title>JeOS Firstboot screen</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase>JeOS Firstboot screen</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     On the next screens, select keyboard, confirm the license agreement, and
     select the time zone.
    </para>
   </step>
   <step>
    <para>
     In the <guimenu>Enter root password</guimenu> dialog window, enter a
     password for the <systemitem class="username">root</systemitem> and confirm it.
    </para>
    <figure>
     <title>Enter root password</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase>Enter root password</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     ALP is successfully deployed using a minimal initial configuration.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="summary-deploy-alp">
  <title>Summary</title>
  <para>
   After the deployment of ALP is finished, you are presented with the
   login prompt. Log in as <systemitem class="username">root</systemitem>, and you are ready to set up the system
   and install additional workloads.
  </para>
 </section>
 <section xml:id="next-deploy-alp">
  <title>Next steps</title>
  <itemizedlist>
   <listitem>
    <para>
     Install additional software with <command>transactional-update</command>. Refer to
     <xref linkend="concept-transactional-update"/> for more details.
    </para>
   </listitem>
   <listitem>
    <para>
     Install and run additional workloads. Refer to
     <xref linkend="reference-available-alp-workloads"/> for more details.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-deploy-alp">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     General description of ALP is included in
     <xref linkend="concept-alp"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Advanced configuration of ALP deployment is described in
     <xref linkend="concept-configure-ignition"/> and
     <xref linkend="concept-configure-combustion"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find detailed information about using the Virtual Machine Manager in
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
     and
     <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-ignition"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Ignition</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <!-- highly inspired by sle/xml/deployment_images_ignition.xml -->
 <section xml:id="what-is-ignition">
  <title>What is Ignition?</title>
  <para>
   Ignition is a provisioning tool that enables you to configure a system
   according to your specification on the first boot.
  </para>
 </section>
 <section xml:id="how-it-works-ignition">
  <title>How does Ignition work?</title>
  <para>
   When the system is booted for the first time, Ignition is loaded as part
   of an <filename>initramfs</filename> and searches for a configuration file
   within a specific directory (on a USB flash disk, or you can provide a URL).
   All changes are performed before the kernel switches from the temporal file
   system to the real root file system (before the
   <literal>switch_root</literal> command is issued).
  </para>
  <para>
   Ignition uses a configuration file in the JSON format named
   <filename>config.ign</filename>. For the purpose of better human
   readability, you can create a YAML file and convert this file to JSON. For
   details, refer to 'Task: Converting YAML file into JSON'.
  </para>
  <section xml:id="sec-ignition-configuration">
   <title><filename>config.ign</filename></title>
   <para>
    When installing on bare metal, the configuration file
    <filename>config.ign</filename> must reside in the
    <filename>ignition</filename> subdirectory on the configuration media
    labeled <literal>ignition</literal>. The directory structure must look as
    follows:
   </para>
<screen>
&lt;root directory&gt;
â””â”€â”€ ignition
    â””â”€â”€ config.ign

 </screen>
   <para>
    If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
    provide the path to the <filename>config.ign</filename> file in its XML
    definition, for example:
   </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/com.coreos/config" file="/location/to/config.ign"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
   <para>
    The <filename>config.ign</filename> contains various data types: objects,
    strings, integers, booleans and lists of objects. For a complete
    specification, refer to
    <link xlink:href="https://coreos.github.io/ignition/configuration-v3_3/">Ignition
    specification v3.3.0</link>.
   </para>
   <para>
    The <literal>version</literal> attribute is mandatory and in case of
    ALP, its value must be set either to <literal>3.3.0</literal> or to
    any lower version. Otherwise, Ignition will fail.
   </para>
   <para>
    If you want to log in to your system as <systemitem class="username">root</systemitem>, you must at least include a
    password for <systemitem class="username">root</systemitem>. However, it is recommended to establish access via
    SSH keys. To configure a password, make sure to use a secure
    one. If you use a randomly generated password, use at least 10 characters.
    If you create your password manually, use even more than 10 characters and
    combine uppercase and lowercase letters and numbers.
   </para>
  </section>
 </section>
 <section xml:id="related-configure-ignition">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Converting YAML to JSON.
    </para>
   </listitem>
   <listitem>
    <para>
     Examples of Ignition configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Configuring with Combustion.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-convert-yaml-to-json"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Converting YAML formatted files into JSON</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-convert-yaml-to-json">
  <title>Introduction</title>
  <para>
   JSON is a universal file format for storing structured data. Applications,
   for example, Ignition, use it to store and retrieve their configuration.
   Because JSON's syntax is complex and hard to read for human beings, you can
   write the configuration in a human-friendly format called YAML and then
   convert it into JSON.
  </para>
 </section>
 <section xml:id="sec-converting-config">
  <title>Converting YAML files into JSON format</title>
  <para>
   The tool that converts YAML files into JSON format is
   <literal>butane</literal>. It also verifies the syntax of
   the YAML file to catch potential errors in the structure. 
   For the latest version of <literal>butane</literal>, add
   the following repository:
  </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command>  zypper ar -f \
  https://download.opensuse.org/repositories/devel:/kubic:/ignition/<replaceable>DISTRIBUTION</replaceable>/ \
  devel_kubic_ignition
</screen>
  <para>
   Replace <replaceable>DISTRIBUTION</replaceable> with one of the following
   (depending on your distribution):
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <literal>openSUSE_Tumbleweed</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>openSUSE_Leap_$release_number</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>15.3</literal>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Now you can install the <literal>butane</literal> tool:
  </para>
<screen><prompt>&gt; </prompt><command>sudo</command>  zypper ref &amp;&amp; zypper in butane</screen>
  <para>
   After the installation is complete, you can invoke <literal>butane</literal>
   by running:
  </para>
<screen><prompt>&gt; </prompt> butane -p -o config.ign config.fcc</screen>
  <itemizedlist>
   <listitem>
    <para>
     <filename>config.fcc</filename> is the path to the YAML configuration
     file.
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>config.ign</filename> is the path to the output JSON
     configuration file.
    </para>
   </listitem>
   <listitem>
    <para>
     The <option>-p</option> command option adds line breaks to the output file
     and thus makes it more readable.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="summary-convert-yaml-to-json">
  <title>Summary</title>
  <para>
   After you completed the described steps, you can write and store
   configuration files in YAML format while providing them in JSON format if
   applications, for example, Ignition, require it.
  </para>
 </section>
 <section xml:id="related-convert-yaml-to-json">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Configuration using Ignition is described in
     <xref linkend="concept-configure-ignition"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Examples of the Ignition configuration are included in
     <xref linkend="reference-ignition-configuration"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-ignition-configuration"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Ignition configuration examples</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="sec-ignition-examples">
  <title>Configuration examples in YAML</title>
  <para>
   This section will provide you with some common examples of the Ignition
   configuration in the YAML format. Note that Ignition does not accept
   configuration in the YAML format but rather JSON. To convert a YAML file
   to the JSON format, use the <literal>butane</literal> tool as described in
   <xref linkend="introduction-convert-yaml-to-json"/>.
  </para>
  <note>
   <title>The <literal>version</literal> attribute is mandatory</title>
   <para>
    Each <filename>config.fcc</filename> must include version 1.4.0 or lower
    that is then converted to the corresponding Ignition specification.
   </para>
  </note>
  <section xml:id="sec-ignition-storage">
   <title>Storage configuration</title>
   <para>
    The <literal>storage</literal> attribute is used to configure partitions,
    RAID, define file systems, create files, etc. To define partitions, use the
    <literal>disks</literal> attribute. The <literal>filesystem</literal>
    attribute is used to format partitions and define mount points of
    particular partitions. The <literal>files</literal> attribute can be used
    to create files in the file system. Each of the mentioned attributes is
    described in the following sections.
   </para>
   <section xml:id="sec-storage-disks">
    <title>The <literal>disks</literal> attribute</title>
    <para>
     The <literal>disks</literal> attribute is a list of devices that enables
     you to define partitions on these devices. The <literal>disks</literal>
     attribute must contain at least one <literal>device</literal>, other
     attributes are optional. The following example will use a single virtual
     device and divide the disk into four partitions:
    </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  disks:
    - device:	"/dev/vda"
      wipeTable: true
      partitions:
       - label: root
         number: 1
         typeGuid: 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709
       - label: boot
         number: 2
         typeGuid: BC13C2FF-59E6-4262-A352-B275FD6F7172
        - label: swap
         number: 3
         typeGuid: 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
       - label: home
         number: 4
         typeGuid: 933AC7E1-2EB4-4F13-B844-0E14E2AEF915
 </screen>
   </section>
   <section xml:id="sec-storage-raid">
    <title>The <literal>raid</literal> attribute</title>
    <para>
     The <literal>raid</literal> is a list of RAID arrays. The following
     attributes of <literal>raid</literal> are mandatory:
    </para>
    <variablelist>
     <varlistentry>
      <term>level</term>
      <listitem>
       <para>
        a level of the particular RAID array (linear, raid0, raid1, raid2,
        raid3, raid4, raid5, raid6)
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>devices</term>
      <listitem>
       <para>
        a list of devices in the array referenced by their absolute paths
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        a name that will be used for the md device
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
<screen>
variant: fcos
version: 1.0.0
storage:
	- raid: data
	  name: system
	  level: raid1
	  devices: "/dev/sda", "/dev/sdb"
 </screen>
   </section>
   <section xml:id="sec-storage-filesystem">
    <title>The <literal>filesystem</literal> attribute</title>
    <para>
     <literal>filesystem</literal> must contain the following attributes:
    </para>
    <variablelist>
     <varlistentry>
      <term>device</term>
      <listitem>
       <para>
        the absolute path to the device, typically <literal>/dev/sda</literal>
        in case of physical disk
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>format</term>
      <listitem>
       <para>
        the file system format (btrfs, ext4, xfs, vfat or swap)
       </para>
       <note>
        <para>
         In case of ALP, the <literal>root</literal> file system must be
         formatted to btrfs.
        </para>
       </note>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The following example demonstrates using the <literal>filesystem</literal>
     attribute. The <filename>/opt</filename> directory will be mounted to the
     <literal>/dev/sda1</literal> partition, which is formatted to btrfs. The
     partition table will not be erased.
    </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  filesystems:
    - path: /opt
      device: "/dev/sda1"
      format: btrfs
      wipe_filesystem: false
 </screen>
   </section>
   <section xml:id="sec-storage-files">
    <title>The <literal>files</literal> attribute</title>
    <para>
     You can use the <literal>files</literal> attribute to create any files on
     your machine. Bear in mind that if you want to create files outside the
     default partitioning schema, you need to define the directories by using
     the <literal>filesystem</literal> attribute.
    </para>
    <para>
     In the following example, a host name is created by using the
     <literal>files</literal> attribute. The file
     <filename>/etc/hostname</filename> will be created with the
     <emphasis>alp-1</emphasis> host name:
    </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      overwrite: true
      contents:
        inline: "alp-1"
 </screen>
   </section>
   <section xml:id="sec-storage-directories">
    <title>The <literal>directories</literal> attribute</title>
    <para>
     The <literal>directories</literal> attribute is a list of directories that
     will be created in the file system. The <literal>directories</literal>
     attribute must contain at least one <literal>path</literal> attribute.
    </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  directories:
  	- path: /home/tux/
  	  user:
  	   - name: tux
 </screen>
   </section>
  </section>
  <section xml:id="sec-ignition-users">
   <title>Users administration</title>
   <para>
    The <literal>passwd</literal> attribute is used to add users. If you intend
    to log in to your system, create <systemitem class="username">root</systemitem> and set the <systemitem class="username">root</systemitem>'s
    password and/or add the SSH key to the Ignition configuration. You need
    to hash the <systemitem class="username">root</systemitem> password, for example by using the
    <command>openssl</command> command:
   </para>
<screen>
 openssl passwd -6
 </screen>
   <para>
    The command creates a hash of the password you chose. Use this hash as the
    value of the <literal>password_hash</literal> attribute.
   </para>
<screen>
variant: fcos
version: 1.0.0
passwd:
  users:
   - name: root
     password_hash: "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1"
     ssh_authorized_keys:
       - ssh-rsa long...key user@host
 </screen>
   <para>
    The <literal>users</literal> attribute must contain at least one
    <literal>name</literal> attribute. <literal>ssh_authorized_keys</literal>
    is a list of ssh keys for the user.
   </para>
  </section>
  <section xml:id="sec-ignition-systemd">
   <title>Enabling <literal>systemd</literal> services</title>
   <para>
    You can enable <systemitem class="daemon">systemd</systemitem> services by specifying them in the
    <literal>systemd</literal> attribute.
   </para>
<screen>
variant: fcos
version: 1.0.0
systemd:
  units:
  - name: sshd.service
    enabled: true
 </screen>
   <para>
    The <literal>name</literal> must be the exact name of a service to be
    enabled (including the suffix).
   </para>
  </section>
 </section>
 <section xml:id="related-ignition-configuration">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Configuring the ALP deployment with Ignition is described in
     <xref linkend="concept-configure-ignition"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Converting YAML files to the JSON format is described in
     <xref linkend="task-convert-yaml-to-json"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Configuring the ALP deployment with Combustion is described in
     <xref linkend="concept-configure-combustion"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section></section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-combustion"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Combustion</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <!-- highly inspired by sle/xml/deployment_images_combustion.xml -->
 <section xml:id="what-is-combustion">
  <title>What is Combustion?</title>
  <para>
   Combustion is a dracut module that enables you to configure your system on
   the first boot. You can use Combustion, for example, to change the default
   partitions, set user passwords, create files, or install packages.
  </para>
 </section>
 <section xml:id="how-it-works-combustion">
  <title>How does Combustion work?</title>
  <para>
   Combustion is invoked after the <literal>ignition.firstboot</literal>
   argument is passed to the kernel command line. Combustion reads a provided
   file named <filename>script</filename>, executes included commands, and thus
   performs changes to the file system. If <filename>script</filename> includes
   the network flag, Combustion tries to configure the network. After
   <literal>/sysroot</literal> is mounted, Combustion tries to activate all
   mount points in <filename>/etc/fstab</filename> and then calls
   <command>transactional-update</command> to apply other changes, for example,
   setting <systemitem class="username">root</systemitem> password or installing packages.
  </para>
  <section xml:id="sec-combustion-configuration">
   <title>The <filename>script</filename> file</title>
   <para>
    When installing on bare metal, the configuration file
    <filename>script</filename> must reside in the
    <filename>combustion</filename> subdirectory on the configuration media
    labeled <literal>combustion</literal>. The directory structure must look as
    follows:
   </para>
<screen>
&lt;root directory&gt;
â””â”€â”€ combustion
    â””â”€â”€ script
    â””â”€â”€ other files
</screen>
   <para>
    If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
    provide the path to the <filename>script</filename> file in its XML
    definition, for example:
   </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/org.opensuse.combustion/script" file="/location/to/script"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
   <tip>
    <title>Using Combustion together with Ignition</title>
    <para>
     Combustion can be used along with Ignition. If you intend to do so,
     label your configuration medium <literal>ignition</literal> and include
     the <filename>ignition</filename> directory with the
     <filename>config.ign</filename> to your directory structure as shown
     below:
    </para>
<screen>
&lt;root directory&gt;
â””â”€â”€ combustion
    â””â”€â”€ script
    â””â”€â”€ other files
â””â”€â”€ ignition
    â””â”€â”€ config.ign
</screen>
    <para>
     In this scenario, Ignition runs before Combustion.
    </para>
   </tip>
  </section>
 </section>
 <section xml:id="related-configure-combustion">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Examples of Combustion configuration are detailed in <xref linkend="reference-combustion-configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     General information about ALP deployment is included in <xref linkend="concept-alp-deployment"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For more details about configuring the ALP deployment with
     Ignition, refer to <xref linkend="concept-configure-ignition"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Transactional updates are described in
     <xref linkend="concept-transactional-update"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-combustion-configuration"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Combustion configuration examples</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="configuring-combustion-script">
  <title>The <filename>script</filename> configuration file</title>
  <para>
   The <filename>script</filename> configuration file is a set of commands that
   are parsed and executed by Combustion in a <command>transactional-update</command> shell. This article
   provides examples of configuration tasks performed by Combustion.
  </para>
  <important>
   <title>Include interpreter declaration</title>
   <para>
    As the <filename>script</filename> file is interpreted by Bash, always
    start the file with the interpreter declaration at its first line:
   </para>
   <screen>#!/usr/bin/bash</screen>
  </important>
  <para>
  To log in to your system, include at least the <systemitem class="username">root</systemitem>
   password. However, it is recommended to establish the authentication using
   SSH keys. If you need to use a <systemitem class="username">root</systemitem> password, make sure to configure a
   secure password. If you use a randomly generated password, use at least 10
   characters. If you create your password manually, use even more than 10
   characters and combine uppercase and lowercase letters and numbers.
  </para>
  <section xml:id="sec-script-network">
   <title>Network configuration</title>
   <para>
    To configure and use the network connection during the first boot, add the
    following statement to <filename>script</filename>:
   </para>
<screen># combustion: network</screen>
   <para>
    Using this statement will pass the <literal>rd.neednet=1</literal> argument
    to dracut. If you do not use the statement, the system will be configured
    without any network connection.
   </para>
  </section>
  <section xml:id="combustion-script-partitioning">
   <title>Partitioning</title>
   <para>
    ALP raw images are delivered with a default partitioning scheme as
    described in <xref linkend="alp-deployment-default-partitioning"/>. You
    might want to use a different partitioning. The following set of example
    snippets moves the <filename>/home</filename> to a different partition.
   </para>
   <note>
    <title>Performing changes outside of directories included in snapshots</title>
    <para>
     The following script performs changes that are not included in snapshots.
     If the script fails and the snapshot is discarded, some changes remain
     visible and cannot be reverted, for example, the changes to the
     <literal>/dev/vdb</literal> device.
    </para>
   </note>
   <para>
    The following snippet creates a GPT partitioning schema with a single
    partition on the <literal>/dev/vdb</literal> device:
   </para>
<screen>
sfdisk /dev/vdb &lt;&lt;EOF
label: gpt
type=linux
EOF

partition=/dev/vdb1
</screen>
   <para>
    The partition is formatted to BTRFS:
   </para>
<screen>
wipefs --all ${partition}
mkfs.btrfs ${partition}
</screen>
   <para>
    Possible content of <filename>/home</filename> is moved to the new
    <filename>/home</filename> folder location by the following snippet:
   </para>
<screen>
mount /home
mount ${partition} /mnt
rsync -aAXP /home/ /mnt/
umount /home /mnt
</screen>
   <para>
    The snippet below removes an old entry in <filename>/etc/fstab</filename>
    and creates a new entry:
   </para>
<screen>
awk -i inplace '$2 != "/home"' /etc/fstab
echo "$(blkid -o export ${partition} | grep ^UUID=) /home btrfs defaults 0 0" &gt;&gt;/etc/fstab
</screen>
  </section>
  <section xml:id="combustion-script-security">
   <title>Setting a password for <systemitem class="username">root</systemitem></title>
   <para>
    Before you set the <systemitem class="username">root</systemitem> password, generate a hash of the password,
    for example, by using the <command>openssl passwd -6</command>. To set the
    password, add the following to the <filename>script</filename>:
   </para>
<screen>echo 'root:$5$.wn2BZHlEJ5R3B1C$TAHEchlU.h2tvfOpOki54NaHpGYKwdNhjaBuSpDotD7' | chpasswd -e</screen>
  </section>
  <section xml:id="combustion-script-sshkeys">
   <title>Adding SSH keys</title>
   <para>
    The following snippet creates a directory to store the <systemitem class="username">root</systemitem>'s SSH key
    and then copies the public SSH key located on the configuration device to
    the <filename>authorized_keys</filename> file.
   </para>
<screen>
mkdir -pm700 /root/.ssh/
cat id_rsa_new.pub &gt;&gt; /root/.ssh/authorized_keys
</screen>
   <note>
    <para>
     The SSH service must be enabled in case you need to use remote login via
     SSH. For details, refer to <xref linkend="combustion-script-services"/>.
    </para>
   </note>
  </section>
  <section xml:id="combustion-script-services">
   <title>Enabling services</title>
   <para>
    To enable system services, for example, the SSH service, add the following
    line to <filename>script</filename>:
   </para>
<screen>systemctl enable sshd.service</screen>
  </section>
  <section xml:id="combustion-script-install">
   <title>Installing packages</title>
   <important>
    <title>Network connection and registering your system may be necessary</title>
    <para>
     As some packages may require additional subscription, you may need to
     register your system beforehand. An available network connection may also
     be needed to install additional packages.
    </para>
   </important>
   <para>
    During the first boot configuration, you can install additional packages to
    your system. For example, you can install the <literal>vim</literal> editor
    by adding:
   </para>
<screen>zypper --non-interactive install vim-small</screen>
   <note>
    <para>
     Bear in mind that you will not be able to use <command>zypper</command>
     after the configuration is complete and you boot to the configured system.
     To perform changes later, you must use the
     <command>transactional-update</command> command to create a changed
     snapshot.
    </para>
   </note>
  </section>
 </section>
 <section xml:id="related-combustion-configuration">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     The ALP deployment if generally described in <xref linkend="concept-alp-deployment"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The concept of Combustion is outlined in <xref linkend="concept-configure-combustion"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     First boot configuration using Ignition is described in <xref linkend="concept-configure-ignition"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section></section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-transactional-update"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Basic concept of transactional updates</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="what-is-transactional-update">
  <title>What are transactional updates?</title>
  <para>
   The Adaptable Linux Platform (ALP) was designed to use a read-only root file system.
   This means that after the deployment is complete, you are not able to
   perform direct modifications to the root file system, for example, by using
   the <command>zypper</command> command. Instead, ALP introduces the
   concept of transactional updates which enables you to modify your system and
   keep it up to date.
  </para>
 </section>
 <section xml:id="how-it-works-transactional-update">
  <title>How do transactional updates work?</title>
  <para>
   Each time you call the <command>transactional-update</command> command to change your systemâ€”either to
   install a package, perform an update or apply a patchâ€”the following
   actions take place:
  </para>
  <procedure>
   <title>Modifying the root file system</title>
   <step>
    <para>
     A new read-write snapshot is created from your current root file system,
     or from a snapshot that you specified.
    </para>
   </step>
   <step>
    <para>
     All changes are applied (updates, patches or package installation).
    </para>
   </step>
   <step>
    <para>
     The snapshot is switched back to read-only mode.
    </para>
   </step>
   <step>
    <para>
     The new root file system snapshot is prepared, so that it will be active
     after you reboot.
    </para>
   </step>
   <step>
    <para>
     After rebooting, the new root file system is set as the default snapshot.
    </para>
    <note>
     <para>
      Bear in mind that without rebooting your system, the changes will not be
      applied.
     </para>
    </note>
   </step>
  </procedure>
  <warning>
   <para>
    If you do not reboot your machine before performing further changes,
    the <command>transactional-update</command> command will create a new snapshot from the current root file
    system. This means that you will end up with several parallel snapshots,
    each including that particular change but not changes from the other
    invocations of the command. After reboot, the most recently created
    snapshot will be used as your new root file system, and it will not include
    changes done in the previous snapshots.
   </para>
  </warning>
 </section>
 <section xml:id="how-it-works-transactional-update-repositories">
  <title>Software repositories</title>
  <para>
   The current ALP image points to the following two software
   repositories:
  </para>
  <variablelist>
   <varlistentry>
    <term>ALP</term>
    <listitem>
     <para>
      <literal>https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/repo/ALP-0.1-x86_64-Media1/</literal>
     </para>
     <para>
      This repository is enabled. It is a subset of the build repository and an
      equivalent of the <literal>POOL</literal> repository known from other
      SUSE software products. It will remain unchanged until the release of
      the next ALP prototype.
     </para>
     <tip>
      <para>
       If you need a package which is not included in the
       <literal>ALP</literal> repository, you may find it in the
       <literal>ALP-Build</literal> repository. To enable it, run:
      </para>
<screen><prompt role="root"># </prompt>zypper mr -e ALP-Build</screen>
     </tip>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ALP-Build</term>
    <listitem>
     <para>
      <literal>https://download.opensuse.org/repositories/SUSE:/ALP/standard/</literal>
     </para>
     <para>
      This repository is disabled by default. It is used for building the
      project. It includes all packages built in the
      <literal>SUSE:ALP</literal> project in the build service and will be
      moving forward over the time with future development.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="benefits-transactional-update">
  <title>Benefits of transactional updates</title>
  <itemizedlist>
   <listitem>
    <para>
     They are atomicâ€”the update is applied only if it completes
     successfully.
    </para>
   </listitem>
   <listitem>
    <para>
     Changes are applied in a separate snapshot and so do not influence the
     running system.
    </para>
   </listitem>
   <listitem>
    <para>
     Changes can easily be rolled back.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-transactional-update">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Usage of the <command>transactional-update</command> command is detailed in <xref linkend="reference-transactional-update-usage"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-transactional-update-usage"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>transactional-update</command> command</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="sec-command-list">
  <title><command>transactional-update</command> usage</title>
  <para>
   The <command>transactional-update</command> command enables the atomic installation or removal of updates.
   Updates are applied only if all of them can be successfully installed.
   <command>transactional-update</command> creates a snapshot of your system and uses it to update the system.
   Later you can restore this snapshot. All changes become active only after
   reboot.
  </para>
  <para>
   The <command>transactional-update</command> command syntax is as follows:
  </para>
<screen>
transactional-update <option>[option]</option> <replaceable>[general_command]</replaceable> <replaceable>[package_command]</replaceable> <replaceable>standalone_command</replaceable>
</screen>
  <note>
   <title>Running <command>transactional-update</command> without arguments</title>
   <para>
    If you do not specify any command or option while running the <command>transactional-update</command>
    command, the system updates itself.
   </para>
  </note>
  <para>
   Possible command parameters are described further.
  </para>
  <variablelist>
   <title><command>transactional-update</command> options</title>
   <varlistentry>
    <term><option>--interactive, -i</option></term>
    <listitem>
     <para>
      Can be used along with a package command to turn on interactive mode.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--non-interactive, -n</option></term>
    <listitem>
     <para>
      Can be used along with a package command to turn on non-interactive mode.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--continue [<replaceable>number</replaceable>], -c</option></term>
    <listitem>
     <para>
      The <option>--continue</option> option is for making multiple changes to
      an existing snapshot without rebooting.
     </para>
     <para>
      The default <command>transactional-update</command> behavior is to create a new snapshot from the current
      root file system. If you forget something, such as installing a new
      package, you have to reboot to apply your previous changes, run <command>transactional-update</command>
      again to install the forgotten package, and reboot again. You cannot run
      the <command>transactional-update</command> command multiple times without rebooting to add more changes
      to the snapshot, because this will create separate independent snapshots
      that do not include changes from the previous snapshots.
     </para>
     <para>
      Use the <option>--continue</option> option to make as many changes as you
      want without rebooting. A separate snapshot is made each time, and each
      snapshot contains all the changes you made in the previous snapshots,
      plus your new changes. Repeat this process as many times as you want, and
      when the final snapshot includes everything you want, reboot the system,
      and your final snapshot becomes the new root file system.
     </para>
     <para>
      Another useful feature of the <option>--continue</option> option is that you
      may select any existing snapshot as the base for your new snapshot. The
      following example demonstrates running <command>transactional-update</command> to install a new package
      in a snapshot based on snapshot 13, and then running it again to install
      another package:
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_1</replaceable></command></screen>
<screen><prompt role="root"># </prompt><command>transactional-update --continue 13 pkg install <replaceable>package_2</replaceable></command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--no-selfupdate</option></term>
    <listitem>
     <para>
      Disables self-updating of <command>transactional-update</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--drop-if-no-change, -d</option></term>
    <listitem>
     <para>
      Discards the snapshot created by <command>transactional-update</command> if there were no changes to the
      root file system. If there are some changes to the
      <filename>/etc</filename> directory, those changes merged back to the
      current file system.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--quiet</option></term>
    <listitem>
     <para>
      The <command>transactional-update</command> command will not output to <literal>stdout</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--help, -h</option></term>
    <listitem>
     <para>
      Prints help for the <command>transactional-update</command> command.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--version</option></term>
    <listitem>
     <para>
      Displays the version of the <command>transactional-update</command> command.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   The general commands are the following:
  </para>
  <variablelist>
   <title>General commands</title>
   <varlistentry>
    <term><command>cleanup-snapshots</command></term>
    <listitem>
     <para>
      The command marks all unused snapshots that are intended to be removed.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>cleanup-overlays</command></term>
    <listitem>
     <para>
      The command removes all unused overlay layers of
      <filename>/etc</filename>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>cleanup</command></term>
    <listitem>
     <para>
      The command combines the <command>cleanup-snapshots</command> and
      <command>cleanup-overlays</command> commands.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>grub.cfg</command></term>
    <listitem>
     <para>
      Use this command to rebuild the GRUB boot loader configuration file.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>bootloader</command></term>
    <listitem>
     <para>
      The command reinstalls the boot loader.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>initrd</command></term>
    <listitem>
     <para>
      Use the command to rebuild <literal>initrd</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>kdump</command></term>
    <listitem>
     <para>
      In case you perform changes to your hardware or storage, you may need to
      rebuild the kdump initrd.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>shell</command></term>
    <listitem>
     <para>
      Opens a read-write shell in the new snapshot before exiting. The command
      is typically used for debugging purposes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>reboot</command></term>
    <listitem>
     <para>
      The system reboots after the <command>transactional-update</command> command is complete.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>run <replaceable>&lt;command&gt;</replaceable></command></term>
    <listitem>
     <para>
      Runs the provided command in a new snapshot.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>setup-selinux</command></term>
    <listitem>
     <para>
      Installs and enables targeted SELinux policy.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   The package commands are the following:
  </para>
  <variablelist>
   <title>Package commands</title>
   <varlistentry>
    <term><command>dup</command></term>
    <listitem>
     <para>
      Performs upgrade of your system. The default option for this command is
      <option>--non-interactive</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>migration</command></term>
    <listitem>
     <para>
      The command migrates your system to a selected target. Typically, it is
      used to upgrade your system if it has been registered via SUSE Customer Center.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>patch</command></term>
    <listitem>
     <para>
      Checks for available patches and installs them. The default option for
      this command is <option>--non-interactive</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>pkg install</command></term>
    <listitem>
     <para>
      Installs individual packages from the available channels using the
      <command>zypper install</command> command. This command can also be used
      to install Program Temporary Fix (PTF) RPM files. The default option for
      this command is <option>--interactive</option>.
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_name</replaceable></command></screen>
     <para>
      or
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>rpm1 rpm2</replaceable></command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>pkg remove</command></term>
    <listitem>
     <para>
      Removes individual packages from the active snapshot using the
      <command>zypper remove</command> command. This command can also be used
      to remove PTF RPM files. The default option for this command is
      <option>--interactive</option>.
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg remove <replaceable>package_name</replaceable></command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>pkg update</command></term>
    <listitem>
     <para>
      Updates individual packages from the active snapshot using the
      <command>zypper update</command> command. Only packages that are part of
      the snapshot of the base file system can be updated. The default option
      for this command is <option>--interactive</option>.
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg update <replaceable>package_name</replaceable></command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>register</command></term>
    <listitem>
     <para>
      Registers or deregisters your system. For a complete usage description,
      refer to <xref linkend="sec-register-command"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>up</command></term>
    <listitem>
     <para>
      Updates installed packages to newer versions. The default option for this
      command is <option>--non-interactive</option>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   The standalone commands are the following:
  </para>
  <variablelist>
   <title>Standalone commands</title>
   <varlistentry>
    <term><command>rollback <replaceable>&lt;snapshot number&gt;</replaceable></command></term>
    <listitem>
     <para>
      This sets the default subvolume. The current system is set as the new
      default root file system. If you specify a number, that snapshot is used
      as the default root file system. On a read-only file system, it does not
      create any additional snapshots.
     </para>
<screen><prompt role="root"># </prompt><command>transactional-update rollback <replaceable>snapshot_number</replaceable></command></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>rollback last</command></term>
    <listitem>
     <para>
      This command sets the last known to be working snapshot as the default.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>status</command></term>
    <listitem>
     <para>
      This prints a list of available snapshots. The currently booted one is
      marked with an asterisk, the default snapshot is marked with a plus sign.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <section xml:id="sec-register-command">
   <title>The <command>register</command> command</title>
   <para>
    The <command>register</command> command enables you to handle all tasks
    regarding registration and subscription management. You can supply the
    following options:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--list-extensions</option></term>
     <listitem>
      <para>
       With this option, the command will list available extensions for your
       system. You can use the output to find a product identifier for product
       activation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-p, --product</option></term>
     <listitem>
      <para>
       Use this option to specify a product for activation. The product
       identifier has the following format:
       <emphasis>&lt;name&gt;/&lt;version&gt;/&lt;architecture&gt;</emphasis>,
       for example, <literal>sle-module-live-patching/15.3/x86_64</literal>. The
       appropriate command will then be the following:
      </para>
<screen><prompt role="root"># </prompt>transactional-update register -p sle-module-live-patching/15.3/x86_64</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-r, --regcode</option></term>
     <listitem>
      <para>
       Register your system with the registration code provided. The command
       will register the subscription and enable software repositories.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-d, --de-register</option></term>
     <listitem>
      <para>
       The option deregisters the system, or when used along with the
       <literal>-p</literal> option, deregisters an extension.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-e, --email</option></term>
     <listitem>
      <para>
       Specify an email address that will be used in SUSE Customer Center for registration.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--url</option></term>
     <listitem>
      <para>
       Specify the URL of your registration server. The URL is stored in the
       configuration and will be used in subsequent command invocations. For
       example:
      </para>
<screen><prompt role="root"># </prompt>transactional-update register --url https://scc.suse.com</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>-s, --status</option></term>
     <listitem>
      <para>
       Displays the current registration status in JSON format.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--write-config</option></term>
     <listitem>
      <para>
       Writes the provided options value to the
       <filename>/etc/SUSEConnect</filename> configuration file.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--cleanup</option></term>
     <listitem>
      <para>
       Removes old system credentials.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--version</option></term>
     <listitem>
      <para>
       Prints the version.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--help</option></term>
     <listitem>
      <para>
       Displays the usage of the command.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
 </section>
 <section xml:id="related-transactional-update-usage">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     General description of transactional updates is described in
     <xref linkend="concept-transactional-update"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-containers-podman"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Concepts of Containers and Podman</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <!-- highly inspired by https://susedoc.github.io/doc-sle/main/html/SLE-Micro-podman/article-podman.html -->
 <section xml:id="what-is-containers-podman">
  <title>What are containers and Podman?</title>
  <para>
   Containers offer a lightweight virtualization method to run multiple virtual
   environments (containers) simultaneously on a single host. Unlike
   technologies such as Xen or KVM, where the processor simulates a
   complete hardware environment and a hypervisor controls virtual machines,
   containers provide virtualization on the operating system level, where the
   kernel controls the isolated containers.
  </para>
  <para>
   <emphasis>Podman</emphasis> is a short name for Pod Manager Tool. It is a
   daemonless container engine that enables you to run and deploy applications
   using containers and container images. Podman provides a command line
   interface to manage containers.
  </para>
 </section>
 <section xml:id="how-it-works-podman">
  <title>How does Podman work?</title>
  <para>
   Podman provides integration with <systemitem class="daemon">systemd</systemitem>. This way you can control
   containers via <systemitem class="daemon">systemd</systemitem> units. You can create these units for existing
   containers as well as generate units that can start containers if they do
   not exist in the system. Moreover, Podman can run <systemitem class="daemon">systemd</systemitem> inside
   containers.
  </para>
  <para>
   Podman enables you to organize your containers into pods. Pods share the
   same network interface and resources. A typical use case for organizing a
   group of containers into a pod is a container that runs a database and a
   container with a client that accesses the database.
  </para>
  <section xml:id="pod-architecture">
   <title>Pods architecture</title>
   <para>
    A pod is a group of containers that share the same namespace, ports and
    network connection. Usually, containers within one pod can communicate
    directly with each other. Each pod contains an infrastructure container
    (<literal>INFRA</literal>), whose purpose is to hold the namespace.
    <literal>INFRA</literal> also enables Podman to add other containers to
    the pod. Port bindings, cgroup-parent values, and kernel namespaces are
    all assigned to the infrastructure container. Therefore, later changes of
    these values are not possible.
   </para>
   <figure xml:id="fig-pod-architecture">
    <title>Pods architecture</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="pods_architecture.svg" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="pods_architecture.svg" width="100%"/>
     </imageobject>
     <textobject role="description"><phrase>Pods architecture</phrase>
     </textobject>
    </mediaobject>
   </figure>
   <para>
    Each container in a pod has its own instance of a monitoring program. The
    monitoring program watches the container's process and if the container
    dies, the monitoring program saves its exit code. The program also holds
    open the tty interface for the particular container. The monitoring program
    enables you to run containers in the detached mode when Podman exits,
    because this program continues to run and enables you to attach tty later.
   </para>
  </section>
 </section>
 <section xml:id="benefits-containers-podman">
  <title>Benefits of containers</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Containers make it possible to isolate applications in self-contained
     units.
    </para>
   </listitem>
   <listitem>
    <para>
     Containers provide near-native performance. Depending on the runtime, a
     container can use the host kernel directly, thus minimizing overhead.
    </para>
   </listitem>
   <listitem>
    <para>
     It is possible to control network interfaces and apply resources inside
     containers through kernel control groups.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-containers-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Usage of the <command>podman</command> command is detailed in
     <xref linkend="reference-podman-usage"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     List of available workloads with links to their installation using
     Podman is included in
     <xref linkend="reference-available-alp-workloads"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-enable-podman"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Enabling Podman</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-enable-podman">
  <title>Introduction</title>
  <para>
   This article helps you verify that Podman is installed on the ALP
   system and provides guidelines to enable its <systemitem class="daemon">systemd</systemitem> service when
   Cockpit requires it.
  </para>
 </section>
 <section xml:id="requirements-enable-podman">
  <title>Requirements</title>
  <itemizedlist>
   <listitem>
    <para>
     Deployed ALP base OS.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="install-podman">
  <title>Installing Podman</title>
  <procedure>
   <step>
    <para>
     Verify that Podman is installed on your system by running the following
     command:
    </para>
<screen><prompt role="root"># </prompt>zypper se -i podman</screen>
   </step>
   <step>
    <para>
     If Podman is not listed in the output, install it by running:
    </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install podman*</screen>
   </step>
   <step>
    <para>
     Reboot the ALP host for the changes to take effect.
    </para>
   </step>
   <step>
    <para>
     Optionally, enable and start the
     <systemitem class="daemon">podman.service</systemitem> service for
     applications that require it, such as Cockpit. You can enable it either
     in Cockpit by clicking <menuchoice><guimenu>Podman
     containers</guimenu><guimenu>Start podman</guimenu></menuchoice>, or by
     running the following command:
    </para>
<screen><prompt role="root"># </prompt>systemctl enable --now podman.service</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="install-podman-rootless">
  <title>Enabling rootless mode</title>
  <para>
   By default, Podman requires <systemitem class="username">root</systemitem> privileges. To enable rootless mode
   for the current user, run the following command:
  </para>
<screen>
<prompt>&gt; </prompt>sudo usermod --add-subuids 100000-165535 \
  --add-subgids 100000-165535 <replaceable>USER</replaceable>
</screen>
  <para>
   Reboot the machine to enable the change. The command above defines a range
   of local UIDs to which the UIDs allocated to users inside the container are
   mapped on the host. Note that the ranges defined for different users must
   not overlap. It is also important that the ranges do not reuse the UID of an
   existing local user or group. By default, adding a user with the
   <command>useradd</command> command automatically allocates subUID and subGID
   ranges.
  </para>
  <note>
   <title>Limitations of rootless containers</title>
   <para>
    Running a container with Podman in rootless mode on SLE Micro may fail,
    because the container might need access to directories or files that
    require <systemitem class="username">root</systemitem> privileges.
   </para>
  </note>
 </section>
 <section xml:id="next-enable-podman">
  <title>Next steps</title>
  <itemizedlist>
   <listitem>
    <para>
     Run containerized workloads. For details, refer to
     <xref linkend="reference-available-alp-workloads"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-enable-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     Containers and Podman are outlined in
     <xref linkend="concept-containers-podman"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Usage of the <command>podman</command> command is listed in
     <xref linkend="reference-podman-usage"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Available workloads and links to their installation using Podman are
     listed in <xref linkend="reference-available-alp-workloads"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-podman-usage"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Podman usage</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
  <abstract xmlns:its="http://www.w3.org/2005/11/its">
   <para>
    This article introduces basic Podman usage that you may need when running
    containerized workloads.
   </para>
  </abstract>
 </info>
 
 <section xml:id="sec-getting-images">
  <title>Getting container images</title>
  <para>
   To run a container, you need an image. An image includes all dependencies
   needed to run an application. You can obtain images from an image registry.
   Available registries are defined in the
   <filename>/etc/containers/registries.conf</filename> configuration file. If
   you have a local image registry or want to use other registries, add the
   registries into the configuration file.
  </para>
  <important>
   <title>No tools for building images in ALP</title>
   <para>
    ALP does not provide tools for building custom images. Therefore,
    the only way to get an image is to pull it from an image registry.
   </para>
  </important>
  <para>
   The <command>podman pull</command> command pulls an image from an image
   registry. The syntax is as follows:
  </para>
<screen><prompt role="root"># </prompt>podman pull <replaceable>[OPTIONS]</replaceable> <replaceable>SOURCE</replaceable></screen>
  <para>
   The <replaceable>source</replaceable> can be an image without the registry
   name. In that case, Podman tries to pull the image from all registries
   configured in the <filename>/etc/containers/registries.conf</filename> file.
   The default image tag is <literal>latest</literal>. The default location of
   pulled images is
   <filename>/var/lib/containers/storage/overlay-images/</filename>.
  </para>
  <para>
   To view all possible options of the <command>podman pull</command> command,
   run:
  </para>
<screen><prompt role="root"># </prompt>podman pull --help</screen>
  <note>
   <title>Getting images using Cockpit</title>
   <para>
    If you are using Cockpit, you can also pull images from an image registry
    in the <guimenu>Podman containers</guimenu> menu by clicking <guimenu>+ Get
    new image</guimenu>.
   </para>
  </note>
  <para>
   Podman enables you to search for images in an image registry or a list of
   registries using the command:
  </para>
<screen><prompt role="root"># </prompt>podman search <replaceable>IMAGE_NAME</replaceable></screen>
 </section>
 <section xml:id="sec-working-containers">
  <title>Working with containers</title>
  <para>
   The following section covers common container management tasks. This
   includes creating, starting, and modifying containers.
  </para>
  <warning>
   <para>
    The current version of ALP does not provide tools for building custom
    images. Therefore, the only way to get a container image is to pull it from
    an image registry.
   </para>
  </warning>
  <section xml:id="sec-podman-run">
   <title>Running containers</title>
   <para>
    After you have pulled your container image, you can create containers based
    on it. You can run an instance of the image using the <command>podman
    run</command> command. The command syntax is as follows:
   </para>
<screen><prompt role="root"># </prompt>podman run [<replaceable>OPTIONS</replaceable>] <replaceable>IMAGE</replaceable> [<replaceable>CONTAINER_NAME</replaceable>]</screen>
   <para>
    <replaceable>IMAGE</replaceable> is specified in format
    <emphasis>transport:path</emphasis>. If <emphasis>transport</emphasis> is
    omitted, the default <literal>docker</literal> is used. The
    <emphasis>path</emphasis> can reference to a specific image registry. If
    omitted, Podman searches for the image in registries defined in the
    <filename>/etc/containers/registries.conf</filename> file. An example that
    runs a container called <literal>sles15</literal> based on the
    <literal>sle15</literal> image follows:
   </para>
<screen><prompt role="root"># </prompt>podman run registry.opensuse.org/suse/templates/images/sle-15-sp3/base/images/suse/sle15 sles15</screen>
   <para>
    Below is a list of frequently used options. For a complete list of
    available options, run the command: <command>podman run --help</command>.
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>--detach, -d</literal></term>
     <listitem>
      <para>
       The container will run in the background.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--env, -e=env</literal></term>
     <listitem>
      <para>
       This option allows arbitrary environment variables that are available
       for the process to be launched inside of the container. If an
       environment variable is specified without a value, Podman will check the
       host environment for a value and set the variable only if it is set on
       the host.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--help</literal></term>
     <listitem>
      <para>
       Prints help for the <command>podman run</command> command.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--hostname=</literal><emphasis>name</emphasis>,<literal> -h</literal></term>
     <listitem>
      <para>
       Sets the container host name that is available inside the container.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--pod=</literal><emphasis>name</emphasis></term>
     <listitem>
      <para>
       Runs the container in an existing pod. To create a pod, prefix the pod name
       with <literal>new:</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--read-only</literal></term>
     <listitem>
      <para>
       Mounts the containerâ€™s root file system as read-only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--systemd=true|false|always</literal></term>
     <listitem>
      <para>
       Runs the container in systemd mode. The default is true.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
  <section xml:id="sec-podman-stop">
   <title>Stopping containers</title>
   <para>
    If the <command>podman run</command> command finished successfully, a new
    container has been started. You can stop the container by running:
   </para>
<screen><prompt role="root"># </prompt>podman stop <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
   <para>
    You can specify a single container name or ID or a space-separated list of
    containers. The command takes the following options:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>--all, -a</literal></term>
     <listitem>
      <para>
       Stops all running containers.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--latest, -l</literal></term>
     <listitem>
      <para>
       Instead of providing a container name, the last created container will
       be stopped.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--time, -t=</literal><emphasis>seconds</emphasis></term>
     <listitem>
      <para>
       Seconds to wait before forcibly stopping the container.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    To view all possible options of the <command>podman stop</command> command,
    run the following:
   </para>
<screen><prompt role="root"># </prompt>podman stop --help</screen>
  </section>
  <section xml:id="sec-podman-start">
   <title>Starting containers</title>
   <para>
    To start already created but stopped containers, use the <command>podman
    start</command> command. The command syntax is as follows:
   </para>
<screen><prompt role="root"># </prompt>podman start <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
   <para>
    <replaceable>CONTAINER</replaceable> can be a container name or a container
    ID.
   </para>
   <para>
    For a complete list of possible options of <command>podman start</command>,
    run the command:
   </para>
<screen><prompt role="root"># </prompt>podman start --help</screen>
  </section>
  <section xml:id="sec-podman-commit">
   <title>Committing modified containers</title>
   <para>
    You can run a new container with specific attributes that are not part of
    the original image. To save the container with these attributes as a new
    image, you can use the <command>podman commit</command> command:
   </para>
<screen><prompt role="root"># </prompt>podman commit <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable> <replaceable>IMAGE</replaceable></screen>
   <para>
    <replaceable>CONTAINER</replaceable> is a container name or a container ID.
    <replaceable>IMAGE</replaceable> is the new image name. If the image name
    does not start with a registry name, the value <literal>localhost</literal>
    is used.
   </para>
   <para>
    When using Cockpit, you can perform the <command>commit</command> operation
    directly from a container's <guimenu>Details</guimenu>, by clicking
    <guimenu>Commit</guimenu>. A dialog box opens. Specify all required details
    as shown below and click <guimenu>Commit</guimenu>:
   </para>
   <figure>
    <title>Committing a container in Cockpit</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cockpit_commit_container.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cockpit_commit_container.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </figure>
  </section>
  <section xml:id="sec-podman-ps">
   <title>Listing containers</title>
   <para>
    Podman enables you to list all running containers using the <command>podman
    ps</command> command. The generic syntax of the command is as follows:
   </para>
<screen><prompt role="root"># </prompt>podman  ps <replaceable>[OPTIONS]</replaceable></screen>
   <para>
    Command options can change the displayed information. For example, using
    the <literal>--all</literal> option will output all containers created by
    Podman (not only the running containers).
   </para>
   <para>
    For a complete list of <command>podman ps</command> options, run:
   </para>
<screen><prompt role="root"># </prompt>podman ps --help</screen>
  </section>
  <section xml:id="sec-podman-rm">
   <title>Removing containers</title>
   <para>
    To remove one or more unused containers from the host, use the
    <command>podman rm</command> command as follows:
   </para>
<screen><prompt role="root"># </prompt>podman rm <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
   <para>
    <replaceable>CONTAINER</replaceable> can be a container name or a container
    ID.
   </para>
   <para>
    The command does not remove the specified container if the container is
    running. To remove a running container, use the <literal>-f</literal>
    option.
   </para>
   <para>
    For a complete list of <command>podman rm</command> options, run:
   </para>
<screen><prompt role="root"># </prompt>podman rm --help</screen>
   <note>
    <title>Deleting all stopped containers</title>
    <para>
     You can delete all stopped containers from your host with a single
     command:
    </para>
<screen><prompt role="root"># </prompt>podman container prune</screen>
    <para>
     Make sure that each stopped container is intended to be removed before you
     run the command, otherwise you might remove containers that are still in
     use and were stopped only temporarily.
    </para>
   </note>
  </section>
 </section>
 <section xml:id="sec-working-pods">
  <title>Working with pods</title>
  <para>
   Containers can be grouped into a pod. The containers in the pod then share
   network, pid, and IPC namespace. Pods can be managed by <command>podman
   pod</command> commands. This section provides an overview of the commands
   for managing pods.
  </para>
  <section xml:id="sec-creating-pods">
   <title>Creating pods</title>
   <para>
    The command <command>podman pod create</command> is used to create a pod.
    The syntax of the command is as follows:
   </para>
<screen><prompt role="root"># </prompt>podman pod create <replaceable>[OPTIONS]</replaceable></screen>
   <para>
    The command outputs the pod ID. By default, the pods are created without
    being started. You can start a pod by running a container in the pod, or by
    starting the pod as described in <xref linkend="sec-starting-pods"/>.
   </para>
   <note>
    <title>Default pod names</title>
    <para>
     If you do not specify a pod name with the <literal>--name</literal>
     option, Podman will assign a default name for the pod.
    </para>
   </note>
   <para>
    For a complete list of possible options, run the following command:
   </para>
<screen><prompt role="root"># </prompt>podman pod create --help</screen>
  </section>
  <section xml:id="sec-listing-pods">
   <title>Listing pods</title>
   <para>
    You can list all pods by running the command:
   </para>
<screen><prompt role="root"># </prompt>podman pod list</screen>
   <para>
    The output looks as follows:
   </para>
<screen>
POD ID        NAME               STATUS   CREATED       # OF CONTAINERS  INFRA ID
30fba506fecb  upbeat_mcclintock  Created  19 hours ago  1                4324f40c9651
976a83b4d88b  nervous_feynman    Running  19 hours ago  2                daa5732ecd02
</screen>
   <para>
    As each pod includes the <literal>INFRA</literal> container, the number of
    containers in a pod is always larger than zero.
   </para>
  </section>
  <section xml:id="sec-starting-pods">
   <title>Starting/stopping/restarting pods</title>
   <para>
    After a pod is created, you must start it, as it is not in the state
    <literal>running</literal> by default. In the commands below,
    <replaceable>POD</replaceable> can be a pod name or a pod ID.
   </para>
   <para>
    To start a pod, run the command:
   </para>
<screen><prompt role="root"># </prompt>podman pod start <replaceable>[OPTIONS]</replaceable> <replaceable>POD</replaceable></screen>
   <para>
    For a complete list of possible options, run:
   </para>
<screen><prompt role="root"># </prompt>podman pod start --help</screen>
   <para>
    To stop a pod, use the <command>podman pod stop</command> as follows:
   </para>
<screen><prompt role="root"># </prompt>podman pod stop <replaceable>POD</replaceable></screen>
   <para>
    To restart a pod, use the <command>podman pod restart</command> command as
    follows:
   </para>
<screen><prompt role="root"># </prompt>podman pod restart <replaceable>POD</replaceable></screen>
  </section>
  <section xml:id="sec-adding-pods">
   <title>Managing containers in a pod</title>
   <para>
    To add a new container to a pod, use the <command>podman run</command>
    command with the option <literal>--pod</literal>. A general syntax of the
    command follows:
   </para>
<screen><prompt role="root"># </prompt>podman run <replaceable>[OPTIONS]</replaceable> --pod <replaceable>POD_NAME</replaceable> <replaceable>IMAGE</replaceable></screen>
   <para>
    For details about the <command>podman run</command> command, refer to
    <xref linkend="sec-podman-run"/>.
   </para>
   <note>
    <title>Only new containers can be added to a pod</title>
    <para>
     The <command>podman start</command> command does not allow for starting a
     container in a pod if the container was not added to the pod during the
     container's initial running.
    </para>
   </note>
   <para>
    You cannot remove a container from a pod and keep the container running,
    because the container itself is removed from the host.
   </para>
   <para>
    Other actions like start, restart and stop can be performed on specific
    containers without affecting the status of the pod.
   </para>
  </section>
  <section xml:id="sec-removing-pods">
   <title>Removing pods</title>
   <para>
    There are two ways to remove pods. You can use the <command>podman pod
    rm</command> command to remove one or more pods. Alternatively, you can
    remove all stopped pods using the <command>podman pod prune</command>
    command.
   </para>
   <para>
    To remove a pod or several pods, run the <command>podman pod rm</command>
    command as follows:
   </para>
<screen><prompt role="root"># </prompt>podman pod rm <replaceable>POD</replaceable></screen>
   <para>
    <replaceable>POD</replaceable> can be a pod name or a pod ID.
   </para>
   <para>
    To remove all currently stopped pods, use the <command>podman pod
    prune</command> command. Make sure that all stopped pods are intended to be
    removed before you run the <command>podman pod prune</command> command,
    otherwise you might remove pods that are still in use.
   </para>
  </section>
  <section xml:id="sec-monitoring-pods">
   <title>Monitoring processes in pods</title>
   <para>
    To view all containers in all pods, use the following command:
   </para>
<screen><prompt role="root"># </prompt>podman ps -a --pod</screen>
   <para>
    The output of the command will be similar to the following one:
   </para>
   <!-- Decreased text size and removed a few columns to make this fit on the PDF. -->
<screen>
<?dbsuse-fo font-size="0.70em"?>
CONTAINER ID  IMAGE                       COMMAND    CREATED       STATUS                 [...]
4324f40c9651  k8s.gcr.io/pause:3.2                   21 hours ago  Created
daa5732ecd02  k8s.gcr.io/pause:3.2                   22 hours ago  Up 3 hours ago
e5c8e360c54b  localhost/test:latest       /bin/bash  3 days ago    Exited (137) 3 days ago
82dad15828f7  localhost/opensuse/toolbox  /bin/bash  3 days ago    Exited (137) 3 days ago
1a23da456b6f  docker.io/i386/ubuntu       /bin/bash  4 days ago    Exited (0) 6 hours ago
df890193f651  localhost/opensuse/toolbox  /bin/bash  4 days ago    Created
  </screen>
   <para>
    The first two records are the <literal>INFRA</literal> containers of each
    pod, based on the <literal>k8s.gcr.io/pause:3.2</literal> image. Other
    containers in the output are stand-alone containers that do not belong to
    any pod.
   </para>
  </section>
  <section xml:id="related-podman-usage">
   <title>Related topics</title>
   <itemizedlist>
    <listitem>
     <para>
      Enabling Podman is described in <xref linkend="task-enable-podman"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Containers and Podman are outlined in <xref linkend="concept-containers-podman"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Available workloads and links to their installation are listed in <xref linkend="reference-available-alp-workloads"/>.
     </para>
    </listitem>
   </itemizedlist>
  </section>
 </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-yast-with-podman"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Running the YaST workload using Podman</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-run-yast-with-podman">
  <title>Introduction</title>
  <para>
   This article describes how to start the YaST workload on the Adaptable Linux Platform
   (ALP).
  </para>
 </section>
 <section xml:id="requirements-run-yast-with-podman">
  <title>Requirements</title>
  <itemizedlist>
   <listitem>
    <para>
     Deployed ALP base OS.
    </para>
   </listitem>
   <listitem>
    <para>
     Installed and enabled Podman.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="alp-starting-yast-text-mode">
  <title>Starting YaST in text mode</title>
  <para>
   To start the text version (ncurses) of YaST as a workload, follow these
   steps:
  </para>
  <procedure>
   <step>
    <para>
     Identify the full URL address in a registry of container images, 
     for example:
    </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-ncurses
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses
[...]
</screen>
   </step>
   <step>
    <para>
     To start the container, run the following command:
    </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest
</screen>
    <figure>
     <title>YaST running in text mode on ALP</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase>YaST running in text mode on ALP</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </section>
 <section xml:id="alp-starting-yast-qt">
  <title>Starting graphical YaST</title>
  <para>
   Tor start the graphical Qt version of YaST as a workload, follow these
   steps:
  </para>
  <procedure>
   <step>
    <para>
     To view the graphical YaST on your local X server, you need to use SSH X
     forwarding. It requires the <package>xauth</package> package installed,
     applied by the host reboot:
    </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install xauth &amp;&amp; reboot</screen>
   </step>
   <step>
    <para>
     Connect to the ALP host using <command>ssh</command> with the X
     forwarding enabled:
    </para>
<screen><prompt>&gt; </prompt>ssh -X <replaceable>ALP_HOST</replaceable></screen>
   </step>
   <step>
    <para>
     Identify the full URL address in a registry of container images, for
     example:
    </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-qt
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
[...]
</screen>
   </step>
   <step>
    <para>
     To start the container, run the following command:
    </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest
</screen>
    <figure>
     <title>Running graphical YaST on top of ALP</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp-yast-qt.png" width="75%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp-yast-qt.png" width="75%"/>
      </imageobject>
      <textobject role="description"><phrase>Running graphical YaST on top of ALP</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </section>
 <section xml:id="related-run-yast-with-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     YaST is generally described in
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-yast-gui.html"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-kvm-with-podman"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Running the KVM virtualization workload using Podman</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-run-kvm-with-podman">
  <title>Introduction</title>
  <para>
   This article describes how to run KVM VM Host Server on the Adaptable Linux Platform (ALP).
  </para>
 </section>
 <section xml:id="requirements-run-kvm-with-podman">
  <title>Requirements</title>
  <itemizedlist>
   <listitem>
    <para>
     Deployed ALP base OS.
    </para>
   </listitem>
   <listitem>
    <para>
     When running ALP in a virtualized environment, you need
     to enable the nested KVM virtualization on the bare-metal host
     operating system and use <literal>kernel-default</literal> kernel
     instead of the default <literal>kernel-default-base</literal> in ALP.
    </para>
   </listitem>
   <listitem>
    <para>
     Installed and enabled Podman.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="alp-starting-kvm">
  <title>Starting the KVM workload</title>
  <para>
   ALP can serve as a host running virtual machines. The following
   procedure describes steps to prepare the ALP host to run
   containerized KVM VM Host Server and run an example VM Guest on top of it.
  </para>
  <procedure>
   <step>
    <para>
     Identify the KVM workload image:
    </para>
<screen>
<prompt role="root"># </prompt>podman search kvm
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm
</screen>
   </step>
   <step>
    <para>
     Pull the image from the registry and install all the wrapper scripts:
    </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm:latest</screen>
   </step>
   <step>
    <para>
     Create the <literal>libvirtd</literal> container from the downloaded
     image:
    </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh create</screen>
   </step>
   <step>
    <para>
     Start the container:
    </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh start</screen>
   </step>
   <step>
    <para>
     Optionally, run a VM Guest on top of the started KVM VM Guest using
     the <command>virt-install.sh</command> script.
    </para>
    <tip>
     <para>
      <command>virt-install.sh</command> uses the
      <filename>openSUSE-Tumbleweed-JeOS.x86_64-OpenStack-Cloud.qcow2</filename>
      image by default. To specify another VM image, modify the
      <option>APPLIANCE_MIRROR</option> and <option>APPLIANCE</option> options
      in the <filename>/etc/kvm-container.conf</filename> file.
     </para>
    </tip>
    <tip>
     <para>
      <command>virsh.sh</command> is a wrapper script to launch the
      <command>virsh</command> command inside the container (the default
      container name is <command>libvirtd</command>).
     </para>
    </tip>
<screen>
<prompt>&gt; </prompt>virt-install.sh
[...]
Starting install...
Password for first root login is: OPjQok1nlfKp5DRZ
Allocating 'Tumbleweed-JeOS_5221fd7860.qcow2'            |    0 B  00:00:00 ...
Creating domain...                                       |    0 B  00:00:00
Running text console command: virsh --connect qemu:///system console Tumbleweed-JeOS_5221fd7860
Connected to domain 'Tumbleweed-JeOS_5221fd7860'
Escape character is ^] (Ctrl + ])

Welcome to openSUSE Tumbleweed 20220919 - Kernel 5.19.8-1-default (hvc0).

eth0: 192.168.10.67 fe80::5054:ff:fe5a:c416

localhost login:
</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="related-run-kvm-with-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     ALP deployment is described in
     <xref linkend="concept-alp-deployment"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Enabling KVM nested virtualization is described in
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-vt-installation.html#sec-vt-installation-nested-vms"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Enabling Podman is described in <xref linkend="task-enable-podman"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find details about <systemitem class="library">libvirt</systemitem> in
     <link xlink:href="https://susedoc.github.io/doc-sle/main/html/SLES-virtualization/part-virt-libvirt.html"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-cockpit-with-podman"><info>
  <title xmlns:its="http://www.w3.org/2005/11/its">Running the Cockpit Web server using Podman</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:its="http://www.w3.org/2005/11/its">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Smart Docs</dm:component>
    <dm:product>Documentation</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <dm:translation>no</dm:translation>
  </dm:docmanager>
 </info>
 
 <section xml:id="introduction-run-cockpit-with-podman">
  <title>Introduction</title>
  <para>
   This article describes how to run a containerized Cockpit Web server
   on the Adaptable Linux Platform (ALP) using Podman.
  </para>
  <note>
   <para>
    An alternative way of installing and enabling the Cockpit Web server is
    described in
    <link xlink:href="https://en.opensuse.org/openSUSE:ALP/Workgroups/SysMngmnt/Cockpit#Install_the_Web_Server_Via_Packages"/>.
   </para>
  </note>
 </section>
 <section xml:id="requirements-run-cockpit-with-podman">
  <title>Requirements</title>
  <itemizedlist>
   <listitem>
    <para>
     Deployed ALP base OS.
    </para>
   </listitem>
   <listitem>
    <para>
     Installed and enabled Podman.
    </para>
   </listitem>
   <listitem>
    <para>
     Installed the <package>alp_cockpit</package> pattern.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="alp-starting-cockpit">
  <title>Starting the Cockpit workload</title>
  <para>
   Cockpit is a tool to administer one or more hosts from one place via a Web
   user interface. Its default functionality is extended by plug-ins that you
   can install additionally. You do not need the Cockpit Web user interface
   installed on every ALP host. One instance of the Web interface can
   connect to multiple hosts if they have the <package>alp_cockpit</package>
   pattern installed.
  </para>
  <para>
   ALP has the base part of the Cockpit component installed by
   default. It is included in the <package>alp_cockpit</package> pattern. To
   install and run Cockpit's Web interface, follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Identify the Cockpit Web server workload image:
    </para>
<screen>
<prompt role="root"># </prompt>podman search cockpit-ws
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws
</screen>
   </step>
   <step>
    <para>
     Pull the image from the registry:
    </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
   </step>
   <step>
    <para>
     Run the Cockpit's containerized Web server:
    </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name cockpit-ws run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
   </step>
   <step>
    <para>
     To run the Cockpit's Web server on each ALP boot, enable its
     service:
    </para>
<screen><prompt role="root"># </prompt>systemctl enable cockpit.service</screen>
   </step>
   <step>
    <para>
     To view the Cockpit Web user interface, point your Web browser to the
     following address and accept the self-signed certificate:
    </para>
<screen>https://<replaceable>HOSTNAME_OR_IP_OF_ALP_HOST:9090</replaceable></screen>
    <figure>
     <title>Cockpit running on ALP</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="alp-cockpit.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="alp-cockpit.png" width="100%"/>
      </imageobject>
      <textobject role="description"><phrase>Cockpit running on ALP</phrase>
      </textobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </section>
 <section xml:id="next-run-cockpit-with-podman">
  <title>Next steps</title>
  <itemizedlist>
   <listitem>
    <para>
     Administer the system using Cockpit.
    </para>
   </listitem>
   <listitem>
    <para>
     Install and run additional workloads. For their list and description,
     refer to <xref linkend="reference-available-alp-workloads"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="related-run-cockpit-with-podman">
  <title>Related topics</title>
  <itemizedlist>
   <listitem>
    <para>
     ALP deployment is described in
     <xref linkend="concept-alp-deployment"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Installing software packages and patterns is detailed in
     <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Enabling Podman is described in <xref linkend="task-enable-podman"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Podman usage is listed in <xref linkend="reference-podman-usage"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Managing <systemitem class="daemon">systemd</systemitem> services is described in
     <link xlink:href="https://documentation.suse.com/smart/linux/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html"/>.
    </para>
   </listitem>
  </itemizedlist>
 </section>
</section></chapter></book>
