<?xml version="1.0"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="orig-DC-assembly-alp.xml" version="5.2" xml:lang="en"><info>
 <title>The Adaptable Linux Platform Guide</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
    <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
    <dm:component>Documentation</dm:component>
    <dm:product>ALP</dm:product>
    <dm:assignee>tbazant@suse.com</dm:assignee>
   </dm:bugtracker>
   <!-- 2022-11-07 toba: removing for now
   <dm:editurl>https://github.com/SUSE/doc-modular/edit/main/xml/</dm:editurl>
   -->
   <dm:translation>no</dm:translation>
  </dm:docmanager>
  <!-- can we try to use the same date format here as in other docs? how can we use the    
   <date><?dbtimestamp format="B d, Y"?></date>
   format here? -->
  <pubdate>
   <!-- enter the original publishing date -->
  </pubdate>
  <meta name="updated" content="enter ISO date of last update as YYYY-MM-DD"/>
  <meta name="time-to-read" content="enter time to read in minutes"/>
  <meta name="bugtracker"><phrase role="url">https://bugzilla.suse.com/enter_bug.cgi</phrase><phrase role="component">Non-product-specific documentation</phrase><phrase role="product">Smart Docs</phrase><phrase role="assignee">tbazant@suse.com</phrase>
  </meta>
  <meta name="translation"><phrase role="trans">no</phrase><phrase role="language">
    <!-- comma-separated list of languages, for example en-us,de-de,cs-cz --></phrase>
  </meta>
  <meta name="refers-to" content="enter id of parent chapter or article"/>
  <meta name="architecture" content=""/>
  <meta name="Adaptable Linux Platform"><productname version="1">Adaptable Linux Platform</productname>
  </meta>
  <meta name="title">Adaptable Linux Platform</meta>
  <meta name="description">The Adaptable Linux Platform (ALP) is a lightweight operating system. Instead of applications distributed in traditional software packages, it runs containerized and virtualized workloads.</meta>
  <meta name="social-descr"/>
  <meta name="targetgroup" content=""/>
  <meta name="function" content=""/>
  <meta name="task" content=""/>
  <meta name="category" content=""/>
  <!--        <xi:include href="common_copyright_gfdl.xml"/>-->
  <abstract>
   <para>
    This guide introduces the Adaptable Linux Platform (ALP)—its deployment, system
    management and software installation as well as running of containerized
    workloads. To enhance this ALP documentation, find its sources at
    <link xlink:href="https://github.com/SUSE/doc-modular/edit/main/xml/"/>.
   </para>
   <variablelist>
    <varlistentry>
     <term>WHAT?</term>
     <listitem>
      <para>
       ALP is a lightweight operating system. Instead of
       applications distributed in traditional software packages, it runs
       containerized and virtualized workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WHY?</term>
     <listitem>
      <para>
       This guide introduces an overview of what ALP is and how it is
       different from traditional operating systems. It also describes how to
       administer ALP and install and manage individual workloads.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>EFFORT?</term>
     <listitem>
      <para>
       To understand the concepts and perform tasks described in this guide,
       you need to have good knowledge and practice with the Linux
       operating system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>GOAL!</term>
     <listitem>
      <para>
       After having read this guide, you will be able to deploy ALP,
       modify its file system in a transactional way, and install and run
       specific workloads on top of it.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </abstract>
 </info>
 
<chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp"><info><title xmlns:its="http://www.w3.org/2005/11/its">General description</title></info>
  
  <section xml:id="what-is-alp">
    <title>What is ALP?</title>
    <para>
      The Adaptable Linux Platform (ALP) is a lightweight operating system. Instead of
      applications distributed in traditional software packages, it runs
      containerized and virtualized workloads.
    </para>
  </section>
  <section xml:id="how-it-works-alp">
    <title>Core components of ALP</title>
    <para>
      The Adaptable Linux Platform (ALP) consists of the following components:
    </para>
    <variablelist>
      <varlistentry>
        <term>Base operating system</term>
        <listitem>
          <para>
            The core of ALP which runs all required services. It is an
            immutable operating system with a read-only root file system. The
            file system is modified by transactional updates which utilize the
            snapshotting feature of BTRFS.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Transactional updates</term>
        <listitem>
          <para>
            The <command>transactional-update</command> command performs changes on the file system. You can
            use it to install software, update existing workloads, or apply
            software patches. Because it uses file system snapshots, applied
            changes can be easily rolled back.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Container orchestration</term>
        <listitem>
          <para>
            ALP runs containerized workloads instead of applications
            packed in software packages. The default container orchestrator in
            ALP is Podman which is responsible for managing containers
            and container images.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Containerized workloads</term>
        <listitem>
          <para>
            Workloads replace traditional applications. A containerized
            workload contains all software dependencies required to run a
            specific application or tool.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Cockpit</term>
        <listitem>
          <para>
            A Web-based graphical interface to administer single or multiple
            ALP workloads from one place. It helps you manage, for
            example, user accounts, network settings, or container
            orchestration.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="benefits-alp">
    <title>Benefits of ALP</title>
    <para>
      The Adaptable Linux Platform offers the following customer benefits:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          High security of running workloads.
        </para>
      </listitem>
      <listitem>
        <para>
          Minimal maintenance with keeping the workloads up to date.
        </para>
      </listitem>
      <listitem>
        <para>
          Stable immutable base operating system that utilizes transactions
          when modifying the file system.
        </para>
      </listitem>
      <listitem>
        <para>
          Ability to roll back modifications on the file system in case the
          transaction result is undesirable.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-alp">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Find more details about ALP deployment in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Transactional updates are detailed in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman is introduced in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Available workloads are described in
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-alp-deployment"><info><title xmlns:its="http://www.w3.org/2005/11/its">Deployment</title></info>
  
  <section xml:id="what-is-alp-deployment">
    <title>Introduction</title>
    <para>
      The Adaptable Linux Platform (ALP) is distributed as a pre-built raw disk image. You
      can download the latest published image from
      <link xlink:href="https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/"/>. There are two types of ALP
      images, depending whether you intend to run ALP on an encrypted
      disk or an unencrypted disk. You can deploy ALP either with a
      minimal initial configuration (JeOS Firstboot), or use additional
      tools—Combustion and Ignition—to specify a detailed
      system setup.
    </para>
  </section>
  <section xml:id="alp-deployment-firstboot-detection">
    <title>First boot detection</title>
    <para>
      The deployment configuration runs on the first boot only. To distinguish
      between the first and subsequent boots, the flag file
      <filename>/boot/writable/firstboot_happened</filename> is created after
      the first boot finishes. If the file is not present in the file system,
      the attribute <literal>ignition.firstboot</literal> is passed to the
      kernel command line, which triggers the creation of
      <filename>initramfs</filename> (Ignition) or running a specific dracut
      module (Combustion). After completing the first boot, the
      <filename>/boot/writable/firstboot_happened</filename> flag file is
      created.
    </para>
    <note>
      <title>The flag file is always created</title>
      <para>
        Even though the configuration may not be successful, due to improper or
        missing configuration files, the
        <filename>/boot/writable/firstboot_happened</filename> flag file is
        created.
      </para>
    </note>
    <tip>
      <para>
        You may force the first boot configuration on subsequent boot by
        passing the <literal>ignition.firstboot</literal> attribute to the
        kernel command line or by deleting the
        <filename>/boot/writable/firstboot_happened</filename> flag file.
      </para>
    </tip>
  </section>
  <section xml:id="alp-deployment-default-partitioning">
    <title>Default partitioning</title>
    <para>
      The pre-built images are delivered with a default partitioning scheme.
      You can change it during the first boot by using Ignition or
      Combustion.
    </para>
    <important>
      <title>BTRFS is mandatory for the root file system</title>
      <para>
        If you intend to perform any changes to the default partitioning
        scheme, the root file system must be BTRFS.
      </para>
    </important>
    <para>
      Each image has the following subvolumes:
    </para>
<screen>
 /home
 /root
 /opt
 /srv
 /usr/local
 /var
 </screen>
    <para>
      The <literal>/etc</literal> directory is mounted as overlayfs, where the
      upper directory is mounted to
      <filename>/var/lib/overlay/1/etc/</filename>.
    </para>
    <para>
      You can recognize the subvolumes mounted by default by the option
      <literal>x-initrd.mount</literal> in <filename>/etc/fstab</filename>.
      Other subvolumes or partitions must be configured either by Ignition or
      Combustion.
    </para>
  </section>
  <section xml:id="related-alp-deployment">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          For deploying with minimal configuration, refer to
          <xref linkend="task-deploy-alp"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For adjusting the deployment with Ignition, refer to
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For adjusting the deployment with Combustion, refer to
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          List
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-deploy-alp"><info><title xmlns:its="http://www.w3.org/2005/11/its">Deploying ALP</title></info>
  
  <section xml:id="introduction-deploy-alp">
    <title>Introduction</title>
    <para>
      This article describes how to deploy the Adaptable Linux Platform (ALP) raw disk
      image. It applies to ALP running both on encrypted and unencrypted
      disk.
    </para>
  </section>
  <section xml:id="requirements-deploy-alp">
    <title>Hardware requirements</title>
    <para>
      The minimum supported hardware requirements for deploying ALP
      follow:
    </para>
    <variablelist>
      <varlistentry>
        <term>CPU</term>
        <listitem>
          <para>
            AMD64/Intel 64 CPU architecture is supported
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Maximum number of CPUs</term>
        <listitem>
          <para>
            The maximum number of CPUs supported by software design is 8192.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Memory</term>
        <listitem>
          <para>
            ALP requires at least 1 GB RAM. Bear in mind that this
            is a minimal value for the operation system, the actual memory size
            depends on the workload.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Hard disk</term>
        <listitem>
          <para>
            The minimum hard disk space is 12GB, while the recommended value is
            20GB of hard disk space. Adjust the value according to the
            workloads of your containers.
          </para>
          <important>
            <title>Encrypted image does not expand to the full disk capacity</title>
            <para>
              As of now, the encrypted image does not expand to the full disk
              capacity automatically. As a workaround, the following steps are
              required:
            </para>
            <procedure>
              <step>
                <para>
                  Use the <command>qemu-img</command> command to increase the
                  disk image to the desired size.
                </para>
              </step>
              <step>
                <para>
                  Setup the virtual machine and boot it. When the
                  JeOS Firstboot wizard asks you which method to use for
                  encryption, select <guimenu>passphrase</guimenu>.
                </para>
              </step>
              <step>
                <para>
                  When the system is ready, use the <command>parted</command>
                  command to resize the partition where the LUKS device resides
                  (for example, partition number 3) to the desired size.
                </para>
              </step>
              <step>
                <para>
                  Run the <command>cryptsetup resize luks</command> command.
                  When asked, enter the passphrase to resize the encrypted
                  device.
                </para>
              </step>
              <step>
                <para>
                  Run the <command>transactional-update shell</command> command
                  to open a read-write shell in the current disk snapshot. Then
                  resize the BTRFS file system to the desired size, for
                  example:
                </para>
<screen><prompt role="root"># </prompt>btrfs fi resize max /</screen>
              </step>
              <step>
                <para>
                  Leave the shell with <command>exit</command> and reboot the
                  system with <command>reboot</command>.
                </para>
              </step>
            </procedure>
          </important>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="deploy-alp-kvm">
    <title>Deploying ALP on a KVM VM Host Server</title>
    <para>
      This procedure describes steps to deploy ALP as a KVM virtual
      machine using the Virtual Machine Manager.
    </para>
    <procedure>
      <step>
        <para>
          Download ALP virtual machine image on the host where you will
          run ALP. Go to <link xlink:href="https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/"/> and download the latest disk
          image of ALP.
        </para>
        <para>
          For example, for the unencrypted image:
        </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/ALP-VM.x86_64-0.0.1-kvm-Build15.17.qcow2</screen>
        <para>
          And for the encrypted image:
        </para>
<screen><prompt>&gt; </prompt>curl -LO https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/ALP-VM.x86_64-0.0.1-kvm_encrypted-Build15.18.qcow2</screen>
      </step>
      <step>
        <para>
          Start Virtual Machine Manager, select <menuchoice><guimenu>File</guimenu><guimenu>New
          VM</guimenu></menuchoice> and <guimenu>Import existing disk
          image</guimenu>. Confirm with <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the path to the ALP disk image that you previously
          downloaded and the type of linux OS you are deploying, for example,
          <literal>Generic Linux 2020</literal>. Confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the amount of memory and number of processors that you want
          to assign to the ALP virtual machine and confirm with
          <guimenu>Forward</guimenu>.
        </para>
      </step>
      <step>
        <para>
          Specify the name for the virtual machine and the network to be used.
        </para>
      </step>
      <step>
        <para>
          If you are deploying an encrypted ALP image, perform these
          additional steps:
        </para>
        <substeps>
          <step>
            <para>
              Enable <guimenu>Customize configuration before install</guimenu>
              and confirm with <guimenu>Finish</guimenu>.
            </para>
          </step>
          <step>
            <para>
              Click <guimenu>Overview</guimenu> from the left menu and change
              the boot method from BIOS to UEFI for secure boot. Confirm with
              <guimenu>Apply</guimenu>.
            </para>
            <figure>
              <title>Set UEFI firmware for the encrypted ALP image</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-uefi.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Set UEFI firmware for the encrypted ALP image</phrase>
                </textobject>
              </mediaobject>
            </figure>
          </step>
          <step>
            <para>
              Add a Trusted Platform Module (TPM) device. Click <guimenu>Add
              Hardware</guimenu>, select <guimenu>TPM</guimenu> from the left
              menu, and select the <guimenu>Emulated</guimenu> type.
            </para>
            <figure>
              <title>Add emulated TPM device</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-tpm.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Add emulated TPM device</phrase>
                </textobject>
              </mediaobject>
            </figure>
            <para>
              Confirm with <guimenu>Finish</guimenu> and start the ALP
              deployment by clicking <guimenu>Begin Installation</guimenu> from
              the top menu.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <substeps>
          <step>
            <para>
              If you want to deploy ALP with only minimal setup options,
              confirm with <guimenu>Finish</guimenu>. The ALP disk image
              will be booted and JeOS Firstboot will take care of the
              deployment. Refer to <xref linkend="deploy-alp-jeos-firstboot"/>
              for next steps.
            </para>
          </step>
          <step>
            <para>
              If you want to specify detailed deployment options, you need to
              use Ignition or Combustion tools to supply your setup during
              the disk image boot process. For more details, refer to
              <xref linkend="concept-configure-ignition"/> and
              <xref linkend="concept-configure-combustion"/>.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
  </section>
  <section xml:id="deploy-alp-jeos-firstboot">
    <title>Deploying ALP with JeOS Firstboot</title>
    <para>
      When booting the ALP raw image for the first time,
      <emphasis>JeOS Firstboot</emphasis> enables you to perform a minimal
      configuration of your system:
    </para>
    <procedure>
      <step>
        <para>
          After booting the ALP disk image, you will be presented with a
          bootloader screen. Select <guimenu>ALP</guimenu> and confirm
          with <keycap function="enter"/>.
        </para>
        <figure>
          <title>ALP boot screen</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-1.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-1.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>ALP boot screen</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          <guimenu>JeOS Firstboot</guimenu> displays a welcome screen. Confirm
          with <keycap function="enter"/>.
        </para>
        <figure>
          <title>JeOS Firstboot screen</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-firstboot.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>JeOS Firstboot screen</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          On the next screens, select keyboard, confirm the license agreement,
          and select the time zone.
        </para>
      </step>
      <step>
        <para>
          In the <guimenu>Enter root password</guimenu> dialog window, enter a
          password for the <systemitem class="username">root</systemitem> and confirm it.
        </para>
        <figure>
          <title>Enter root password</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-deploy-rootpwd.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Enter root password</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
      <step>
        <para>
          When deploying with an encrypted disk, follow these additional steps:
        </para>
        <substeps>
          <step>
            <para>
              Select the desired protection method and confirm with
              <guimenu>OK</guimenu>.
            </para>
            <figure>
              <title>Select method for encryption</title>
              <mediaobject>
                <imageobject role="fo">
                  <imagedata fileref="alp-deploy-encrypted-passkey.png" width="75%"/>
                </imageobject>
                <imageobject role="html">
                  <imagedata fileref="alp-deploy-encrypted-passkey.png" width="75%"/>
                </imageobject>
                <textobject role="description"><phrase>Select method for encryption</phrase>
                </textobject>
              </mediaobject>
            </figure>
          </step>
          <step>
            <para>
              Enter a recovery password for LUKS encryption and retype it. The
              root file system re-encryption will begin.
            </para>
          </step>
        </substeps>
      </step>
      <step>
        <para>
          ALP is successfully deployed using a minimal initial
          configuration.
        </para>
      </step>
    </procedure>
  </section>
  <section xml:id="summary-deploy-alp">
    <title>Summary</title>
    <para>
      After the deployment of ALP is finished, you are presented with
      the login prompt. Log in as <systemitem class="username">root</systemitem>, and you are ready to set up the
      system and install additional workloads.
    </para>
  </section>
  <section xml:id="next-deploy-alp">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Install additional software with <command>transactional-update</command>. Refer to
          <xref linkend="concept-transactional-update"/> for more details.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. Refer to
          <xref linkend="reference-available-alp-workloads"/> for more
          details.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-deploy-alp">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          General description of ALP is included in
          <xref linkend="concept-alp"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Advanced configuration of ALP deployment is described in
          <xref linkend="concept-configure-ignition"/> and
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find detailed information about using the Virtual Machine Manager in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-kvm-inst.html"/>
          and
          <link xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-ignition"><info><title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Ignition</title></info>
  
  <!-- highly inspired by sle/xml/deployment_images_ignition.xml -->
  <section xml:id="what-is-ignition">
    <title>What is Ignition?</title>
    <para>
      <link xlink:href="https://coreos.github.io/ignition/">Ignition</link>
      is a provisioning tool that enables you to configure a system according
      to your specification on the first boot.
    </para>
  </section>
  <section xml:id="how-it-works-ignition">
    <title>How does Ignition work?</title>
    <para>
      When the system is booted for the first time, Ignition is loaded as
      part of an <filename>initramfs</filename> and searches for a
      configuration file within a specific directory (on a USB flash disk, or
      you can provide a URL). All changes are performed before the kernel
      switches from the temporal file system to the real root file system
      (before the <literal>switch_root</literal> command is issued).
    </para>
    <para>
      Ignition uses a configuration file in the JSON format named
      <filename>config.ign</filename>. For the purpose of better human
      readability, you can create a YAML file and convert this file to JSON.
      For details, refer to 'Task: Converting YAML file into JSON'.
    </para>
    <section xml:id="sec-ignition-configuration">
      <title><filename>config.ign</filename></title>
      <para>
        When installing on bare metal, the configuration file
        <filename>config.ign</filename> must reside in the
        <filename>ignition</filename> subdirectory on the configuration media
        labeled <literal>ignition</literal>. The directory structure must look
        as follows:
      </para>
<screen>
&lt;root directory&gt;
└── ignition
    └── config.ign

 </screen>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>config.ign</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/com.coreos/config" file="/location/to/config.ign"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <para>
        The <filename>config.ign</filename> contains various data types:
        objects, strings, integers, booleans and lists of objects. For a
        complete specification, refer to
        <link xlink:href="https://coreos.github.io/ignition/configuration-v3_3/">Ignition
        specification v3.3.0</link>.
      </para>
      <para>
        The <literal>version</literal> attribute is mandatory and in case of
        ALP, its value must be set either to <literal>3.3.0</literal> or
        to any lower version. Otherwise, Ignition will fail.
      </para>
      <para>
        If you want to log in to your system as <systemitem class="username">root</systemitem>, you must at least
        include a password for <systemitem class="username">root</systemitem>. However, it is recommended to
        establish access via SSH keys. To configure a password, make sure to
        use a secure one. If you use a randomly generated password, use at
        least 10 characters. If you create your password manually, use even
        more than 10 characters and combine uppercase and lowercase letters and
        numbers.
      </para>
    </section>
  </section>
  <section xml:id="related-configure-ignition">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Converting YAML to JSON.
        </para>
      </listitem>
      <listitem>
        <para>
          Examples of Ignition configuration.
        </para>
      </listitem>
      <listitem>
        <para>
          Configuring with Combustion.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-convert-yaml-to-json"><info><title xmlns:its="http://www.w3.org/2005/11/its">Converting YAML formatted files into JSON</title></info>
  
  <section xml:id="introduction-convert-yaml-to-json">
    <title>Introduction</title>
    <para>
      JSON is a universal file format for storing structured data.
      Applications, for example, Ignition, use it to store and retrieve their
      configuration. Because JSON's syntax is complex and hard to read for
      human beings, you can write the configuration in a more friendly format
      called YAML and then convert it into JSON.
    </para>
  </section>
  <section xml:id="sec-converting-config">
    <title>Converting YAML files into JSON format</title>
    <para>
      The tool that converts Ignition-specific vocabularies in YAML files
      into JSON format is <literal>butane</literal>. It also verifies the
      syntax of the YAML file to catch potential errors in the structure. For
      the latest version of <literal>butane</literal>, add the following
      repository:
    </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command>  zypper ar -f \
  https://download.opensuse.org/repositories/devel:/kubic:/ignition/openSUSE_Tumbleweed/ \
  devel_kubic_ignition
</screen>
    <para>
      Replace <literal>openSUSE_Tumbleweed</literal> with one of the following
      (depending on your distribution):
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>'openSUSE_Leap_$releasever'</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>15.3</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Now you can install the <literal>butane</literal> tool:
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command>  zypper ref &amp;&amp; zypper in butane</screen>
    <para>
      After the installation is complete, you can invoke
      <literal>butane</literal> by running:
    </para>
<screen><prompt>&gt; </prompt> butane -p -o config.ign config.fcc</screen>
    <itemizedlist>
      <listitem>
        <para>
          <filename>config.fcc</filename> is the path to the YAML configuration
          file.
        </para>
      </listitem>
      <listitem>
        <para>
          <filename>config.ign</filename> is the path to the output JSON
          configuration file.
        </para>
      </listitem>
      <listitem>
        <para>
          The <option>-p</option> command option adds line breaks to the output
          file and thus makes it more readable.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="summary-convert-yaml-to-json">
    <title>Summary</title>
    <para>
      After you completed the described steps, you can write and store
      configuration files in YAML format while providing them in JSON format if
      applications, for example, Ignition, require it.
    </para>
  </section>
  <section xml:id="related-convert-yaml-to-json">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Configuration using Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Examples of the Ignition configuration are included in
          <xref linkend="reference-ignition-configuration"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-ignition-configuration"><info><title xmlns:its="http://www.w3.org/2005/11/its">Ignition configuration examples</title></info>
  
  <section xml:id="sec-ignition-examples">
    <title>Configuration examples in YAML</title>
    <para>
      This section will provide you with some common examples of the Ignition
      configuration in the YAML format. Note that Ignition does not accept
      configuration in the YAML format but rather JSON. To convert a YAML file
      to the JSON format, use the <literal>butane</literal> tool as described
      in <xref linkend="introduction-convert-yaml-to-json"/>.
    </para>
    <note>
      <title>The <literal>version</literal> attribute is mandatory</title>
      <para>
        Each <filename>config.fcc</filename> must include version 1.4.0 or
        lower that is then converted to the corresponding Ignition
        specification.
      </para>
    </note>
    <section xml:id="sec-ignition-storage">
      <title>Storage configuration</title>
      <para>
        The <literal>storage</literal> attribute is used to configure
        partitions, RAID, define file systems, create files, etc. To define
        partitions, use the <literal>disks</literal> attribute. The
        <literal>filesystems</literal> attribute is used to format partitions
        and define mount points of particular partitions. The
        <literal>files</literal> attribute can be used to create files in the
        file system. Each of the mentioned attributes is described in the
        following sections.
      </para>
      <section xml:id="sec-storage-disks">
        <title>The <literal>disks</literal> attribute</title>
        <para>
          The <literal>disks</literal> attribute is a list of devices that
          enables you to define partitions on these devices. The
          <literal>disks</literal> attribute must contain at least one
          <literal>device</literal>, other attributes are optional. The
          following example will use a single virtual device and divide the
          disk into four partitions:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  disks:
    - device: "/dev/vda"
      wipe_table: true
      partitions:
       - label: root
         number: 1
         type_guid: 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709
       - label: boot
         number: 2
         type_guid: BC13C2FF-59E6-4262-A352-B275FD6F7172
       - label: swap
         number: 3
         type_guid: 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
       - label: home
         number: 4
         type_guid: 933AC7E1-2EB4-4F13-B844-0E14E2AEF915
 </screen>
      </section>
      <section xml:id="sec-storage-raid">
        <title>The <literal>raid</literal> attribute</title>
        <para>
          The <literal>raid</literal> is a list of RAID arrays. The following
          attributes of <literal>raid</literal> are mandatory:
        </para>
        <variablelist>
          <varlistentry>
            <term>level</term>
            <listitem>
              <para>
                a level of the particular RAID array (linear, raid0, raid1,
                raid2, raid3, raid4, raid5, raid6)
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>devices</term>
            <listitem>
              <para>
                a list of devices in the array referenced by their absolute
                paths
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>name</term>
            <listitem>
              <para>
                a name that will be used for the md device
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
<screen>
variant: fcos
version: 1.0.0
storage:
  raid:
    - name: system
      level: raid1
      devices:
        - "/dev/sda"
        - "/dev/sdb"
 </screen>
      </section>
      <section xml:id="sec-storage-filesystem">
        <title>The <literal>filesystems</literal> attribute</title>
        <para>
          <literal>filesystems</literal> must contain the following attributes:
        </para>
        <variablelist>
          <varlistentry>
            <term>device</term>
            <listitem>
              <para>
                the absolute path to the device, typically
                <literal>/dev/sda</literal> in case of physical disk
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>format</term>
            <listitem>
              <para>
                the file system format (btrfs, ext4, xfs, vfat or swap)
              </para>
              <note>
                <para>
                  In case of ALP, the <literal>root</literal> file
                  system must be formatted to btrfs.
                </para>
              </note>
            </listitem>
          </varlistentry>
        </variablelist>
        <para>
          The following example demonstrates using the
          <literal>filesystems</literal> attribute. The
          <filename>/opt</filename> directory will be mounted to the
          <literal>/dev/sda1</literal> partition, which is formatted to btrfs.
          The device will not be erased.
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  filesystems:
    - path: /opt
      device: "/dev/sda1"
      format: btrfs
      wipe_filesystem: false
 </screen>
      </section>
      <section xml:id="sec-storage-files">
        <title>The <literal>files</literal> attribute</title>
        <para>
          You can use the <literal>files</literal> attribute to create any
          files on your machine. Bear in mind that if you want to create files
          outside the default partitioning schema, you need to define the
          directories by using the <literal>filesystems</literal> attribute.
        </para>
        <para>
          In the following example, a host name is created by using the
          <literal>files</literal> attribute. The file
          <filename>/etc/hostname</filename> will be created with the
          <emphasis>alp-1</emphasis> host name:
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      overwrite: true
      contents:
        inline: "alp-1"
 </screen>
      </section>
      <section xml:id="sec-storage-directories">
        <title>The <literal>directories</literal> attribute</title>
        <para>
          The <literal>directories</literal> attribute is a list of directories
          that will be created in the file system. The
          <literal>directories</literal> attribute must contain at least one
          <literal>path</literal> attribute.
        </para>
<screen>
variant: fcos
version: 1.0.0
storage:
  directories:
    - path: /home/tux
      user:
        name: tux
 </screen>
      </section>
    </section>
    <section xml:id="sec-ignition-users">
      <title>Users administration</title>
      <para>
        The <literal>passwd</literal> attribute is used to add users. If you
        intend to log in to your system, create <systemitem class="username">root</systemitem> and set the
        <systemitem class="username">root</systemitem>'s password and/or add the SSH key to the Ignition
        configuration. You need to hash the <systemitem class="username">root</systemitem> password, for example by
        using the <command>openssl</command> command:
      </para>
<screen>
 openssl passwd -6
 </screen>
      <para>
        The command creates a hash of the password you chose. Use this hash as
        the value of the <literal>password_hash</literal> attribute.
      </para>
<screen>
variant: fcos
version: 1.0.0
passwd:
  users:
   - name: root
     password_hash: "$6$PfKm6Fv5WbqOvZ0C$g4kByYM.D2B5GCsgluuqDNL87oeXiHqctr6INNNmF75WPGgkLn9O9uVx4iEe3UdbbhaHbTJ1vpZymKWuDIrWI1"
     ssh_authorized_keys:
       - ssh-rsa long...key user@host
 </screen>
      <para>
        The <literal>users</literal> attribute must contain at least one
        <literal>name</literal> attribute.
        <literal>ssh_authorized_keys</literal> is a list of ssh keys for the
        user.
      </para>
    </section>
    <section xml:id="sec-ignition-systemd">
      <title>Enabling <literal>systemd</literal> services</title>
      <para>
        You can enable <systemitem class="daemon">systemd</systemitem> services by specifying them in the
        <literal>systemd</literal> attribute.
      </para>
<screen>
variant: fcos
version: 1.0.0
systemd:
  units:
  - name: sshd.service
    enabled: true
 </screen>
      <para>
        The <literal>name</literal> must be the exact name of a service to be
        enabled (including the suffix).
      </para>
    </section>
  </section>
  <section xml:id="related-ignition-configuration">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Configuring the ALP deployment with Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Converting YAML files to the JSON format is described in
          <xref linkend="task-convert-yaml-to-json"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Configuring the ALP deployment with Combustion is described
          in <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section><section xml:lang="en" role="concept" version="5.1" xml:id="concept-configure-combustion"><info><title xmlns:its="http://www.w3.org/2005/11/its">Configuring with Combustion</title></info>
  
  <!-- highly inspired by sle/xml/deployment_images_combustion.xml -->
  <section xml:id="what-is-combustion">
    <title>What is Combustion?</title>
    <para>
      Combustion is a dracut module that enables you to configure your system
      on the first boot. You can use Combustion, for example, to change the
      default partitions, set user passwords, create files, or install
      packages.
    </para>
  </section>
  <section xml:id="how-it-works-combustion">
    <title>How does Combustion work?</title>
    <para>
      Combustion is invoked after the <literal>ignition.firstboot</literal>
      argument is passed to the kernel command line. Combustion reads a
      provided file named <filename>script</filename>, executes included
      commands, and thus performs changes to the file system. If
      <filename>script</filename> includes the network flag, Combustion tries
      to configure the network. After <literal>/sysroot</literal> is mounted,
      Combustion tries to activate all mount points in
      <filename>/etc/fstab</filename> and then calls
      <command>transactional-update</command> to apply other changes, for
      example, setting <systemitem class="username">root</systemitem> password or installing packages.
    </para>
    <section xml:id="sec-combustion-configuration">
      <title>The <filename>script</filename> file</title>
      <para>
        When installing on bare metal, the configuration file
        <filename>script</filename> must reside in the
        <filename>combustion</filename> subdirectory on the configuration media
        labeled <literal>combustion</literal>. The directory structure must
        look as follows:
      </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
</screen>
      <para>
        If you intend to configure a virtual machine with Virtual Machine Manager (<systemitem class="library">libvirt</systemitem>),
        provide the path to the <filename>script</filename> file in its XML
        definition, for example:
      </para>
<screen>
&lt;domain ... &gt;
  &lt;sysinfo type="fwcfg"&gt;
    &lt;entry name="opt/org.opensuse.combustion/script" file="/location/to/script"/&gt;
  &lt;/sysinfo&gt;
&lt;/domain&gt;
</screen>
      <tip>
        <title>Using Combustion together with Ignition</title>
        <para>
          Combustion can be used along with Ignition. If you intend to do
          so, label your configuration medium <literal>ignition</literal> and
          include the <filename>ignition</filename> directory with the
          <filename>config.ign</filename> to your directory structure as shown
          below:
        </para>
<screen>
&lt;root directory&gt;
└── combustion
    └── script
    └── other files
└── ignition
    └── config.ign
</screen>
        <para>
          In this scenario, Ignition runs before Combustion.
        </para>
      </tip>
    </section>
  </section>
  <section xml:id="related-configure-combustion">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Examples of Combustion configuration are detailed in
          <xref linkend="reference-combustion-configuration"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          General information about ALP deployment is included in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          For more details about configuring the ALP deployment with
          Ignition, refer to <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Transactional updates are described in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-combustion-configuration"><info><title xmlns:its="http://www.w3.org/2005/11/its">Combustion configuration examples</title></info>
  
  <section xml:id="configuring-combustion-script">
    <title>The <filename>script</filename> configuration file</title>
    <para>
      The <filename>script</filename> configuration file is a set of commands
      that are parsed and executed by Combustion in a <command>transactional-update</command> shell. This
      article provides examples of configuration tasks performed by
      Combustion.
    </para>
    <important>
      <title>Include interpreter declaration</title>
      <para>
        As the <filename>script</filename> file is interpreted by Bash, always
        start the file with the interpreter declaration at its first line:
      </para>
<screen>#!/usr/bin/bash</screen>
    </important>
    <para>
      To log in to your system, include at least the <systemitem class="username">root</systemitem> password.
      However, it is recommended to establish the authentication using SSH
      keys. If you need to use a <systemitem class="username">root</systemitem> password, make sure to configure a
      secure password. If you use a randomly generated password, use at least
      10 characters. If you create your password manually, use even more than
      10 characters and combine uppercase and lowercase letters and numbers.
    </para>
    <section xml:id="sec-script-network">
      <title>Network configuration</title>
      <para>
        To configure and use the network connection during the first boot, add
        the following statement to <filename>script</filename>:
      </para>
<screen># combustion: network</screen>
      <para>
        Using this statement will pass the <literal>rd.neednet=1</literal>
        argument to dracut. If you do not use the statement, the system will be
        configured without any network connection.
      </para>
    </section>
    <section xml:id="combustion-script-partitioning">
      <title>Partitioning</title>
      <para>
        ALP raw images are delivered with a default partitioning scheme
        as described in <xref linkend="alp-deployment-default-partitioning"/>.
        You might want to use a different partitioning. The following set of
        example snippets moves the <filename>/home</filename> to a different
        partition.
      </para>
      <note>
        <title>Performing changes outside of directories included in snapshots</title>
        <para>
          The following script performs changes that are not included in
          snapshots. If the script fails and the snapshot is discarded, some
          changes remain visible and cannot be reverted, for example, the
          changes to the <literal>/dev/vdb</literal> device.
        </para>
      </note>
      <para>
        The following snippet creates a GPT partitioning schema with a single
        partition on the <literal>/dev/vdb</literal> device:
      </para>
<screen>
sfdisk /dev/vdb &lt;&lt;EOF
label: gpt
type=linux
EOF

partition=/dev/vdb1
</screen>
      <para>
        The partition is formatted to BTRFS:
      </para>
<screen>
wipefs --all ${partition}
mkfs.btrfs ${partition}
</screen>
      <para>
        Possible content of <filename>/home</filename> is moved to the new
        <filename>/home</filename> folder location by the following snippet:
      </para>
<screen>
mount /home
mount ${partition} /mnt
rsync -aAXP /home/ /mnt/
umount /home /mnt
</screen>
      <para>
        The snippet below removes an old entry in
        <filename>/etc/fstab</filename> and creates a new entry:
      </para>
<screen>
awk -i inplace '$2 != "/home"' /etc/fstab
echo "$(blkid -o export ${partition} | grep ^UUID=) /home btrfs defaults 0 0" &gt;&gt;/etc/fstab
</screen>
    </section>
    <section xml:id="combustion-script-security">
      <title>Setting a password for <systemitem class="username">root</systemitem></title>
      <para>
        Before you set the <systemitem class="username">root</systemitem> password, generate a hash of the
        password, for example, by using the <command>openssl passwd
        -6</command>. To set the password, add the following to the
        <filename>script</filename>:
      </para>
<screen>echo 'root:$5$.wn2BZHlEJ5R3B1C$TAHEchlU.h2tvfOpOki54NaHpGYKwdNhjaBuSpDotD7' | chpasswd -e</screen>
    </section>
    <section xml:id="combustion-script-sshkeys">
      <title>Adding SSH keys</title>
      <para>
        The following snippet creates a directory to store the <systemitem class="username">root</systemitem>'s SSH
        key and then copies the public SSH key located on the configuration
        device to the <filename>authorized_keys</filename> file.
      </para>
<screen>
mkdir -pm700 /root/.ssh/
cat id_rsa_new.pub &gt;&gt; /root/.ssh/authorized_keys
</screen>
      <note>
        <para>
          The SSH service must be enabled in case you need to use remote login
          via SSH. For details, refer to
          <xref linkend="combustion-script-services"/>.
        </para>
      </note>
    </section>
    <section xml:id="combustion-script-services">
      <title>Enabling services</title>
      <para>
        To enable system services, for example, the SSH service, add the
        following line to <filename>script</filename>:
      </para>
<screen>systemctl enable sshd.service</screen>
    </section>
    <section xml:id="combustion-script-install">
      <title>Installing packages</title>
      <important>
        <title>Network connection and registering your system may be necessary</title>
        <para>
          As some packages may require additional subscription, you may need to
          register your system beforehand. An available network connection may
          also be needed to install additional packages.
        </para>
      </important>
      <para>
        During the first boot configuration, you can install additional
        packages to your system. For example, you can install the
        <literal>vim</literal> editor by adding:
      </para>
<screen>zypper --non-interactive install vim-small</screen>
      <note>
        <para>
          Bear in mind that you will not be able to use
          <command>zypper</command> after the configuration is complete and you
          boot to the configured system. To perform changes later, you must use
          the <command>transactional-update</command> command to create a
          changed snapshot.
        </para>
      </note>
    </section>
  </section>
  <section xml:id="related-combustion-configuration">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          The ALP deployment if generally described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The concept of Combustion is outlined in
          <xref linkend="concept-configure-combustion"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          First boot configuration using Ignition is described in
          <xref linkend="concept-configure-ignition"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-transactional-update"><info><title xmlns:its="http://www.w3.org/2005/11/its">Transactional updates</title></info>
  
  <section xml:id="what-is-transactional-update">
    <title>What are transactional updates?</title>
    <para>
      The Adaptable Linux Platform (ALP) was designed to use a read-only root file system.
      This means that after the deployment is complete, you are not able to
      perform direct modifications to the root file system, for example, by
      using the <command>zypper</command> command. Instead, ALP
      introduces the concept of transactional updates which enables you to
      modify your system and keep it up to date.
    </para>
  </section>
  <section xml:id="how-it-works-transactional-update">
    <title>How do transactional updates work?</title>
    <para>
      Each time you call the <command>transactional-update</command> command to change your system—either
      to install a package, perform an update or apply a patch—the
      following actions take place:
    </para>
    <procedure>
      <title>Modifying the root file system</title>
      <step>
        <para>
          A new read-write snapshot is created from your current root file
          system, or from a snapshot that you specified.
        </para>
      </step>
      <step>
        <para>
          All changes are applied (updates, patches or package installation).
        </para>
      </step>
      <step>
        <para>
          The snapshot is switched back to read-only mode.
        </para>
      </step>
      <step>
        <para>
          The new root file system snapshot is prepared, so that it will be
          active after you reboot.
        </para>
      </step>
      <step>
        <para>
          After rebooting, the new root file system is set as the default
          snapshot.
        </para>
        <note>
          <para>
            Bear in mind that without rebooting your system, the changes will
            not be applied.
          </para>
        </note>
      </step>
    </procedure>
    <warning>
      <para>
        If you do not reboot your machine before performing further changes,
        the <command>transactional-update</command> command will create a new snapshot from the current root
        file system. This means that you will end up with several parallel
        snapshots, each including that particular change but not changes from
        the other invocations of the command. After reboot, the most recently
        created snapshot will be used as your new root file system, and it will
        not include changes done in the previous snapshots.
      </para>
    </warning>
  </section>
  <section xml:id="how-it-works-transactional-update-repositories">
    <title>Software repositories</title>
    <para>
      The current ALP image points to the following two software
      repositories:
    </para>
    <variablelist>
      <varlistentry>
        <term>ALP</term>
        <listitem>
          <para>
            <literal>https://download.opensuse.org/repositories/SUSE:/ALP:/PUBLISH/images/repo/ALP-0.1-x86_64-Media1/</literal>
          </para>
          <para>
            This repository is enabled. It is a subset of the build repository
            and an equivalent of the <literal>POOL</literal> repository known
            from other SUSE software products. It will remain unchanged until
            the release of the next ALP prototype.
          </para>
          <tip>
            <para>
              If you need a package which is not included in the
              <literal>ALP</literal> repository, you may find it in the
              <literal>ALP-Build</literal> repository. To enable it, run:
            </para>
<screen><prompt role="root"># </prompt>zypper mr -e ALP-Build</screen>
          </tip>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>ALP-Build</term>
        <listitem>
          <para>
            <literal>https://download.opensuse.org/repositories/SUSE:/ALP/standard/</literal>
          </para>
          <para>
            This repository is disabled by default. It is used for building the
            project. It includes all packages built in the
            <literal>SUSE:ALP</literal> project in the build service and will
            be moving forward over the time with future development.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="benefits-transactional-update">
    <title>Benefits of transactional updates</title>
    <itemizedlist>
      <listitem>
        <para>
          They are atomic—the update is applied only if it completes
          successfully.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes are applied in a separate snapshot and so do not influence
          the running system.
        </para>
      </listitem>
      <listitem>
        <para>
          Changes can easily be rolled back.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-transactional-update">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Usage of the <command>transactional-update</command> command is detailed in
          <xref linkend="reference-transactional-update-usage"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-transactional-update-usage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>transactional-update</command> command</title></info>
  
  <section xml:id="sec-command-list">
    <title><command>transactional-update</command> usage</title>
    <para>
      The <command>transactional-update</command> command enables the atomic installation or removal of
      updates. Updates are applied only if all of them can be successfully
      installed. <command>transactional-update</command> creates a snapshot of your system and uses it to
      update the system. Later you can restore this snapshot. All changes
      become active only after reboot.
    </para>
    <para>
      The <command>transactional-update</command> command syntax is as follows:
    </para>
<screen>
transactional-update <option>[option]</option> <replaceable>[general_command]</replaceable> <replaceable>[package_command]</replaceable> <replaceable>standalone_command</replaceable>
</screen>
    <note>
      <title>Running <command>transactional-update</command> without arguments</title>
      <para>
        If you do not specify any command or option while running the <command>transactional-update</command>
        command, the system updates itself.
      </para>
    </note>
    <para>
      Possible command parameters are described further.
    </para>
    <variablelist>
      <title><command>transactional-update</command> options</title>
      <varlistentry>
        <term><option>--interactive, -i</option></term>
        <listitem>
          <para>
            Can be used along with a package command to turn on interactive
            mode.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--non-interactive, -n</option></term>
        <listitem>
          <para>
            Can be used along with a package command to turn on non-interactive
            mode.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--continue [<replaceable>number</replaceable>], -c</option></term>
        <listitem>
          <para>
            The <option>--continue</option> option is for making multiple
            changes to an existing snapshot without rebooting.
          </para>
          <para>
            The default <command>transactional-update</command> behavior is to create a new snapshot from the
            current root file system. If you forget something, such as
            installing a new package, you have to reboot to apply your previous
            changes, run <command>transactional-update</command> again to install the forgotten package, and
            reboot again. You cannot run the <command>transactional-update</command> command multiple times
            without rebooting to add more changes to the snapshot, because this
            will create separate independent snapshots that do not include
            changes from the previous snapshots.
          </para>
          <para>
            Use the <option>--continue</option> option to make as many changes
            as you want without rebooting. A separate snapshot is made each
            time, and each snapshot contains all the changes you made in the
            previous snapshots, plus your new changes. Repeat this process as
            many times as you want, and when the final snapshot includes
            everything you want, reboot the system, and your final snapshot
            becomes the new root file system.
          </para>
          <para>
            Another useful feature of the <option>--continue</option> option is
            that you may select any existing snapshot as the base for your new
            snapshot. The following example demonstrates running <command>transactional-update</command> to
            install a new package in a snapshot based on snapshot 13, and then
            running it again to install another package:
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_1</replaceable></command></screen>
<screen><prompt role="root"># </prompt><command>transactional-update --continue 13 pkg install <replaceable>package_2</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--no-selfupdate</option></term>
        <listitem>
          <para>
            Disables self-updating of <command>transactional-update</command>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--drop-if-no-change, -d</option></term>
        <listitem>
          <para>
            Discards the snapshot created by <command>transactional-update</command> if there were no changes
            to the root file system. If there are some changes to the
            <filename>/etc</filename> directory, those changes merged back to
            the current file system.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--quiet</option></term>
        <listitem>
          <para>
            The <command>transactional-update</command> command will not output to <literal>stdout</literal>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--help, -h</option></term>
        <listitem>
          <para>
            Prints help for the <command>transactional-update</command> command.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><option>--version</option></term>
        <listitem>
          <para>
            Displays the version of the <command>transactional-update</command> command.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The general commands are the following:
    </para>
    <variablelist>
      <title>General commands</title>
      <varlistentry>
        <term><command>cleanup-snapshots</command></term>
        <listitem>
          <para>
            The command marks all unused snapshots that are intended to be
            removed.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>cleanup-overlays</command></term>
        <listitem>
          <para>
            The command removes all unused overlay layers of
            <filename>/etc</filename>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>cleanup</command></term>
        <listitem>
          <para>
            The command combines the <command>cleanup-snapshots</command> and
            <command>cleanup-overlays</command> commands.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>grub.cfg</command></term>
        <listitem>
          <para>
            Use this command to rebuild the GRUB boot loader configuration
            file.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>bootloader</command></term>
        <listitem>
          <para>
            The command reinstalls the boot loader.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>initrd</command></term>
        <listitem>
          <para>
            Use the command to rebuild <literal>initrd</literal>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>kdump</command></term>
        <listitem>
          <para>
            In case you perform changes to your hardware or storage, you may
            need to rebuild the kdump initrd.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>shell</command></term>
        <listitem>
          <para>
            Opens a read-write shell in the new snapshot before exiting. The
            command is typically used for debugging purposes.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>reboot</command></term>
        <listitem>
          <para>
            The system reboots after the <command>transactional-update</command> command is complete.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>run <replaceable>&lt;command&gt;</replaceable></command></term>
        <listitem>
          <para>
            Runs the provided command in a new snapshot.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>setup-selinux</command></term>
        <listitem>
          <para>
            Installs and enables targeted SELinux policy.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The package commands are the following:
    </para>
    <variablelist>
      <title>Package commands</title>
      <varlistentry>
        <term><command>dup</command></term>
        <listitem>
          <para>
            Performs upgrade of your system. The default option for this
            command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>migration</command></term>
        <listitem>
          <para>
            The command migrates your system to a selected target. Typically,
            it is used to upgrade your system if it has been registered via
            SUSE Customer Center.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>patch</command></term>
        <listitem>
          <para>
            Checks for available patches and installs them. The default option
            for this command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg install</command></term>
        <listitem>
          <para>
            Installs individual packages from the available channels using the
            <command>zypper install</command> command. This command can also be
            used to install Program Temporary Fix (PTF) RPM files. The default
            option for this command is <option>--interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>package_name</replaceable></command></screen>
          <para>
            or
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg install <replaceable>rpm1 rpm2</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg remove</command></term>
        <listitem>
          <para>
            Removes individual packages from the active snapshot using the
            <command>zypper remove</command> command. This command can also be
            used to remove PTF RPM files. The default option for this command
            is <option>--interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg remove <replaceable>package_name</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>pkg update</command></term>
        <listitem>
          <para>
            Updates individual packages from the active snapshot using the
            <command>zypper update</command> command. Only packages that are
            part of the snapshot of the base file system can be updated. The
            default option for this command is <option>--interactive</option>.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update pkg update <replaceable>package_name</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>register</command></term>
        <listitem>
          <para>
            Registers or deregisters your system. For a complete usage
            description, refer to <xref linkend="sec-register-command"/>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>up</command></term>
        <listitem>
          <para>
            Updates installed packages to newer versions. The default option
            for this command is <option>--non-interactive</option>.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      The standalone commands are the following:
    </para>
    <variablelist>
      <title>Standalone commands</title>
      <varlistentry>
        <term><command>rollback <replaceable>&lt;snapshot number&gt;</replaceable></command></term>
        <listitem>
          <para>
            This sets the default subvolume. The current system is set as the
            new default root file system. If you specify a number, that
            snapshot is used as the default root file system. On a read-only
            file system, it does not create any additional snapshots.
          </para>
<screen><prompt role="root"># </prompt><command>transactional-update rollback <replaceable>snapshot_number</replaceable></command></screen>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>rollback last</command></term>
        <listitem>
          <para>
            This command sets the last known to be working snapshot as the
            default.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><command>status</command></term>
        <listitem>
          <para>
            This prints a list of available snapshots. The currently booted one
            is marked with an asterisk, the default snapshot is marked with a
            plus sign.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <section xml:id="sec-register-command">
      <title>The <command>register</command> command</title>
      <para>
        The <command>register</command> command enables you to handle all tasks
        regarding registration and subscription management. You can supply the
        following options:
      </para>
      <variablelist>
        <varlistentry>
          <term><option>--list-extensions</option></term>
          <listitem>
            <para>
              With this option, the command will list available extensions for
              your system. You can use the output to find a product identifier
              for product activation.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-p, --product</option></term>
          <listitem>
            <para>
              Use this option to specify a product for activation. The product
              identifier has the following format:
              <emphasis>&lt;name&gt;/&lt;version&gt;/&lt;architecture&gt;</emphasis>,
              for example,
              <literal>sle-module-live-patching/15.3/x86_64</literal>. The
              appropriate command will then be the following:
            </para>
<screen><prompt role="root"># </prompt>transactional-update register -p sle-module-live-patching/15.3/x86_64</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-r, --regcode</option></term>
          <listitem>
            <para>
              Register your system with the registration code provided. The
              command will register the subscription and enable software
              repositories.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-d, --de-register</option></term>
          <listitem>
            <para>
              The option deregisters the system, or when used along with the
              <literal>-p</literal> option, deregisters an extension.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-e, --email</option></term>
          <listitem>
            <para>
              Specify an email address that will be used in SUSE Customer Center for
              registration.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--url</option></term>
          <listitem>
            <para>
              Specify the URL of your registration server. The URL is stored in
              the configuration and will be used in subsequent command
              invocations. For example:
            </para>
<screen><prompt role="root"># </prompt>transactional-update register --url https://scc.suse.com</screen>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>-s, --status</option></term>
          <listitem>
            <para>
              Displays the current registration status in JSON format.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--write-config</option></term>
          <listitem>
            <para>
              Writes the provided options value to the
              <filename>/etc/SUSEConnect</filename> configuration file.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--cleanup</option></term>
          <listitem>
            <para>
              Removes old system credentials.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--version</option></term>
          <listitem>
            <para>
              Prints the version.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><option>--help</option></term>
          <listitem>
            <para>
              Displays the usage of the command.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
  </section>
  <section xml:id="related-transactional-update-usage">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          General description of transactional updates is described in
          <xref linkend="concept-transactional-update"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></chapter><chapter xml:lang="en" role="concept" version="5.1" xml:id="concept-containers-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Containers and Podman</title></info>
  
  <!-- highly inspired by https://susedoc.github.io/doc-sle/main/html/SLE-Micro-podman/article-podman.html -->
  <section xml:id="what-is-containers-podman">
    <title>What are containers and Podman?</title>
    <para>
      Containers offer a lightweight virtualization method to run multiple
      virtual environments (containers) simultaneously on a single host. Unlike
      technologies such as Xen or KVM, where the processor simulates a
      complete hardware environment and a hypervisor controls virtual machines,
      containers provide virtualization on the operating system level, where
      the kernel controls the isolated containers.
    </para>
    <para>
      <emphasis>Podman</emphasis> is a short name for Pod Manager Tool. It is
      a daemonless container engine that enables you to run and deploy
      applications using containers and container images. Podman provides a
      command line interface to manage containers.
    </para>
  </section>
  <section xml:id="how-it-works-podman">
    <title>How does Podman work?</title>
    <para>
      Podman provides integration with <systemitem class="daemon">systemd</systemitem>. This way you can control
      containers via <systemitem class="daemon">systemd</systemitem> units. You can create these units for existing
      containers as well as generate units that can start containers if they do
      not exist in the system. Moreover, Podman can run <systemitem class="daemon">systemd</systemitem> inside
      containers.
    </para>
    <para>
      Podman enables you to organize your containers into pods. Pods share
      the same network interface and resources. A typical use case for
      organizing a group of containers into a pod is a container that runs a
      database and a container with a client that accesses the database.
    </para>
    <section xml:id="pod-architecture">
      <title>Pods architecture</title>
      <para>
        A pod is a group of containers that share the same namespace, ports and
        network connection. Usually, containers within one pod can communicate
        directly with each other. Each pod contains an infrastructure container
        (<literal>INFRA</literal>), whose purpose is to hold the namespace.
        <literal>INFRA</literal> also enables Podman to add other containers
        to the pod. Port bindings, cgroup-parent values, and kernel namespaces
        are all assigned to the infrastructure container. Therefore, later
        changes of these values are not possible.
      </para>
      <figure xml:id="fig-pod-architecture">
        <title>Pods architecture</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="pods_architecture.svg" width="100%"/>
          </imageobject>
          <textobject role="description"><phrase>Pods architecture</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>
        Each container in a pod has its own instance of a monitoring program.
        The monitoring program watches the container's process and if the
        container dies, the monitoring program saves its exit code. The program
        also holds open the tty interface for the particular container. The
        monitoring program enables you to run containers in the detached mode
        when Podman exits, because this program continues to run and enables
        you to attach tty later.
      </para>
    </section>
  </section>
  <section xml:id="benefits-containers-podman">
    <title>Benefits of containers</title>
    <itemizedlist mark="bullet" spacing="normal">
      <listitem>
        <para>
          Containers make it possible to isolate applications in self-contained
          units.
        </para>
      </listitem>
      <listitem>
        <para>
          Containers provide near-native performance. Depending on the runtime,
          a container can use the host kernel directly, thus minimizing
          overhead.
        </para>
      </listitem>
      <listitem>
        <para>
          It is possible to control network interfaces and apply resources
          inside containers through kernel control groups.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-containers-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Usage of the <command>podman</command> command is detailed in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          List of available workloads with links to their installation using
          Podman is included in
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-enable-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Enabling Podman</title></info>
  
  <section xml:id="introduction-enable-podman">
    <title>Introduction</title>
    <para>
      This article helps you verify that Podman is installed on the
      ALP system and provides guidelines to enable its <systemitem class="daemon">systemd</systemitem> service
      when Cockpit requires it.
    </para>
  </section>
  <section xml:id="requirements-enable-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="install-podman">
    <title>Installing Podman</title>
    <procedure>
      <step>
        <para>
          Verify that Podman is installed on your system by running the
          following command:
        </para>
<screen><prompt role="root"># </prompt>zypper se -i podman</screen>
      </step>
      <step>
        <para>
          If Podman is not listed in the output, install it by running:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install podman*</screen>
      </step>
      <step>
        <para>
          Reboot the ALP host for the changes to take effect.
        </para>
      </step>
      <step>
        <para>
          Optionally, enable and start the
          <systemitem class="daemon">podman.service</systemitem> service for
          applications that require it, such as Cockpit. You can enable it
          either in Cockpit by clicking <menuchoice><guimenu>Podman
          containers</guimenu><guimenu>Start podman</guimenu></menuchoice>, or
          by running the following command:
        </para>
<screen><prompt role="root"># </prompt>systemctl enable --now podman.service</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="install-podman-rootless">
    <title>Enabling rootless mode</title>
    <para>
      By default, Podman requires <systemitem class="username">root</systemitem> privileges. To enable rootless
      mode for the current user, run the following command:
    </para>
<screen>
<prompt>&gt; </prompt>sudo usermod --add-subuids 100000-165535 \
  --add-subgids 100000-165535 <replaceable>USER</replaceable>
</screen>
    <para>
      Reboot the machine to enable the change. The command above defines a
      range of local UIDs to which the UIDs allocated to users inside the
      container are mapped on the host. Note that the ranges defined for
      different users must not overlap. It is also important that the ranges do
      not reuse the UID of an existing local user or group. By default, adding
      a user with the <command>useradd</command> command automatically
      allocates subUID and subGID ranges.
    </para>
    <note>
      <title>Limitations of rootless containers</title>
      <para>
        Running a container with Podman in rootless mode on SLE Micro may fail,
        because the container might need access to directories or files that
        require <systemitem class="username">root</systemitem> privileges.
      </para>
    </note>
  </section>
  <section xml:id="next-enable-podman">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Run containerized workloads. For details, refer to
          <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-enable-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Containers and Podman are outlined in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Usage of the <command>podman</command> command is listed in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Available workloads and links to their installation using Podman
          are listed in <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="reference" version="5.1" xml:id="reference-podman-usage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Podman usage</title></info>
  
    <para>
      This article introduces basic Podman usage that you may need when
      running containerized workloads.
    </para>
  <section xml:id="sec-getting-images">
    <title>Getting container images</title>
    <para>
      To run a container, you need an image. An image includes all dependencies
      needed to run an application. You can obtain images from an image
      registry. Available registries are defined in the
      <filename>/etc/containers/registries.conf</filename> configuration file.
      If you have a local image registry or want to use other registries, add
      the registries into the configuration file.
    </para>
    <important>
      <title>No tools for building images in ALP</title>
      <para>
        ALP does not provide tools for building custom images.
        Therefore, the only way to get an image is to pull it from an image
        registry.
      </para>
    </important>
    <para>
      The <command>podman pull</command> command pulls an image from an image
      registry. The syntax is as follows:
    </para>
<screen><prompt role="root"># </prompt>podman pull <replaceable>[OPTIONS]</replaceable> <replaceable>SOURCE</replaceable></screen>
    <para>
      The <replaceable>source</replaceable> can be an image without the
      registry name. In that case, Podman tries to pull the image from all
      registries configured in the
      <filename>/etc/containers/registries.conf</filename> file. The default
      image tag is <literal>latest</literal>. The default location of pulled
      images is
      <filename>/var/lib/containers/storage/overlay-images/</filename>.
    </para>
    <para>
      To view all possible options of the <command>podman pull</command>
      command, run:
    </para>
<screen><prompt role="root"># </prompt>podman pull --help</screen>
    <note>
      <title>Getting images using Cockpit</title>
      <para>
        If you are using Cockpit, you can also pull images from an image
        registry in the <guimenu>Podman containers</guimenu> menu by clicking
        <guimenu>+ Get new image</guimenu>.
      </para>
    </note>
    <para>
      Podman enables you to search for images in an image registry or a list
      of registries using the command:
    </para>
<screen><prompt role="root"># </prompt>podman search <replaceable>IMAGE_NAME</replaceable></screen>
  </section>
  <section xml:id="sec-working-containers">
    <title>Working with containers</title>
    <para>
      The following section covers common container management tasks. This
      includes creating, starting, and modifying containers.
    </para>
    <warning>
      <para>
        The current version of ALP does not provide tools for building
        custom images. Therefore, the only way to get a container image is to
        pull it from an image registry.
      </para>
    </warning>
    <section xml:id="sec-podman-run">
      <title>Running containers</title>
      <tip>
        <para>
          For specific details on running ALP containers, refer to links
          in the <xref linkend="reference-available-alp-workloads"/> article.
        </para>
      </tip>
      <para>
        After you have pulled your container image, you can create containers
        based on it. You can run an instance of the image using the
        <command>podman run</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman run [<replaceable>OPTIONS</replaceable>] <replaceable>IMAGE</replaceable> [<replaceable>CONTAINER_NAME</replaceable>]</screen>
      <para>
        <replaceable>IMAGE</replaceable> is specified in format
        <emphasis>transport:path</emphasis>. If <emphasis>transport</emphasis>
        is omitted, the default <literal>docker</literal> is used. The
        <emphasis>path</emphasis> can reference to a specific image registry.
        If omitted, Podman searches for the image in registries defined in
        the <filename>/etc/containers/registries.conf</filename> file. An
        example that runs a container called <literal>sles15</literal> based on
        the <literal>sle15</literal> image follows:
      </para>
<screen><prompt role="root"># </prompt>podman run registry.opensuse.org/suse/templates/images/sle-15-sp3/base/images/suse/sle15 sles15</screen>
      <para>
        Below is a list of frequently used options. For a complete list of
        available options, run the command: <command>podman run
        --help</command>.
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--detach, -d</literal></term>
          <listitem>
            <para>
              The container will run in the background.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--env, -e=env</literal></term>
          <listitem>
            <para>
              This option allows arbitrary environment variables that are
              available for the process to be launched inside of the container.
              If an environment variable is specified without a value, Podman
              will check the host environment for a value and set the variable
              only if it is set on the host.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--help</literal></term>
          <listitem>
            <para>
              Prints help for the <command>podman run</command> command.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--hostname=</literal><emphasis>name</emphasis>,<literal> -h</literal></term>
          <listitem>
            <para>
              Sets the container host name that is available inside the
              container.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--pod=</literal><emphasis>name</emphasis></term>
          <listitem>
            <para>
              Runs the container in an existing pod. To create a pod, prefix
              the pod name with <literal>new:</literal>.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--read-only</literal></term>
          <listitem>
            <para>
              Mounts the container’s root file system as read-only.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--systemd=true|false|always</literal></term>
          <listitem>
            <para>
              Runs the container in systemd mode. The default is true.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>
    <section xml:id="sec-podman-stop">
      <title>Stopping containers</title>
      <para>
        If the <command>podman run</command> command finished successfully, a
        new container has been started. You can stop the container by running:
      </para>
<screen><prompt role="root"># </prompt>podman stop <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        You can specify a single container name or ID or a space-separated list
        of containers. The command takes the following options:
      </para>
      <variablelist>
        <varlistentry>
          <term><literal>--all, -a</literal></term>
          <listitem>
            <para>
              Stops all running containers.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--latest, -l</literal></term>
          <listitem>
            <para>
              Instead of providing a container name, the last created container
              will be stopped.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term><literal>--time, -t=</literal><emphasis>seconds</emphasis></term>
          <listitem>
            <para>
              Seconds to wait before forcibly stopping the container.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        To view all possible options of the <command>podman stop</command>
        command, run the following:
      </para>
<screen><prompt role="root"># </prompt>podman stop --help</screen>
    </section>
    <section xml:id="sec-podman-start">
      <title>Starting containers</title>
      <para>
        To start already created but stopped containers, use the
        <command>podman start</command> command. The command syntax is as
        follows:
      </para>
<screen><prompt role="root"># </prompt>podman start <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        For a complete list of possible options of <command>podman
        start</command>, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman start --help</screen>
    </section>
    <section xml:id="podman-update-containers">
      <title>Updating containers</title>
      <para>
        To update an existing container, follow these steps:
      </para>
      <procedure>
        <step>
          <para>
            Identify the image of the container that you want to update, for
            example, <literal>yast-mgmt-qt</literal>:
          </para>
<screen>
<prompt>&gt; </prompt>podman image ls
REPOSITORY                                                                                                  TAG         IMAGE ID      CREATED      SIZE
[...]
registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt  latest      f349194a439d  13 days ago  674 MB
</screen>
        </step>
        <step>
          <para>
            Pull the image from the registry to find out if there is a newer
            version. If you do not specify a version tag, the
            <literal>latest</literal> tag is used:
          </para>
<screen>
<prompt role="root"># </prompt>podman pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
Trying to pull registry.opensuse.org/suse/alp/workloads/publish/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest...
Getting image source signatures
Copying blob 6bfbcdeee2ec done
[...]
Writing manifest to image destination
Storing signatures
f349194a439da249587fbd8baffc5659b390aa14c8db1d597e95be703490ffb1
</screen>
        </step>
        <step>
          <para>
            If the container is running, identify its ID and stop it:
          </para>
<screen>
<prompt role="root"># </prompt>podman ps
CONTAINER ID  IMAGE                                                                             COMMAND     CREATED         STATUS
[...]
28fef404417b /workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest               2 weeks ago     Up 24 seconds ago
<prompt role="root"># </prompt>podman stop 28fef404417b
</screen>
        </step>
        <step>
          <para>
            Run the container following specific instructions at
            <xref linkend="reference-available-alp-workloads"/>, for
            example:
          </para>
<screen><prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest</screen>
        </step>
      </procedure>
    </section>
    <section xml:id="sec-podman-commit">
      <title>Committing modified containers</title>
      <para>
        You can run a new container with specific attributes that are not part
        of the original image. To save the container with these attributes as a
        new image, you can use the <command>podman commit</command> command:
      </para>
<screen><prompt role="root"># </prompt>podman commit <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> is a container name or a container
        ID. <replaceable>IMAGE</replaceable> is the new image name. If the
        image name does not start with a registry name, the value
        <literal>localhost</literal> is used.
      </para>
      <para>
        When using Cockpit, you can perform the <command>commit</command>
        operation directly from a container's <guimenu>Details</guimenu>, by
        clicking <guimenu>Commit</guimenu>. A dialog box opens. Specify all
        required details as shown below and click <guimenu>Commit</guimenu>:
      </para>
      <figure>
        <title>Committing a container in Cockpit</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="cockpit_commit_container.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
    <section xml:id="sec-podman-ps">
      <title>Listing containers</title>
      <para>
        Podman enables you to list all running containers using the
        <command>podman ps</command> command. The generic syntax of the command
        is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman  ps <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        Command options can change the displayed information. For example,
        using the <literal>--all</literal> option will output all containers
        created by Podman (not only the running containers).
      </para>
      <para>
        For a complete list of <command>podman ps</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman ps --help</screen>
    </section>
    <section xml:id="sec-podman-rm">
      <title>Removing containers</title>
      <para>
        To remove one or more unused containers from the host, use the
        <command>podman rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman rm <replaceable>[OPTIONS]</replaceable> <replaceable>CONTAINER</replaceable></screen>
      <para>
        <replaceable>CONTAINER</replaceable> can be a container name or a
        container ID.
      </para>
      <para>
        The command does not remove the specified container if the container is
        running. To remove a running container, use the <literal>-f</literal>
        option.
      </para>
      <para>
        For a complete list of <command>podman rm</command> options, run:
      </para>
<screen><prompt role="root"># </prompt>podman rm --help</screen>
      <note>
        <title>Deleting all stopped containers</title>
        <para>
          You can delete all stopped containers from your host with a single
          command:
        </para>
<screen><prompt role="root"># </prompt>podman container prune</screen>
        <para>
          Make sure that each stopped container is intended to be removed
          before you run the command, otherwise you might remove containers
          that are still in use and were stopped only temporarily.
        </para>
      </note>
    </section>
  </section>
  <section xml:id="sec-working-pods">
    <title>Working with pods</title>
    <para>
      Containers can be grouped into a pod. The containers in the pod then
      share network, pid, and IPC namespace. Pods can be managed by
      <command>podman pod</command> commands. This section provides an overview
      of the commands for managing pods.
    </para>
    <section xml:id="sec-creating-pods">
      <title>Creating pods</title>
      <para>
        The command <command>podman pod create</command> is used to create a
        pod. The syntax of the command is as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod create <replaceable>[OPTIONS]</replaceable></screen>
      <para>
        The command outputs the pod ID. By default, the pods are created
        without being started. You can start a pod by running a container in
        the pod, or by starting the pod as described in
        <xref linkend="sec-starting-pods"/>.
      </para>
      <note>
        <title>Default pod names</title>
        <para>
          If you do not specify a pod name with the <literal>--name</literal>
          option, Podman will assign a default name for the pod.
        </para>
      </note>
      <para>
        For a complete list of possible options, run the following command:
      </para>
<screen><prompt role="root"># </prompt>podman pod create --help</screen>
    </section>
    <section xml:id="sec-listing-pods">
      <title>Listing pods</title>
      <para>
        You can list all pods by running the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod list</screen>
      <para>
        The output looks as follows:
      </para>
<screen>
POD ID        NAME               STATUS   CREATED       # OF CONTAINERS  INFRA ID
30fba506fecb  upbeat_mcclintock  Created  19 hours ago  1                4324f40c9651
976a83b4d88b  nervous_feynman    Running  19 hours ago  2                daa5732ecd02
</screen>
      <para>
        As each pod includes the <literal>INFRA</literal> container, the number
        of containers in a pod is always larger than zero.
      </para>
    </section>
    <section xml:id="sec-starting-pods">
      <title>Starting/stopping/restarting pods</title>
      <para>
        After a pod is created, you must start it, as it is not in the state
        <literal>running</literal> by default. In the commands below,
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To start a pod, run the command:
      </para>
<screen><prompt role="root"># </prompt>podman pod start <replaceable>[OPTIONS]</replaceable> <replaceable>POD</replaceable></screen>
      <para>
        For a complete list of possible options, run:
      </para>
<screen><prompt role="root"># </prompt>podman pod start --help</screen>
      <para>
        To stop a pod, use the <command>podman pod stop</command> as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod stop <replaceable>POD</replaceable></screen>
      <para>
        To restart a pod, use the <command>podman pod restart</command> command
        as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod restart <replaceable>POD</replaceable></screen>
    </section>
    <section xml:id="sec-adding-pods">
      <title>Managing containers in a pod</title>
      <para>
        To add a new container to a pod, use the <command>podman run</command>
        command with the option <literal>--pod</literal>. A general syntax of
        the command follows:
      </para>
<screen><prompt role="root"># </prompt>podman run <replaceable>[OPTIONS]</replaceable> --pod <replaceable>POD_NAME</replaceable> <replaceable>IMAGE</replaceable></screen>
      <para>
        For details about the <command>podman run</command> command, refer to
        <xref linkend="sec-podman-run"/>.
      </para>
      <note>
        <title>Only new containers can be added to a pod</title>
        <para>
          The <command>podman start</command> command does not allow for
          starting a container in a pod if the container was not added to the
          pod during the container's initial running.
        </para>
      </note>
      <para>
        You cannot remove a container from a pod and keep the container
        running, because the container itself is removed from the host.
      </para>
      <para>
        Other actions like start, restart and stop can be performed on specific
        containers without affecting the status of the pod.
      </para>
    </section>
    <section xml:id="sec-removing-pods">
      <title>Removing pods</title>
      <para>
        There are two ways to remove pods. You can use the <command>podman pod
        rm</command> command to remove one or more pods. Alternatively, you can
        remove all stopped pods using the <command>podman pod prune</command>
        command.
      </para>
      <para>
        To remove a pod or several pods, run the <command>podman pod
        rm</command> command as follows:
      </para>
<screen><prompt role="root"># </prompt>podman pod rm <replaceable>POD</replaceable></screen>
      <para>
        <replaceable>POD</replaceable> can be a pod name or a pod ID.
      </para>
      <para>
        To remove all currently stopped pods, use the <command>podman pod
        prune</command> command. Make sure that all stopped pods are intended
        to be removed before you run the <command>podman pod prune</command>
        command, otherwise you might remove pods that are still in use.
      </para>
    </section>
    <section xml:id="sec-monitoring-pods">
      <title>Monitoring processes in pods</title>
      <para>
        To view all containers in all pods, use the following command:
      </para>
<screen><prompt role="root"># </prompt>podman ps -a --pod</screen>
      <para>
        The output of the command will be similar to the following one:
      </para>
      <!-- Decreased text size and removed a few columns to make this fit on the PDF. -->
<screen>
<?dbsuse-fo font-size="0.70em"?>
CONTAINER ID  IMAGE                       COMMAND    CREATED       STATUS                 [...]
4324f40c9651  k8s.gcr.io/pause:3.2                   21 hours ago  Created
daa5732ecd02  k8s.gcr.io/pause:3.2                   22 hours ago  Up 3 hours ago
e5c8e360c54b  localhost/test:latest       /bin/bash  3 days ago    Exited (137) 3 days ago
82dad15828f7  localhost/opensuse/toolbox  /bin/bash  3 days ago    Exited (137) 3 days ago
1a23da456b6f  docker.io/i386/ubuntu       /bin/bash  4 days ago    Exited (0) 6 hours ago
df890193f651  localhost/opensuse/toolbox  /bin/bash  4 days ago    Created
  </screen>
      <para>
        The first two records are the <literal>INFRA</literal> containers of
        each pod, based on the <literal>k8s.gcr.io/pause:3.2</literal> image.
        Other containers in the output are stand-alone containers that do not
        belong to any pod.
      </para>
    </section>
    <section xml:id="related-podman-usage">
      <title>Related topics</title>
      <itemizedlist>
        <listitem>
          <para>
            Enabling Podman is described in
            <xref linkend="task-enable-podman"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            Containers and Podman are outlined in
            <xref linkend="concept-containers-podman"/>.
          </para>
        </listitem>
        <listitem>
          <para>
            Available workloads and links to their installation are listed in
            <xref linkend="reference-available-alp-workloads"/>.
          </para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
</section></chapter><chapter xml:lang="en" role="reference" version="5.1" xml:id="reference-available-alp-workloads"><info><title xmlns:its="http://www.w3.org/2005/11/its">Workloads</title></info>
  
  <section xml:id="introduction-available-alp-workloads">
    <title>Introduction</title>
    <para>
     The Adaptable Linux Platform (ALP) runs containerized workloads instead of traditional
     applications. Images of these containers are stored in image registries
     online. ALP can run any containerized workload that is supported by
     the default container manager Podman. This article lists and describes
     workloads securely distributed and supported by SUSE. You can find the
     source files of the workloads at
     <link xlink:href="https://build.opensuse.org/project/show/SUSE:ALP:Workloads"/>.
    </para>
  </section>
  <section xml:id="alp-workload-yast">
    <title>YaST</title>
    <para>
      The following YaST container images are available:
    </para>
    <variablelist>
      <varlistentry>
        <term>yast-mgmt-ncurses</term>
        <listitem>
          <para>
            The base YaST workload. It contains the text version of YaST
            (ncurses).
          </para>
          <para>
            For more details, refer to
            <xref linkend="task-run-yast-with-podman"/>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-qt</term>
        <listitem>
          <para>
            This workload adds the Qt-based graphical user interface.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>yast-mgmt-web</term>
        <listitem>
          <para>
            This workload exposes the standard graphical interface via a VNC
            server and uses a JavaScript VNC client to render the screen in a
            Web browser.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="alp-workload-kvm">
    <title>KVM</title>
    <para>
      This workload adds virtualization capability to ALP so that you
      can use it as a VM Host Server. It uses the KVM hypervisor supported by the
      <systemitem class="library">libvirt</systemitem> toolkit.
    </para>
    <para>
      For more details, refer to <xref linkend="task-run-kvm-with-podman"/>.
    </para>
  </section>
  <section xml:id="alp-workload-cockpit">
    <title>Cockpit Web server</title>
    <para>
      This workload adds the Cockpit Web server to Adaptable Linux Platform so that you can
      administer the system and container via a user-friendly interface in your
      Web browser.
    </para>
    <para>
      For more details, refer to
      <xref linkend="task-run-cockpit-with-podman"/>.
    </para>
  </section>
  <section xml:id="related-run-workloads-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          <systemitem class="library">libvirt</systemitem> virtualization is described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/part-virt-libvirt.html"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          The general concept of Podman is described in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="task" version="5.1" xml:id="task-run-yast-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the YaST workload using Podman</title></info>
  
  <section xml:id="introduction-run-yast-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to start the YaST workload on the Adaptable Linux Platform
      (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-yast-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-yast-text-mode">
    <title>Starting YaST in text mode</title>
    <para>
      To start the text version (ncurses) of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-ncurses
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses
[...]
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-ncurses:latest
</screen>
        <figure>
          <title>YaST running in text mode on ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp_yast_ncurses.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>YaST running in text mode on ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="alp-starting-yast-qt">
    <title>Starting graphical YaST</title>
    <para>
      To start the graphical Qt version of YaST as a workload, follow these
      steps:
    </para>
    <procedure>
      <step>
        <para>
          To view the graphical YaST on your local X server, you need to use
          SSH X forwarding. It requires the <package>xauth</package> package
          installed, applied by the host reboot:
        </para>
<screen><prompt role="root"># </prompt>transactional-update pkg install xauth &amp;&amp; reboot</screen>
      </step>
      <step>
        <para>
          Connect to the ALP host using <command>ssh</command> with the
          X forwarding enabled:
        </para>
<screen><prompt>&gt; </prompt>ssh -X <replaceable>ALP_HOST</replaceable></screen>
      </step>
      <step>
        <para>
          Identify the full URL address in a registry of container images, for
          example:
        </para>
<screen>
<prompt>&gt; </prompt>podman search yast-mgmt-qt
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt
[...]
</screen>
      </step>
      <step>
        <para>
          To start the container, run the following command:
        </para>
<screen>
<prompt role="root"># </prompt>podman container runlabel run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/yast-mgmt-qt:latest
</screen>
        <figure>
          <title>Running graphical YaST on top of ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-yast-qt.png" width="75%"/>
            </imageobject>
            <textobject role="description"><phrase>Running graphical YaST on top of ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="related-run-yast-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          YaST is generally described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-yast-gui.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-kvm-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the KVM virtualization workload using Podman</title></info>
  
  <section xml:id="introduction-run-kvm-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run KVM VM Host Server on the Adaptable Linux Platform
      (ALP).
    </para>
  </section>
  <section xml:id="requirements-run-kvm-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          When running ALP in a virtualized environment, you need to
          enable the nested KVM virtualization on the bare-metal host
          operating system and use <literal>kernel-default</literal> kernel
          instead of the default <literal>kernel-default-base</literal> in
          ALP.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-kvm">
    <title>Starting the KVM workload</title>
    <para>
      ALP can serve as a host running virtual machines. The following
      procedure describes steps to prepare the ALP host to run
      containerized KVM VM Host Server and run an example VM Guest on top of it.
    </para>
    <procedure>
      <step>
        <para>
          Identify the KVM workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search kvm
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry and install all the wrapper scripts:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm:latest</screen>
      </step>
      <step>
        <para>
          Create the <literal>libvirtd</literal> container from the downloaded
          image:
        </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh create</screen>
      </step>
      <step>
        <para>
          Start the container:
        </para>
<screen><prompt role="root"># </prompt>kvm-container-manage.sh start</screen>
      </step>
      <step>
        <para>
          Optionally, run a VM Guest on top of the started KVM VM Guest
          using the <command>virt-install.sh</command> script.
        </para>
        <tip>
          <para>
            <command>virt-install.sh</command> uses the
            <filename>openSUSE-Tumbleweed-JeOS.x86_64-OpenStack-Cloud.qcow2</filename>
            image by default. To specify another VM image, modify the
            <option>APPLIANCE_MIRROR</option> and <option>APPLIANCE</option>
            options in the <filename>/etc/kvm-container.conf</filename> file.
          </para>
        </tip>
        <tip>
          <para>
            <command>virsh.sh</command> is a wrapper script to launch the
            <command>virsh</command> command inside the container (the default
            container name is <command>libvirtd</command>).
          </para>
        </tip>
<screen>
<prompt>&gt; </prompt>virt-install.sh
[...]
Starting install...
Password for first root login is: OPjQok1nlfKp5DRZ
Allocating 'Tumbleweed-JeOS_5221fd7860.qcow2'            |    0 B  00:00:00 ...
Creating domain...                                       |    0 B  00:00:00
Running text console command: virsh --connect qemu:///system console Tumbleweed-JeOS_5221fd7860
Connected to domain 'Tumbleweed-JeOS_5221fd7860'
Escape character is ^] (Ctrl + ])

Welcome to openSUSE Tumbleweed 20220919 - Kernel 5.19.8-1-default (hvc0).

eth0: 192.168.10.67 fe80::5054:ff:fe5a:c416

localhost login:
</screen>
      </step>
    </procedure>
  </section>
  <section xml:id="related-run-kvm-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Details about the usage of the
          <command>kvm-container-manage.sh</command> script are described in
          <xref linkend="reference-kvm-container-manage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling KVM nested virtualization is described in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-vt-installation.html#sec-vt-installation-nested-vms"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find details about <systemitem class="library">libvirt</systemitem> in
          <link xlink:href="https://susedoc.github.io/doc-sle/main/html/SLES-virtualization/part-virt-libvirt.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-kvm-container-manage"><info><title xmlns:its="http://www.w3.org/2005/11/its">Usage of the <command>kvm-container-manage.sh</command> script</title></info>
  
  <para>
    The <command>kvm-container-manage.sh</command> script is used to manage the
    KVM server container on the Adaptable Linux Platform (ALP). This article lists each
    subcommand of the script and describes its purpose.
  </para>
  <variablelist>
    <varlistentry>
      <term><command>kvm-container-manage.sh create</command></term>
      <listitem>
        <para>
          Creates a KVM server container from a previously downloaded
          container image. To download the images, use
          <command>podman</command>, for example:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/kvm:latest</screen>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh start</command></term>
      <listitem>
        <para>
          Starts the KVM server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh virsh list</command></term>
      <listitem>
        <para>
          Lists all running VM Guests. Append the <option>--all</option>
          option to get the list of all—running and
          stopped—VM Guests.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh stop</command></term>
      <listitem>
        <para>
          Stops the running KVM server container.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><command>kvm-container-manage.sh uninstall</command></term>
      <listitem>
        <para>
          Cleans the host environment by uninstalling all files that were
          required to run the KVM server container.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="related-kvm-container-manage">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          Basic introduction to Podman is in
          <xref linkend="concept-containers-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is explained in
          <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Find details on running the KVM workload in
          <xref linkend="task-run-kvm-with-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section><section xml:lang="en" role="task" version="5.1" xml:id="task-run-cockpit-with-podman"><info><title xmlns:its="http://www.w3.org/2005/11/its">Running the Cockpit Web server using Podman</title></info>
  
  <section xml:id="introduction-run-cockpit-with-podman">
    <title>Introduction</title>
    <para>
      This article describes how to run a containerized Cockpit Web server on
      the Adaptable Linux Platform (ALP) using Podman.
    </para>
    <note>
      <para>
        An alternative way of installing and enabling the Cockpit Web server
        is described in
        <link xlink:href="https://en.opensuse.org/openSUSE:ALP/Workgroups/SysMngmnt/Cockpit#Install_the_Web_Server_Via_Packages"/>.
      </para>
    </note>
  </section>
  <section xml:id="requirements-run-cockpit-with-podman">
    <title>Requirements</title>
    <itemizedlist>
      <listitem>
        <para>
          Deployed ALP base OS.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed and enabled Podman.
        </para>
      </listitem>
      <listitem>
        <para>
          Installed the <package>alp_cockpit</package> pattern.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="alp-starting-cockpit">
    <title>Starting the Cockpit workload</title>
    <para>
      Cockpit is a tool to administer one or more hosts from one place via a
      Web user interface. Its default functionality is extended by plug-ins
      that you can install additionally. You do not need the Cockpit Web user
      interface installed on every ALP host. One instance of the Web
      interface can connect to multiple hosts if they have the
      <package>alp_cockpit</package> pattern installed.
    </para>
    <para>
      ALP has the base part of the Cockpit component installed by
      default. It is included in the <package>alp_cockpit</package> pattern. To
      install and run Cockpit's Web interface, follow these steps:
    </para>
    <procedure>
      <step>
        <para>
          Identify the Cockpit Web server workload image:
        </para>
<screen>
<prompt role="root"># </prompt>podman search cockpit-ws
registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws
</screen>
      </step>
      <step>
        <para>
          Pull the image from the registry:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel install \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
      </step>
      <step>
        <para>
          Run the Cockpit's containerized Web server:
        </para>
<screen><prompt role="root"># </prompt>podman container runlabel --name cockpit-ws run \
 registry.opensuse.org/suse/alp/workloads/tumbleweed_containerfiles/suse/alp/workloads/cockpit-ws:latest</screen>
      </step>
      <step>
        <para>
          To run the Cockpit's Web server on each ALP boot, enable its
          service:
        </para>
<screen><prompt role="root"># </prompt>systemctl enable cockpit.service</screen>
      </step>
      <step>
        <para>
          To view the Cockpit Web user interface, point your Web browser to
          the following address and accept the self-signed certificate:
        </para>
<screen>https://<replaceable>HOSTNAME_OR_IP_OF_ALP_HOST:9090</replaceable></screen>
        <figure>
          <title>Cockpit running on ALP</title>
          <mediaobject>
            <imageobject role="fo">
              <imagedata fileref="alp-cockpit.png" width="100%"/>
            </imageobject>
            <imageobject role="html">
              <imagedata fileref="alp-cockpit.png" width="100%"/>
            </imageobject>
            <textobject role="description"><phrase>Cockpit running on ALP</phrase>
            </textobject>
          </mediaobject>
        </figure>
      </step>
    </procedure>
  </section>
  <section xml:id="next-run-cockpit-with-podman">
    <title>Next steps</title>
    <itemizedlist>
      <listitem>
        <para>
          Administer the system using Cockpit.
        </para>
      </listitem>
      <listitem>
        <para>
          Install and run additional workloads. For their list and description,
          refer to <xref linkend="reference-available-alp-workloads"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="related-run-cockpit-with-podman">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          ALP deployment is described in
          <xref linkend="concept-alp-deployment"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Installing software packages and patterns is detailed in
          <link xlink:href="https://documentation.suse.com/sles/html/SLES-all/cha-sw-cl.html"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Enabling Podman is described in
          <xref linkend="task-enable-podman"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Podman usage is listed in <xref linkend="reference-podman-usage"/>.
        </para>
      </listitem>
      <listitem>
        <para>
          Managing <systemitem class="daemon">systemd</systemitem> services is described in
          <link xlink:href="https://documentation.suse.com/smart/linux/html/reference-systemctl-enable-disable-services/reference-systemctl-enable-disable-services.html"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
<section xml:lang="en" role="reference" version="5.1" xml:id="reference-cockpit-adding-functionality"><info><title xmlns:its="http://www.w3.org/2005/11/its">Adding more functionality to Cockpit</title></info>
  
  <section xml:id="introduction-cockpit-adding-functionality">
    <title>Introduction</title>
    <para>
      After you deploy Cockpit on the Adaptable Linux Platform (ALP), it already provides
      a default functionality. The following sections describe how to extend it
      by installing additional Cockpit extensions. Note that you need to
      reboot ALP to apply the changes.
    </para>
    <important>
      <para>
        Some packages described in this article are available from the
        <literal>ALP-Build</literal> repository which may be disabled by
        default. To make sure the repository is enabled, run the following
        command:
      </para>
<screen><prompt role="root"># </prompt>zypper mr -e ALP-Build &amp;&amp; refresh</screen>
    </important>
  </section>
  <section xml:id="cockpit-metrics">
    <title>Metrics</title>
    <para>
      To enable the visualization of some current metrics, install the PCP
      extension:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-pcp
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Metrics and history in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-metrics.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-metrics.png" width="100%"/>
        </imageobject>
        <textobject role="description"><phrase>Metrics and history in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="cockpit-software-updates">
    <title>Software updates</title>
    <para>
      To be able to perform transactional software updates from Cockpit,
      install the <package>cockpit-tukit</package> package:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-tukit
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Software updates in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-software.png" width="75%"/>
        </imageobject>
        <textobject role="description"><phrase><guimenu>Software Updates</guimenu> in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="cockpit-storage-devices">
    <title>Storage devices</title>
    <para>
      To manage local storage devices and their associated technologies,
      install the <package>cockpit-storaged</package> package:
    </para>
<screen>
<prompt role="root"># </prompt>transactional-update pkg install cockpit-storaged
<prompt role="root"># </prompt>reboot
</screen>
    <figure>
      <title>Storage in Cockpit</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="alp-cockpit-storage.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="alp-cockpit-storage.png" width="75%"/>
        </imageobject>
        <textobject role="description"><phrase><guimenu>Storage</guimenu> in Cockpit</phrase>
        </textobject>
      </mediaobject>
    </figure>
  </section>
  <section xml:id="related-cockpit-adding-functionality">
    <title>Related topics</title>
    <itemizedlist>
      <listitem>
        <para>
          The process of running the Cockpit container is described in
          <xref linkend="task-run-cockpit-with-podman"/>.
        </para>
      </listitem>
    </itemizedlist>
  </section>
</section></section></chapter></book>
