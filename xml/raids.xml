<?xml version="1.0" encoding="UTF-8"?>
<article xmlns="http://docbook.org/ns/docbook" xml:base="raids.xml" version="5.0" xml:id="software-raid" xml:lang="en">
  <info>
      <title xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">Software RAID on <phrase><phrase os="generic">SUSE Linux</phrase></phrase></title>
      <revhistory xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <title>Changelog</title>
        <revision><revnumber>1</revnumber><date>2023-05-09</date>
          <revdescription>
            <para>
              Initial version.
            </para>
          </revdescription>
        </revision>
      </revhistory>
      
      
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
      
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="updated" content="2023-11-11" its:translate="no"/>
      
      
      
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="architecture" content="x86;power" its:translate="no"/>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="productname" its:translate="no">
        <productname version="15-SP5">SUSE Linux Enterprise Server</productname>
      </meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="title" its:translate="yes">Software RAID description and configuration</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="description" its:translate="yes">The article focuses on configuring software RAID and provides detail about various levels of RAID.</meta>
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="social-descr" its:translate="yes">Description of software RAID.</meta>
      
      <meta xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its" name="category" content="Systems Management" its:translate="no"/>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:component>Smart Docs</dm:component>
          <dm:product>Documentation</dm:product>
          <dm:assignee>jsindelarova@suse.com</dm:assignee>
        </dm:bugtracker>
        <dm:editurl>https://github.com/SUSE/doc-modular/tree/main/articles/</dm:editurl>
        <dm:translation>no</dm:translation>
      </dm:docmanager>
      <abstract xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" xmlns:its="http://www.w3.org/2005/11/its">
        <variablelist>
          <varlistentry>
            <term>WHAT?</term>
            <listitem>
              <para>
                Basic information about software RAIDs.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>WHY?</term>
            <listitem>
              <para>
                You need information on RAID levels, or you want to configure
                or monitor a RAID.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>EFFORT</term>
            <listitem>
              <para>
                15 minutes of reading time.
              </para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>GOAL</term>
            <listitem>
              <para>
                You will be able to configure a software RAID using YaST.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </abstract>
    </info>
  <section role="concept" xml:lang="en" version="5.2" xml:id="glue-raids"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Software RAID on <phrase><phrase os="generic">SUSE Linux</phrase></phrase></title>
    
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  
  <para>
    The purpose of RAID (redundant array of independent disks) is to combine
    several hard disk partitions into one large virtual hard disk to optimize
    performance, data security, or both. Most RAID controllers use the SCSI
    protocol, because it can address a larger number of hard disks in a more
    effective way than the IDE protocol and is more suitable for parallel
    processing of commands. There are some RAID controllers that support IDE or
    SATA hard disks. Software RAID provides the advantages of RAID systems
    without the additional cost of hardware RAID controllers. However, this
    requires some CPU time and has memory requirements that make it unsuitable
    for real high performance computers.
  </para>
</section>
  <section role="concept" xml:lang="en" version="5.2" xml:id="concept-raid-levels"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">RAID levels</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com"/>
    <abstract xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">
      <para>
        RAID implies several strategies for combining several hard disks in a
        RAID system, each with different goals, advantages, and
        characteristics. These variations are commonly known as RAID levels.
      </para>
    </abstract>
  </info>
  
  <para>
    The RAID levels can be split into the following categories:
  </para>
  <variablelist>
    <varlistentry>
      <term>Standard levels</term>
      <listitem>
        <para>
          These levels and their associated data formats are standardized by
          the Storage Networking Industry Association (SNIA) in the Common RAID
          Disk Drive Format (DDF) standard. The standard RAID levels are RAID
          0, RAID 1, RAID 2, RAID 3, RAID 4, RAID 5 and RAID 6. For details,
          refer to <xref linkend="standard-raid-levels"/>.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Nested levels</term>
      <listitem>
        <para>
          Combine already existing arrays into a new array. For example, RAID
          0+1 or RAID 1+0.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Non-standard levels</term>
      <listitem>
        <para>
          Usually, these are proprietary RAID configurations designed to meet
          specific needs, for example, Linux MD RAID 10.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <section xml:id="standard-raid-levels">
    <title>Standard RAID levels</title>
    <para>
      Originally, there were only five standard levels of RAID, but other
      levels have evolved as described in the following sections.
    </para>
    <section xml:id="raid-0-level">
      <title>RAID 0</title>
      <para>
        RAID 0 improves the performance of your data operations by spreading
        out blocks of each file across multiple disks. This data distributions
        is called striping. The overall capacity is a sum of capacity of disk
        in the RAID. The benefit of RAID 0 is that the throughput of read and
        write operations to any file is multiplied by the number of drives
        because reads and writes are done concurrently.
      </para>
      <para>
        The disadvantage of RAID 0 is that it does not provide any data backup,
        so if a disk fails, the whole RAID is destroyed and there is data loss.
      </para>
    </section>
    <section xml:id="raid-1-level">
      <title>RAID 1</title>
      <para>
        RAID 1 provides adequate security for your data, because the data is
        copied to another hard disk 1:1. This is known as <emphasis>hard disk
        mirroring</emphasis>. This level does not provide striping, so it does
        not provide a higher read or write throughput. However, the array
        continues to operate as long as at least one drive is functioning.
      </para>
      <para>
        RAID 1 requires at least two devices.
      </para>
    </section>
    <section xml:id="raid-2-level">
      <title>RAID 2</title>
      <para>
        In RAID 2, the striping is performed on a bit level. This RAID level is
        currently not used in practice.
      </para>
    </section>
    <section xml:id="raid-3-level">
      <title>RAID 3</title>
      <para>
        In RAID 3, the striping is performed on a byte level with a dedicated
        parity drive. RAID 3 is not commonly used in practice.
      </para>
    </section>
    <section xml:id="raid-4-level">
      <title>RAID 4</title>
      <para>
        RAID 4 provides a block-level striping with a dedicated parity drive.
        If a data disk fails, the parity data is used to create a replacement
        disk. However, the parity disk might create a bottleneck for write
        access. This level requires at least three devices.
      </para>
      <para>
        RAID 4 is not commonly used in practice.
      </para>
    </section>
    <section xml:id="raid-5-level">
      <title>RAID 5</title>
      <para>
        RAID 5 is an optimized compromise between Level 0 and Level 1 in terms
        of performance and redundancy. The hard disk space equals the number of
        disks used minus one. The data is distributed over the hard disks as
        with RAID 0, including the parity data. Parity blocks are there for
        security reasons. They are linked to each other with XOR, enabling the
        contents to be reconstructed by the corresponding parity block in case
        of system failure.
      </para>
      <para>
        With RAID 5, no more than one hard disk can fail at the same time. If
        one hard disk fails, it must be replaced when possible to avoid the
        risk of losing data.
      </para>
      <para>
        RAID 5 requires at least three disks.
      </para>
    </section>
    <section xml:id="raid-6-level">
      <title>RAID 6</title>
      <para>
        RAID 6 consists of block-level striping with double distributed parity.
        RAID 6 provides extremely high data fault tolerance by sustaining
        multiple simultaneous drive failures. Even if two of the hard disks
        fail, the system continues to be operational, with no data loss.
      </para>
      <para>
        The performance for RAID 6 is slightly lower but comparable to RAID 5
        in normal mode and single disk failure mode. It is very slow in dual
        disk failure mode. A RAID 6 configuration needs a considerable amount
        of CPU time and memory for write operations.
      </para>
      <para>
        RAID 6 requires at least four disks. In general, it requires N+2 disks,
        where N is the number of disks required to store data and 2 is for the
        dual parity.
      </para>
    </section>
  </section>
  <section xml:id="raid-nested-raids">
    <title>Nested RAID</title>
    <section xml:id="raid-nested-01">
      <title>RAID 0+1</title>
      <para>
        RAID 0+1, also called RAID 01, mirrors striped disks, so data are
        replicated and shared at the same time. The minimum number of disks is
        four.
      </para>
    </section>
    <section xml:id="raid-nested-10">
      <title>RAID 1+0</title>
      <para>
        RAID 1+0, also called RAID 10, is a combination of striping and
        mirroring. Data are distributed into several disks and each of these
        disks is mirrored to another disk.
      </para>
    </section>
  </section>
</section>
  <section role="task" xml:lang="en" version="5.2" xml:id="task-configuring-raids-using-yast"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Configuring software RAID using YaST</title>
    
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  <para>
    The YaST software RAID configuration can be reached from the YaST
    Expert Partitioner. This partitioning tool also enables you to edit and
    delete existing partitions and create new ones that should be used with
    software RAID. These instructions apply on setting up RAID levels 0, 1, 5,
    and 6.
    
  </para>
  
  <procedure>
    <step>
      <para>
        Launch YaST.
      </para>
    </step>
    <step>
      <para>
        In the <guimenu>System</guimenu> menu, select
        <guimenu>Partitioner</guimenu> on the right.
      </para>
    </step>
    <step>
      <para>
        If necessary, create partitions that should be used with your RAID
        configuration. Do not format them and set the partition type to
        <guimenu>0xFD Linux RAID</guimenu>. When using existing partitions it
        is not necessary to change their partition type—YaST will
        automatically do so.
      </para>
      <para>
        It is strongly recommended to use partitions stored on different hard
        disks to decrease the risk of losing data if one is defective
        (RAID 1 and 5) and to optimize the performance of RAID 0.
      </para>
      <para>
        For RAID 0 at least two partitions are needed. RAID 1 requires
        exactly two partitions, while at least three partitions are required
        for RAID 5. A RAID 6 setup requires at least four partitions.
        It is recommended to use only partitions of the same size because each
        segment can contribute only the same amount of space as the smallest
        sized partition.
      </para>
    </step>
    <step>
      <para>
        In the <guimenu>Add</guimenu> menu select <guimenu>RAID</guimenu>.
      </para>
    </step>
    <step>
      <para>
        Select a <guimenu>RAID Type</guimenu> and <guimenu>Add</guimenu> an
        appropriate number of partitions from the <guimenu>Available
        Devices</guimenu> dialog.
      </para>
      <para>
        You can optionally assign a <guimenu>RAID Name</guimenu> to your RAID.
        It will make it available as
        <filename>/dev/md/<replaceable>NAME</replaceable></filename>. See
        <xref linkend="task-naming-raids"/> for more information.
      </para>
      <figure xml:id="fig-yast2-raid3">
        <title>Example RAID 1 configuration</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="raid-yast-assign-disks.png" width="80%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="raid-yast-assign-disks.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>
    </step>
    <step>
      <para>
        Proceed with <guimenu>Next</guimenu>.
      </para>
    </step>
    <step>
      <para>
        Select the <guimenu>Chunk Size</guimenu> and, if applicable, the
        <guimenu>Parity Algorithm</guimenu>. The optimal chunk size depends on
        the type of data and the type of RAID. See
        <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes"/>
        for more information. More information on parity algorithms can be
        found with <command>man 8 mdadm</command> when searching for the
        <option>--layout</option> option. If unsure, stick with the defaults.
      </para>
    </step>
    <step>
      <para>
        Choose a <guimenu>Role</guimenu> for the volume. Your choice here only
        affects the default values for the upcoming dialog. They can be changed
        in the next step. If in doubt, choose <guimenu>Raw Volume
        (Unformatted)</guimenu>.
      </para>
    </step>
    <step>
      <para>
        Under <guimenu>Formatting Options</guimenu>, select <guimenu>Format
        device</guimenu>, then select the <guimenu>File system</guimenu>. The
        content of the <guimenu>Options</guimenu> menu depends on the file
        system. Usually there is no need to change the defaults.
      </para>
      <para>
        Under <guimenu>Mounting Options</guimenu>, select <guimenu>Mount
        device</guimenu>, then select the mount point. Select <guimenu>Fstab
        Options</guimenu> to add special mounting options for the volume.
      </para>
    </step>
    <step>
      <para>
        Proceed with <guimenu>Next</guimenu>.
      </para>
    </step>
  </procedure>
  <important>
    <title>RAID on disks</title>
    <para>
      While the partitioner makes it possible to create a RAID on top of disks
      instead of partitions, we do not recommend this approach for a number of
      reasons. Installing a bootloader on such RAID is not supported, so you
      need to use a separate device for booting. Tools like
      <package>fdisk</package> and <package>parted</package> do not work
      properly with such RAIDs, which may lead to incorrect diagnosis and
      actions by a person who is unaware of the RAID's particular setup.
    </para>
  </important>
</section>
  <section role="glue" xml:lang="en" version="5.2" xml:id="raid-management"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Managing software RAID</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  
  <para>
    After you set up a RAID, you can perform additional administration tasks.
    For example:
  </para>
  <itemizedlist>
    <listitem>
      <para>
        Change the default RAID names as described in
        <xref linkend="task-naming-raids"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Monitor the RAID as described in
        <xref linkend="task-raid-monitoring"/>.
      </para>
    </listitem>
    <listitem>
      <para>
        Configure the stripe size on the AArch64 architecture as described in
        <xref linkend="task-configuring-stripe-size"/>.
      </para>
    </listitem>
  </itemizedlist>
<section role="task" xml:lang="en" version="5.2" xml:id="task-naming-raids"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Naming software RAID</title>
    
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  <section xml:id="task-default-names">
    <title>Default names</title>
    <para>
      By default, software RAID devices have names following the pattern
      <literal>mdN</literal>, where <literal>N</literal> is a number. For
      example, they can be accessed as <filename>/dev/md127</filename> and are
      listed as <literal>md127</literal> in <filename>/proc/mdstat</filename>
      and <filename>/proc/partitions</filename>.
    </para>
  </section>
  <section xml:id="task-example-requirements">
    <title>Providing non-default names</title>
    <para>
      As working with the default names might be clumsy, there are two ways how
      to work around this:
    </para>
    <variablelist>
      <varlistentry>
        <term>Providing a named link to the device</term>
        <listitem>
          <para>
            You can optionally specify a name for the RAID device when creating
            it with YaST or on the command line with <command>mdadm --create
            '/dev/md/</command> <replaceable>NAME</replaceable>'. The device
            name will still be <literal>mdN</literal>, but the link
            <filename>/dev/md/<replaceable>NAME</replaceable></filename> will
            be created:
          </para>
<screen><prompt>&gt; </prompt>ls -og /dev/md
total 0
lrwxrwxrwx 1 8 Dec  9 15:11 myRAID -&gt; ../md127</screen>
          <para>
            The device will still be listed as <literal>md127</literal> under
            <filename>/proc</filename>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Providing a named device</term>
        <listitem>
          <para>
            If a named link to the device is not sufficient for your setup, add
            the line <literal>CREATE names=yes</literal> to
            <filename>/etc/mdadm.conf</filename> by running the following
            command:
          </para>
<screen><prompt>&gt; </prompt>echo "CREATE names=yes" | sudo tee -a  /etc/mdadm.conf</screen>
          <para>
            This will cause names like <literal>myRAID</literal> to be used as
            a <quote>real</quote> device name. The device will not only be
            accessible at <filename>/dev/myRAID</filename>, but also listed as
            <literal>myRAID</literal> under <filename>/proc</filename>. Note
            that this will only apply to RAIDs configured after the change to
            the configuration file. Active RAIDs will continue to use the
            <literal>mdN</literal> names until they get stopped and
            reassembled.
          </para>
          <warning>
            <title>Incompatible tools</title>
            <para>
              Not all tools may support named RAID devices. If a tool expects a
              RAID device to be named <literal>mdN</literal>, it will fail to
              identify the devices.
            </para>
          </warning>
        </listitem>
      </varlistentry>
    </variablelist>
  </section>
</section><section role="task" xml:lang="en" version="5.2" xml:id="task-configuring-stripe-size"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Configuring stripe size on RAID 5 on AArch64</title>
    
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  <para>
    By default, the stripe size is set to 4 kB. If you need to change the
    default stripe size, for example, to match the typical page size of
    64 kB on AArch64, you can configure the stripe size manually using
    CLI:
  </para>
<screen><prompt>&gt; </prompt><command>sudo</command> echo 16384  &gt; /sys/block/md1/md/stripe_size</screen>
  <para>
    The above command sets the stripe size to 16 kB. You can set other
    values such as 4096 or 8192 but the value must be a power of 2.
  </para>
</section><section role="task" xml:lang="en" version="5.2" xml:id="task-raid-monitoring"><info>
    <title xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion">Monitoring software RAIDs</title>
    <meta xmlns:its="http://www.w3.org/2005/11/its" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:trans="http://docbook.org/ns/transclusion" name="maintainer" content="jsindelarova@suse.com" its:translate="no"/>
  </info>
  
  <para>
    You can run <command>mdadm</command> as a daemon in the
    <literal>monitor</literal> mode to monitor your software RAID. In the
    <literal>monitor</literal> mode, <command>mdadm</command> performs regular
    checks on the array for disk failures. If there is a failure,
    <command>mdadm</command> sends an e-mail to the administrator. To define
    the time interval of the checks, run the following command:
  </para>
<screen>mdadm --monitor --mail=root@localhost --delay=1800 /dev/md2</screen>
  <para>
    The command above turns on monitoring of the <literal>/dev/md2</literal>
    array in intervals of 1800 seconds. In the event of a failure, an
    e-mail will be sent to <literal>root@localhost</literal>.
  </para>
  <note>
    <title>RAID checks are enabled by default</title>
    <para>
      The RAID checks are enabled by default. It may happen that the interval
      between each check is not long enough and you may encounter warnings.
      Thus, you can increase the interval by setting a higher value with the
      <literal>delay</literal> option.
    </para>
  </note>
</section></section>
</article>
